{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "DEdOoJ93e008",
        "f_Qg27JlTHS1",
        "jopehmrVTRSm",
        "limqRnayUnj5",
        "7M3M0cKNUsfb",
        "WwSO5W3KUsjX",
        "fdjE61pw7CBy",
        "KgwBXdjq3IkB",
        "H9Z6RxOnzLa9",
        "Jfky7439zNhN",
        "TmXsuwuctVw7",
        "jRFofx7s1jJa",
        "cbS1GfWq2DmK",
        "_3qwJt8Q1qN1",
        "2_uEiN_p1ufE",
        "celagCJTtFd2",
        "yi7oQJcHs7KJ"
      ],
      "machine_shape": "hm",
      "gpuClass": "premium"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Start"
      ],
      "metadata": {
        "id": "cngn85uSngfu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NEW TASKS:\n",
        "* [X] Seq2Seq: sort by src_len and unsort output --> ensure output matches with trg\n",
        "* [X] Pivot model: ensure it works for $n$ seq2seq models\n",
        "* [ ] Triang model: ensure outputs from all submodels match"
      ],
      "metadata": {
        "id": "FCN31jhjnCzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# piv_endefr_74kset_2.pt using PivotModel in bentrevett/pytorch-seq2seq-OLD.ipynb"
      ],
      "metadata": {
        "id": "yTMYDLiGey7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/bentrevett/pytorch-seq2seq/blob/master/4%20-%20Packed%20Padded%20Sequences%2C%20Masking%2C%20Inference%20and%20BLEU.ipynb\n",
        "# based on https://gmihaila.github.io/tutorial_notebooks/pytorchtext_bucketiterator/#dataset-class"
      ],
      "metadata": {
        "id": "V6P0jGmKybf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "piv_langs = ['it']\n",
        "langs = ['en', 'fr'] + piv_langs\n",
        "DIR_PATH = '/content/gdrive/MyDrive/Colab Notebooks/eaai24'"
      ],
      "metadata": {
        "id": "bktHQ8XCnDIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "DEdOoJ93e008"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOnOF0x_PM0F",
        "outputId": "f2f272e2-32b6-49c8-fa4c-f397e92c3629"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install torch==1.8.2 torchvision==0.9.2 torchaudio==0.8.2 --extra-index-url https://download.pytorch.org/whl/lts/1.8/cu111 -q"
      ],
      "metadata": {
        "id": "SA0jFbKHbfHP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "085efd1d-00c5-4d9b-8d88-78392f0c7d22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 GB\u001b[0m \u001b[31m991.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.14.1 requires torch==1.13.1, but you have torch 1.8.2+cu111 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1754nP4obkXv",
        "outputId": "932b463b-0fa5-4907-c3ac-c29886a6c9c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.8.2+cu111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMlSEzldbP3q",
        "outputId": "3d0bc193-dc7e-4e7d-9a78-f6a09583e957"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m735.5/735.5 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.9.2+cu111 requires torch==1.8.2, but you have torch 1.8.0 which is incompatible.\n",
            "torchaudio 0.8.2 requires torch==1.8.2, but you have torch 1.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install torchtext==0.9 -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if 'en' in langs:\n",
        "  !python -m spacy download en_core_web_sm -q\n",
        "if 'de' in langs:\n",
        "  !python -m spacy download de_core_news_sm -q\n",
        "if 'fr' in langs:\n",
        "  !python -m spacy download fr_core_news_sm -q\n",
        "if 'it' in langs:\n",
        "  !python -m spacy download it_core_news_sm -q\n",
        "if 'es' in langs:\n",
        "  !python -m spacy download es_core_news_sm -q\n",
        "if 'pt' in langs:\n",
        "  !python -m spacy download pt_core_news_sm -q\n",
        "if 'ro' in langs:\n",
        "  !python -m spacy download ro_core_news_sm -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dw6ZjirTbTE8",
        "outputId": "e9598487-896a-4b5f-fcc1-d9527478ad79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-03-17 15:04:46.209344: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-17 15:04:46.363442: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-03-17 15:04:47.113026: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-17 15:04:47.113129: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-17 15:04:47.113148: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "2023-03-17 15:05:02.710265: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-17 15:05:02.862331: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-03-17 15:05:03.654519: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-17 15:05:03.654626: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-17 15:05:03.654647: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n",
            "2023-03-17 15:05:17.417004: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-17 15:05:17.571309: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-03-17 15:05:18.326093: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-17 15:05:18.326225: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-17 15:05:18.326253: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('it_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchtext.legacy.data import Field, BucketIterator\n",
        "from torchtext.legacy.data import Dataset, Example\n",
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import pickle\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "6N_lgaPWd4AX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# My Section"
      ],
      "metadata": {
        "id": "IZIH_L_HeyEJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "f_Qg27JlTHS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FIELD_DICT = {}\n",
        "if 'en' in langs:\n",
        "  spacy_en = spacy.load('en_core_web_sm')\n",
        "  def tokenize_en(text):\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "  EN_FIELD = Field(tokenize = tokenize_en, init_token = '<sos>', eos_token = '<eos>', lower = True, include_lengths = True)\n",
        "  FIELD_DICT['en'] = EN_FIELD\n",
        "if 'de' in langs:\n",
        "  spacy_de = spacy.load('de_core_news_sm')\n",
        "  def tokenize_de(text):\n",
        "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
        "  DE_FIELD = Field(tokenize = tokenize_de, init_token = '<sos>', eos_token = '<eos>', lower = True, include_lengths = True)\n",
        "  FIELD_DICT['de'] = DE_FIELD\n",
        "if 'fr' in langs:\n",
        "  spacy_fr = spacy.load('fr_core_news_sm')\n",
        "  def tokenize_fr(text):\n",
        "    return [tok.text for tok in spacy_fr.tokenizer(text)]\n",
        "  FR_FIELD = Field(tokenize = tokenize_fr, init_token = '<sos>', eos_token = '<eos>', lower = True, include_lengths = True)\n",
        "  FIELD_DICT['fr'] = FR_FIELD\n",
        "if 'it' in langs:\n",
        "  spacy_it = spacy.load('it_core_news_sm')\n",
        "  def tokenize_it(text):\n",
        "    return [tok.text for tok in spacy_it.tokenizer(text)]\n",
        "  IT_FIELD = Field(tokenize = tokenize_it, init_token = '<sos>', eos_token = '<eos>', lower = True, include_lengths = True)\n",
        "  FIELD_DICT['it'] = IT_FIELD\n",
        "if 'es' in langs:\n",
        "  spacy_es = spacy.load('es_core_news_sm')\n",
        "  def tokenize_es(text):\n",
        "    return [tok.text for tok in spacy_es.tokenizer(text)]\n",
        "  ES_FIELD = Field(tokenize = tokenize_es, init_token = '<sos>', eos_token = '<eos>', lower = True, include_lengths = True)\n",
        "  FIELD_DICT['es'] = ES_FIELD\n",
        "if 'pt' in langs:\n",
        "  spacy_pt = spacy.load('pt_core_news_sm')\n",
        "  def tokenize_pt(text):\n",
        "    return [tok.text for tok in spacy_pt.tokenizer(text)]\n",
        "  PT_FIELD = Field(tokenize = tokenize_pt, init_token = '<sos>', eos_token = '<eos>', lower = True, include_lengths = True)\n",
        "  FIELD_DICT['pt'] = PT_FIELD\n",
        "if 'ro' in langs:\n",
        "  spacy_ro = spacy.load('ro_core_news_sm')\n",
        "  def tokenize_ro(text):\n",
        "    return [tok.text for tok in spacy_ro.tokenizer(text)]\n",
        "  RO_FIELD = Field(tokenize = tokenize_ro, init_token = '<sos>', eos_token = '<eos>', lower = True, include_lengths = True)\n",
        "  FIELD_DICT['ro'] = RO_FIELD"
      ],
      "metadata": {
        "id": "aXSlRvZqPRqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "jopehmrVTRSm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 16\n",
        "\n",
        "train_len = 64000\n",
        "valid_len = 3200\n",
        "test_len = 6400\n",
        "\n",
        "train_pt = train_len\n",
        "valid_pt = train_pt + valid_len\n",
        "test_pt = valid_pt + test_len\n",
        "\n",
        "print(train_pt, valid_pt, test_pt)"
      ],
      "metadata": {
        "id": "T5DlQGmUoKoF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8b8a06b-31d5-4c4f-de76-ff5c921fef88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64000 67200 73600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://discuss.pytorch.org/t/how-to-save-and-load-torchtext-data-field-build-vocab-result/50407/3\n",
        "def save_vocab(vocab, path):\n",
        "  with open(path, 'w+', encoding='utf-8') as f:\n",
        "    for token, index in vocab.stoi.items():\n",
        "      f.write(f'{index}\\t{token}\\n')\n",
        "def read_vocab(path):\n",
        "  vocab = dict()\n",
        "  with open(path, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "      index, token = line.split('\\t')\n",
        "      vocab[token] = int(index)\n",
        "  return vocab"
      ],
      "metadata": {
        "id": "lqFlyKPUq1Bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with open('/content/gdrive/MyDrive/Colab Notebooks/eaai24/Datasets/endefr_75kpairs_2k5-freq-words.pkl', 'rb') as f:\n",
        "# with open('/content/gdrive/MyDrive/Colab Notebooks/eaai24/Datasets/enfr_160kpairs_2k5-freq-words.pkl', 'rb') as f:\n",
        "#   data = pickle.load(f)\n",
        "# data[80], len(data)"
      ],
      "metadata": {
        "id": "pHT8AUMJe7PF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EnDeFrItEsPtRo-60k-most10k-1.pkl is most suitable. Each lang has ~5-7k words\n",
        "# EnDeFrItEsPtRo-76k-most5k.pkl: each lang has 6->12k words (too much. use this in the future)"
      ],
      "metadata": {
        "id": "0jNRBlIUzV_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataname = 'EnDeFrItEsPtRo-76k-most5k.pkl'\n",
        "with open(f'{DIR_PATH}/Datasets/{dataname}', 'rb') as f:\n",
        "  data = pickle.load(f)\n",
        "data[8], len(data)"
      ],
      "metadata": {
        "id": "g8r6JHsEXCDI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc0017fe-c7e8-4573-9eef-5f5749605282"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'en': 'What is the result?',\n",
              "  'de': 'Und wie sehen die Ergebnisse aus?',\n",
              "  'fr': 'Quelle en est la conséquence ?',\n",
              "  'it': 'Quali sono i risultati?',\n",
              "  'es': '¿Cuáles son los resultados?',\n",
              "  'pt': 'Quais são os resultados?',\n",
              "  'ro': 'Care este rezultatul?'},\n",
              " 76245)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data_set = [[pair['en'], pair['it'], pair['fr']] for pair in data]\n",
        "# FIELDS = [('en', EN_FIELD), ('it', IT_FIELD), ('fr', FR_FIELD)]\n",
        "data_set = [[pair[lang] for lang in langs] for pair in data]\n",
        "FIELDS = [(lang, FIELD_DICT[lang]) for lang in langs]\n",
        "train_examples = list(map(lambda x: Example.fromlist(list(x), fields=FIELDS), data_set[: train_pt]))\n",
        "valid_examples = list(map(lambda x: Example.fromlist(list(x), fields=FIELDS), data_set[train_pt : valid_pt]))\n",
        "test_examples = list(map(lambda x: Example.fromlist(list(x), fields=FIELDS), data_set[valid_pt : test_pt]))"
      ],
      "metadata": {
        "id": "cyFzD7FQfHBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dt = Dataset(train_examples, fields=FIELDS)\n",
        "valid_dt = Dataset(valid_examples, fields=FIELDS)\n",
        "test_dt = Dataset(test_examples, fields=FIELDS)"
      ],
      "metadata": {
        "id": "Mron6AMwfH6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EN_FIELD.build_vocab(train_dt, min_freq = 2)\n",
        "# IT_FIELD.build_vocab(train_dt, min_freq = 2)\n",
        "# FR_FIELD.build_vocab(train_dt, min_freq = 2)\n",
        "# len(EN_FIELD.vocab), len(IT_FIELD.vocab), len(FR_FIELD.vocab)\n",
        "for lang in langs:\n",
        "  FIELD_DICT[lang].build_vocab(train_dt, min_freq = 2)\n",
        "  print(f'{lang}: {len(FIELD_DICT[lang].vocab)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYY067rbwZGN",
        "outputId": "c232726b-6c61-45b4-b49c-bbbbe58cbc7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "en: 6964\n",
            "fr: 9703\n",
            "it: 10712\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save_vocab(EN_FIELD.vocab, f'{DIR_PATH}/Datasets/EnDeFrItEsPtRo-76k-most5k-en_vocab.txt')\n",
        "# EN_FIELD.vocab = read_vocab(f'{DIR_PATH}/Datasets/EnDeFrItEsPtRo-76k-most5k-en_vocab.txt')\n",
        "# FR_FIELD.vocab = read_vocab(f'{DIR_PATH}/Datasets/EnDeFrItEsPtRo-76k-most5k-fr_vocab.txt')"
      ],
      "metadata": {
        "id": "_IpSjObrOs8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_dt, valid_dt, test_dt),\n",
        "     batch_size = BATCH_SIZE,\n",
        "     sort_within_batch = True,\n",
        "     sort_key = lambda x : len(x.en),\n",
        "     device = device)"
      ],
      "metadata": {
        "id": "9ZrebZgnkiuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i, batch in enumerate(train_iterator):\n",
        "#   break\n",
        "# print(batch.en[0].shape, batch.en[1])\n",
        "# print(batch.fr[0].shape, batch.fr[1])\n",
        "# print(batch.fields)"
      ],
      "metadata": {
        "id": "XxntiGWVlA4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# src_sent, piv_sent, trg_sent = [], [], []\n",
        "# for i in batch.en[0][: , 0]:\n",
        "#   src_sent.append(EN_FIELD.vocab.itos[i])\n",
        "# for i in batch.fr[0][:, 0]:\n",
        "#   trg_sent.append(FR_FIELD.vocab.itos[i])\n",
        "# print(' '.join(src_sent))\n",
        "# print(' '.join(trg_sent))"
      ],
      "metadata": {
        "id": "m7IO5L0nvEeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in range(10):\n",
        "#   print(EN_FIELD.vocab.itos[i], FR_FIELD.vocab.itos[i])"
      ],
      "metadata": {
        "id": "-XROmZP0olgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "3Cp4hrsvTlPy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The correct implementation of that [paper](https://arxiv.org/pdf/1409.0473.pdf) should be like this: [repo](https://github.com/graykode/nlp-tutorial#4-attention-mechanism) - [colab](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/4-2.Seq2Seq(Attention)/Seq2Seq(Attention).ipynb). My method is a bit different, actually it is implemented based on [Luong et. al.](https://arxiv.org/pdf/1508.04025.pdf) with Global Attention using concat method ([implement in tf](https://github.com/philipperemy/keras-attention/blob/master/attention/attention.py))."
      ],
      "metadata": {
        "id": "pnHQspOYMciQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder"
      ],
      "metadata": {
        "id": "limqRnayUnj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "    self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
        "    self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, src, src_len):\n",
        "    #src = [src len, batch size]\n",
        "    #src_len = [batch size]\n",
        "    embedded = self.dropout(self.embedding(src))  #embedded = [src len, batch size, emb dim]\n",
        "\n",
        "    #need to explicitly put lengths on cpu!\n",
        "    packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len.to('cpu'))\n",
        "\n",
        "    #  when the input is a pad token are all zeros\n",
        "    packed_outputs, hidden = self.rnn(packed_embedded)\n",
        "    #packed_outputs is a packed sequence containing all hidden states\n",
        "    #hidden is now from the final non-padded element in the batch\n",
        "\n",
        "    outputs, len_list = nn.utils.rnn.pad_packed_sequence(packed_outputs) #outputs is now a non-packed sequence, all hidden states obtained\n",
        "    #  when the input is a pad token are all zeros\n",
        "\n",
        "    #outputs = [src len, batch size, hid dim * num directions]\n",
        "    #hidden = [n layers * num directions, batch size, hid dim]\n",
        "\n",
        "    #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
        "    #outputs are always from the last layer\n",
        "\n",
        "    #hidden [-2, :, : ] is the last of the forwards RNN\n",
        "    #hidden [-1, :, : ] is the last of the backwards RNN\n",
        "\n",
        "    #initial decoder hidden is final hidden state of the forwards and backwards\n",
        "    #  encoder RNNs fed through a linear layer\n",
        "    hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
        "\n",
        "    #outputs = [src len, batch size, enc hid dim * 2]\n",
        "    #hidden = [batch size, dec hid dim]\n",
        "    return outputs, hidden"
      ],
      "metadata": {
        "id": "kYO9-yg4UpEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attn"
      ],
      "metadata": {
        "id": "vJm8VpgbUsbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "  def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "    super().__init__()\n",
        "    self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
        "    self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
        "\n",
        "  def forward(self, hidden, encoder_outputs, mask):\n",
        "    #hidden = [batch size, dec hid dim]\n",
        "    #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
        "    batch_size = encoder_outputs.shape[1]\n",
        "    src_len = encoder_outputs.shape[0]\n",
        "\n",
        "    #repeat decoder hidden state src_len times\n",
        "    hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)  #hidden = [batch size, src len, dec hid dim]\n",
        "    encoder_outputs = encoder_outputs.permute(1, 0, 2)  #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
        "    energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) #energy = [batch size, src len, dec hid dim]\n",
        "    # eij = a(si−1, hj )\n",
        "\n",
        "    attention = self.v(energy).squeeze(2) #attention = [batch size, src len]\n",
        "    attention = attention.masked_fill(mask == 0, -1e10)\n",
        "    return F.softmax(attention, dim = 1)"
      ],
      "metadata": {
        "id": "hthEkeIXU1lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder"
      ],
      "metadata": {
        "id": "7M3M0cKNUsfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
        "    super().__init__()\n",
        "    self.output_dim = output_dim\n",
        "    self.attention = attention\n",
        "    self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "    self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
        "    self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, input, hidden, encoder_outputs, mask):\n",
        "    #input = [batch size]\n",
        "    #hidden = [batch size, dec hid dim]\n",
        "    #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
        "    #mask = [batch size, src len]\n",
        "    input = input.unsqueeze(0)  #input = [1, batch size]\n",
        "    embedded = self.dropout(self.embedding(input))  #embedded = [1, batch size, emb dim]\n",
        "\n",
        "    a = self.attention(hidden, encoder_outputs, mask) #a = [batch size, src len]\n",
        "    a = a.unsqueeze(1)  #a = [batch size, 1, src len]\n",
        "\n",
        "    encoder_outputs = encoder_outputs.permute(1, 0, 2)  #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
        "\n",
        "    weighted = torch.bmm(a, encoder_outputs)  #weighted = [batch size, 1, enc hid dim * 2]\n",
        "    weighted = weighted.permute(1, 0, 2)  #weighted = [1, batch size, enc hid dim * 2]\n",
        "\n",
        "    rnn_input = torch.cat((embedded, weighted), dim = 2)  #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n",
        "\n",
        "    output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
        "    #output = [seq len, batch size, dec hid dim * n directions]\n",
        "    #hidden = [n layers * n directions, batch size, dec hid dim]\n",
        "\n",
        "    #seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
        "    #output = [1, batch size, dec hid dim]\n",
        "    #hidden = [1, batch size, dec hid dim]\n",
        "    #this also means that output == hidden\n",
        "    assert (output == hidden).all()\n",
        "\n",
        "    embedded = embedded.squeeze(0)\n",
        "    output = output.squeeze(0)\n",
        "    weighted = weighted.squeeze(0)\n",
        "\n",
        "    prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))  #prediction = [batch size, output dim]\n",
        "    return prediction, hidden.squeeze(0), a.squeeze(1)"
      ],
      "metadata": {
        "id": "JVIADpjwU48J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Seq2Seq"
      ],
      "metadata": {
        "id": "WwSO5W3KUsjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, encoder, decoder, src_pad_idx, device):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.src_pad_idx = src_pad_idx\n",
        "    self.device = device\n",
        "\n",
        "  def create_mask(self, src):\n",
        "    mask = (src != self.src_pad_idx).permute(1, 0)\n",
        "    return mask\n",
        "\n",
        "  def forward(self, datas, criterion=None, teacher_forcing_ratio = 0.5):\n",
        "    #src = [src len, batch size]\n",
        "    #src_len = [batch size]\n",
        "    #trg = [trg len, batch size]\n",
        "    #trg_len = [batch size]\n",
        "    #teacher_forcing_ratio is probability of using trg to be input else prev output to be input for next prediction.\n",
        "    (src, src_len), (trg, _) = datas\n",
        "    batch_size = src.shape[1]\n",
        "    trg_len = trg.shape[0]\n",
        "    trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "    # SORT\n",
        "    sort_ids, unsort_ids = self.sort_by_sent_len(src_len)\n",
        "    src, src_len, trg = src[:, sort_ids], src_len[sort_ids], trg[:, sort_ids]\n",
        "\n",
        "    #tensor to store decoder outputs\n",
        "    outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "\n",
        "    #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
        "    #hidden is the final forward and backward hidden states, passed through a linear layer\n",
        "    encoder_outputs, hidden = self.encoder(src, src_len)\n",
        "\n",
        "    #first input to the decoder is the <sos> tokens\n",
        "    input = trg[0,:]\n",
        "\n",
        "    mask = self.create_mask(src)  #mask = [batch size, src len]\n",
        "\n",
        "    for t in range(1, trg_len):\n",
        "      #insert input token embedding, previous hidden state, all encoder hidden states and mask\n",
        "      #receive output tensor (predictions) and new hidden state\n",
        "      output, hidden, _ = self.decoder(input, hidden, encoder_outputs, mask)\n",
        "\n",
        "      #place predictions in a tensor holding predictions for each token\n",
        "      outputs[t] = output\n",
        "\n",
        "      #if teacher forcing, use actual next token as next input. Else, use predicted token\n",
        "      input = trg[t] if random.random() < teacher_forcing_ratio else output.argmax(1)\n",
        "\n",
        "    if criterion != None:\n",
        "      loss = self.compute_loss(outputs, trg, criterion)\n",
        "      return loss, outputs[:, unsort_ids, :]\n",
        "    return outputs[:, unsort_ids, :]\n",
        "\n",
        "  def compute_loss(self, output, trg, criterion):\n",
        "    #output = (trg_len, batch_size, trg_vocab_size)\n",
        "    #trg = [trg len, batch size]\n",
        "    output = output[1:].view(-1, output.shape[-1])  #output = [(trg len - 1) * batch size, output dim]\n",
        "    trg = trg[1:].view(-1)  #trg = [(trg len - 1) * batch size]\n",
        "    loss = criterion(output, trg)\n",
        "    return loss\n",
        "\n",
        "  # NEWLY ADDED ##########################\n",
        "  def sort_by_sent_len(self, sent_len):\n",
        "    _, sort_ids = sent_len.sort(descending=True)\n",
        "    unsort_ids = sort_ids.argsort()\n",
        "    return sort_ids, unsort_ids\n",
        "  # END ADDED ############################"
      ],
      "metadata": {
        "id": "Pyz0ZITCU9r5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pivot model (update)"
      ],
      "metadata": {
        "id": "fdjE61pw7CBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PivotSeq2Seq(nn.Module):\n",
        "  def __init__(self, models: list, fields: list, device, alpha=1.1, lamda=0.75):\n",
        "    super().__init__()\n",
        "    self.num_model = len(models)\n",
        "    self.fields = fields\n",
        "    self.num_field = len(fields)\n",
        "    self.device = device\n",
        "    self.alpha = alpha\n",
        "    self.lamda = lamda\n",
        "\n",
        "    for i in range(self.num_model):\n",
        "      self.add_module(f'model_{i}', models[i])\n",
        "\n",
        "    assert len(models)+1 == len(fields), f\"Not enough Fields for models: num_field={len(fields)} != {len(models)+1}\"\n",
        "\n",
        "  def forward(self, datas: list, criterions=None, teacher_forcing_ratio=0.5):\n",
        "    '''\n",
        "    datas: list of data: [(src, src_len), (piv1, piv_len1), ... , (pivM, piv_lenM), (trg, trg_len)] given M models\n",
        "      src = [src len, batch_size]\n",
        "      src_len = [batch_size]\n",
        "      ...\n",
        "      trg = [trg len, batch_size]\n",
        "      trg_len = [batch_size]\n",
        "    criterions: list of criterion for each model\n",
        "    '''\n",
        "    if criterions != None:\n",
        "      loss_list, output_list = self.run(datas, criterions, teacher_forcing_ratio)\n",
        "      total_loss = self.compute_loss(loss_list)\n",
        "      return total_loss, output_list[-1]\n",
        "    else:\n",
        "      criterions = [None for _ in range(self.num_model)]\n",
        "      _, output_list = self.run(datas, criterions, teacher_forcing_ratio)\n",
        "      return output_list[-1]\n",
        "\n",
        "  def run(self, datas, criterions, teacher_forcing_ratio):\n",
        "    assert self.num_model+1 == len(datas), f\"Not enough datas for models: data_len={len(datas)} != {self.num_model+1}\"\n",
        "    assert self.num_model == len(criterions), f'Criterions must have for each model: num_criterion={len(criterions)} != {self.num_model}'\n",
        "\n",
        "    output_list, loss_list = [], []\n",
        "    for i in range(self.num_model):\n",
        "      isForceOn = True if i==0 else random.random() < teacher_forcing_ratio # 1st model must always use src\n",
        "\n",
        "      # GET NEW INPUT\n",
        "      src, src_len = datas[i] if isForceOn else self.process_output(output_list[-1], self.fields[i+1])\n",
        "      trg, trg_len = datas[i+1]\n",
        "\n",
        "      # FORWARD MODEL\n",
        "      model = getattr(self, f'model_{i}') # Seq2Seq model already sort src by src_len in forward\n",
        "      data = [(src, src_len), (trg, trg_len)]\n",
        "      criterion = criterions[i]\n",
        "      output = model(data, criterion, 0 if criterion==None else teacher_forcing_ratio)\n",
        "\n",
        "      if criterion == None:\n",
        "        output_list.append(output)\n",
        "      else:\n",
        "        assert len(output) == 2, 'With criterion, model should return loss & prediction'\n",
        "        loss, out = output\n",
        "        loss_list.append(loss)\n",
        "        output_list.append(out)\n",
        "\n",
        "    return loss_list, output_list\n",
        "\n",
        "  def compute_loss(self, loss_list):\n",
        "    total_loss = 0.0\n",
        "    for i in range(len(loss_list) - 1): # except final output\n",
        "      total_loss += loss_list[i]\n",
        "    total_loss += self.alpha*loss_list[-1]\n",
        "    return total_loss + self.lamda*self.compute_embed_loss()\n",
        "\n",
        "  def compute_embed_loss(self):\n",
        "    embed_loss = 0.0\n",
        "    for i in range(1, self.num_model):\n",
        "      model1 = getattr(self, f'model_{i-1}')\n",
        "      model2 = getattr(self, f'model_{i}')\n",
        "      embed_loss += torch.sum(F.pairwise_distance(model1.decoder.embedding.weight, model2.encoder.embedding.weight, p=2))\n",
        "    return embed_loss\n",
        "\n",
        "  def sort_by_src_len(self, piv, piv_len, datas): # piv = [piv_len, batch_size]\n",
        "    piv_len, sorted_ids = piv_len.sort(descending=True)\n",
        "    sorted_datas = [(sent[:, sorted_ids], sent_len[sorted_ids]) for (sent, sent_len) in datas]\n",
        "    return piv[:, sorted_ids], piv_len, sorted_datas  # piv sorted along batch_size\n",
        "\n",
        "  def process_output(self, output, piv_field):\n",
        "    # output = [trg len, batch size, output dim]\n",
        "    # trg = [trg len, batch size]\n",
        "    # Process output1 to be input for model2\n",
        "    seq_len, N, _ = output.shape\n",
        "    tmp_out = output.argmax(2)  # tmp_out = [seq_len, batch_size]\n",
        "    # re-create pivot as src for model2\n",
        "    piv = torch.zeros_like(tmp_out).type(torch.long).to(output.device)\n",
        "    piv[0, :] = torch.full_like(piv[0, :], piv_field.vocab.stoi[piv_field.init_token])  # fill all first idx with sos_token\n",
        "\n",
        "    for i in range(1, seq_len):  # for each i in seq_len\n",
        "      # if tmp_out's prev is eos_token, replace w/ pad_token, else current value\n",
        "      eos_mask = (tmp_out[i-1, :] == piv_field.vocab.stoi[piv_field.eos_token])\n",
        "      piv[i, :] = torch.where(eos_mask, piv_field.vocab.stoi[piv_field.pad_token], tmp_out[i, :])\n",
        "      # if piv's prev is pad_token, replace w/ pad_token, else current value\n",
        "      pad_mask = (piv[i-1, :] == piv_field.vocab.stoi[piv_field.pad_token])\n",
        "      piv[i, :] = torch.where(pad_mask, piv_field.vocab.stoi[piv_field.pad_token], piv[i, :])\n",
        "\n",
        "    # Trim down extra pad tokens\n",
        "    tensor_list = [piv[i] for i in range(seq_len) if not all(piv[i] == piv_field.vocab.stoi[piv_field.pad_token])]  # tensor_list = [new_seq_len, batch_size]\n",
        "    piv = torch.stack([x for x in tensor_list], dim=0).type(torch.long).to(output.device)\n",
        "    assert not all(piv[-1] == piv_field.vocab.stoi[piv_field.pad_token]), 'Not completely trim down tensor'\n",
        "\n",
        "    # get seq_id + eos_tok id of each sequence\n",
        "    piv_ids, eos_ids = (piv.permute(1, 0) == piv_field.vocab.stoi[piv_field.eos_token]).nonzero(as_tuple=True)  # piv_len = [N]\n",
        "    piv_len = torch.full_like(piv[0], seq_len).type(torch.long)  # init w/ longest seq\n",
        "    piv_len[piv_ids] = eos_ids + 1 # seq_len = eos_tok + 1\n",
        "\n",
        "    return piv, piv_len"
      ],
      "metadata": {
        "id": "kdQAxGHj7E2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Triangulate model"
      ],
      "metadata": {
        "id": "KgwBXdjq3IkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TriangSeq2Seq(nn.Module):\n",
        "  def __init__(self, models: list, output_dim, device, alpha=1.1, method='weighted', train_backbone=True):\n",
        "    # output_dim = trg vocab size\n",
        "    super().__init__()\n",
        "    self.num_model = len(models)\n",
        "    self.output_dim = output_dim\n",
        "    self.device = device\n",
        "    self.alpha = alpha\n",
        "    self.method = method\n",
        "    self.train_backbone = train_backbone\n",
        "    if method=='weighted':\n",
        "      self.head = nn.Sequential(\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(output_dim*self.num_model, output_dim)\n",
        "      )\n",
        "    elif method=='weighted_1':\n",
        "      self.head = nn.Sequential(\n",
        "          nn.Linear(self.num_model, self.num_model*2),\n",
        "          nn.ReLU(),\n",
        "          nn.Dropout(0.2),\n",
        "          nn.Linear(self.num_model*2, self.num_model*2),\n",
        "          nn.ReLU(),\n",
        "          nn.Dropout(0.2),\n",
        "          nn.Linear(self.num_model*2, 1),\n",
        "      )\n",
        "    elif method=='weighted_2':\n",
        "      self.head = nn.Sequential(\n",
        "          nn.Linear(self.num_model, self.num_model),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(self.num_model, self.num_model),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(self.num_model, self.num_model),\n",
        "      )\n",
        "    for i in range(self.num_model):\n",
        "      for param in models[i].parameters():\n",
        "        param.requires_grad = train_backbone\n",
        "      self.add_module(f'model_{i}', models[i])\n",
        "\n",
        "  def forward(self, datas: dict, criterions=None, teacher_forcing_ratio=0.5):\n",
        "    '''\n",
        "    datas: dict of data:\n",
        "      {\"model_0\": (src, src_len, trg, trg_len), \"model_1\": [(src, src_len), (piv, piv_len), (trg, trg_len)], ..., \"TRG\": (trg, trg_len)}\n",
        "      src = [src len, batch size]\n",
        "      src_len = [batch size]\n",
        "    criterions: dict of criterions\n",
        "      {\"model_0\": criterion_0, \"model_1\": criterion_1, ..., \"TRG\": criterion_M}\n",
        "    '''\n",
        "    if criterions != None:\n",
        "      loss_list, output_list = self.run(datas, criterions, teacher_forcing_ratio)\n",
        "      final_out = self.get_final_pred(output_list)\n",
        "      total_loss = self.alpha*self.compute_final_pred_loss(final_out, datas[\"TRG\"], criterions[\"TRG\"]) + self.compute_submodels_loss(loss_list)\n",
        "      return total_loss, final_out\n",
        "    else:\n",
        "      criterions = {f'model_{i}':None for i in range(self.num_model)}\n",
        "      criterions['TRG'] = None\n",
        "      loss_list, output_list = self.run(datas, criterions, teacher_forcing_ratio)\n",
        "      final_out = self.get_final_pred(output_list)\n",
        "      return final_out\n",
        "\n",
        "  def run(self, datas, criterions, teacher_forcing_ratio):\n",
        "    assert self.num_model+1 == len(datas), f\"Not enough datas for models: data_len={len(datas)} != {self.num_model+1}\"  # include 'TRG'\n",
        "    assert self.num_model+1 == len(criterions), f'Criterions must have for each model: num_criterion={len(criterions)} != {self.num_model+1}' # include 'TRG'\n",
        "\n",
        "    output_list = []\n",
        "    loss_list = []\n",
        "    for i in range(self.num_model):\n",
        "      data = datas[f'model_{i}']\n",
        "      model = getattr(self, f'model_{i}')\n",
        "      criterion = criterions[f'model_{i}']\n",
        "      output = model(data, criterion, 0 if criterion==None else teacher_forcing_ratio)\n",
        "\n",
        "      if criterion == None:\n",
        "        output_list.append(output)\n",
        "      else:\n",
        "        assert len(output) == 2, 'With criterion, model should return loss & prediction'\n",
        "        loss_list.append(output[0])\n",
        "        output_list.append(output[1])\n",
        "\n",
        "    return loss_list, output_list\n",
        "\n",
        "  def compute_submodels_loss(self, loss_list):\n",
        "    total_loss = 0.0\n",
        "    for loss in loss_list:\n",
        "      total_loss += loss\n",
        "    return total_loss\n",
        "\n",
        "  def compute_final_pred_loss(self, output, data, criterion):\n",
        "    #output = (trg_len, batch_size, trg_vocab_size)\n",
        "    #data = [trg, trg_len]  # trg.shape = [seq_len, batch_size]\n",
        "    trg, _ = data\n",
        "    output = output[1:].reshape(-1, output.shape[-1])  #output = [(trg len - 1) * batch size, output dim]\n",
        "    trg = trg[1:].reshape(-1)  #trg = [(trg len - 1) * batch size]\n",
        "    loss = criterion(output, trg)\n",
        "    return loss\n",
        "\n",
        "  def get_final_pred(self, output_list):  # output_list[0] shape = [seq_len, N, out_dim]\n",
        "    # assert all([output_list[i].shape == output_list[i-1].shape for i in range(1, len(output_list))]), 'all outputs must match shape [seq_len, N, out_dim]'\n",
        "    seq_len, N, out_dim = output_list[0].shape\n",
        "    if self.method=='weighted':\n",
        "      linear_in = torch.cat([out for out in output_list], dim=-1) # linear_in = [seq_len, N, out_dim * num_model]. Note that num_model = len(output_list)\n",
        "      final_out = self.head(linear_in)  # final_out = [seq_len, N, out_dim]\n",
        "      return final_out\n",
        "    elif self.method=='weighted_1':\n",
        "      output_list = [out.permute(1, 0, 2).reshape(N, -1) for out in output_list]  # [N, seq_len, out_dim] --> [N, seq_len*out_dim]\n",
        "      final_out = self.head(torch.stack(output_list, dim=-1)).squeeze(-1)  # [N, seq_len*out_dim, num_model] --> [N, seq_len*out_dim, 1] --> [N, seq_len*out_dim]\n",
        "      return final_out.reshape(N, seq_len, out_dim).permute(1, 0, 2) # [N, seq_len, out_dim] --> [seq_len, N, out_dim]\n",
        "    elif self.method=='weighted_2':\n",
        "      output_list = [out.permute(1, 0, 2).argmax(-1) for out in output_list]  # [N, seq_len, out_dim] --> [N, seq_len]\n",
        "      inp = torch.stack(output_list, dim=-1).type(torch.float32)  # [N, seq_len, num_model]\n",
        "      out = self.head(inp)\n",
        "      finalout = torch.gather(inp, -1, out.argmax(-1, True)).squeeze(-1).type(torch.long)\n",
        "      finalout = F.one_hot(finalout, num_classes=self.output_dim)\n",
        "      return finalout.permute(1, 0, 2).type(torch.float32)\n",
        "    elif self.method=='max': # get max along seq_len b/w all outputs\n",
        "      output_list = [out.permute(1, 0, 2).reshape(N, -1) for out in output_list]  # [N, seq_len, out_dim] --> [N, seq_len*out_dim]\n",
        "      final_out, _ = torch.max(torch.stack(output_list, dim=-1), dim=-1)  # [N, seq_len*out_dim, num_model] --> [N, seq_len*out_dim]\n",
        "      return final_out.reshape(N, seq_len, out_dim).permute(1, 0, 2) # [N, seq_len, out_dim] --> [seq_len, N, out_dim]\n",
        "    else:\n",
        "      return output_list[0]"
      ],
      "metadata": {
        "id": "FKCumOEK3T6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train func"
      ],
      "metadata": {
        "id": "2cDqYW-LTnGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_trainlog(data: list, filename: str=f'{DIR_PATH}/training_log.txt'):\n",
        "  ''' Update training log w/ new losses\n",
        "  Args:\n",
        "      data (List): a list of infor for many epochs as tuple, each tuple has model_name, loss, etc.\n",
        "      filename (String): path + file_name\n",
        "  Return:\n",
        "      None: new data is appended into train-log\n",
        "  '''\n",
        "  with open(filename, 'a') as f: # save\n",
        "    for epoch in data:\n",
        "      f.write(','.join(epoch))\n",
        "      f.write(\"\\n\")\n",
        "  print('update_trainlog SUCCESS')\n",
        "  return []\n",
        "def init_weights(m):\n",
        "  for name, param in m.named_parameters():\n",
        "    if 'weight' in name:\n",
        "      nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "    else:\n",
        "      nn.init.constant_(param.data, 0)\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "metadata": {
        "id": "iaocGp1dS3RJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Seq2Seq"
      ],
      "metadata": {
        "id": "H9Z6RxOnzLa9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trainSeq2Seq(model, iterator, optimizer, criterion, clip):\n",
        "  model.train()\n",
        "  epoch_loss = 0.0\n",
        "  for batch in tqdm(iterator):\n",
        "    optimizer.zero_grad()\n",
        "    datas = [batch.en, batch.fr]\n",
        "    loss, _ = model(datas, criterion, 0.5)\n",
        "\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "    optimizer.step()\n",
        "    epoch_loss += loss.item()\n",
        "  return epoch_loss / len(iterator)\n",
        "\n",
        "def evaluateSeq2Seq(model, iterator, criterion):\n",
        "  model.eval()\n",
        "  epoch_loss = 0.0\n",
        "  with torch.no_grad():\n",
        "    for batch in tqdm(iterator):\n",
        "      datas = [batch.en, batch.fr]\n",
        "      loss, _ = model(datas, criterion, 0) # turn off teacher forcing\n",
        "      epoch_loss += loss.item()\n",
        "    return epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "PyiHo_rj7b3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pivot"
      ],
      "metadata": {
        "id": "Jfky7439zNhN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trainPivot(model, iterator, optimizer, criterions, clip):\n",
        "  model.train()\n",
        "  epoch_loss = 0.0\n",
        "  for batch in tqdm(iterator):\n",
        "    optimizer.zero_grad()\n",
        "    model_inputs = [batch.en, vars(batch)['it'], batch.fr]\n",
        "    loss, _ = model(model_inputs, criterions, 0.5)\n",
        "\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "    optimizer.step()\n",
        "    epoch_loss += loss.item()\n",
        "  return epoch_loss / len(iterator)\n",
        "\n",
        "def evaluatePivot(model, iterator, criterions):\n",
        "  model.eval()\n",
        "  epoch_loss = 0.0\n",
        "  with torch.no_grad():\n",
        "    for batch in tqdm(iterator):\n",
        "      model_inputs = [batch.en, vars(batch)['it'], batch.fr]\n",
        "      loss, _ = model(model_inputs, criterions, 0)\n",
        "      epoch_loss += loss.item()\n",
        "    return epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "Au60zQ85n8No"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Triangulate"
      ],
      "metadata": {
        "id": "ARfI-GykzP9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for triangulate model\n",
        "def trainTriang(model, iterator, optimizer, criterions, clip):\n",
        "  model.train()\n",
        "  epoch_loss = 0.0\n",
        "  for batch in tqdm(iterator):\n",
        "    optimizer.zero_grad()\n",
        "    model_inputs = {\n",
        "        'model_0': [batch.en, batch.fr],\n",
        "        'model_1': [batch.en, vars(batch)['it'], batch.fr],\n",
        "        'TRG': batch.fr\n",
        "    }\n",
        "    loss, _ = model(model_inputs, criterions, 0.5)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "    optimizer.step()\n",
        "    epoch_loss += loss.item()\n",
        "  return epoch_loss / len(iterator)\n",
        "\n",
        "def evaluateTriang(model, iterator, criterions):\n",
        "  model.eval()\n",
        "  epoch_loss = 0.0\n",
        "  with torch.no_grad():\n",
        "    for batch in tqdm(iterator):\n",
        "      model_inputs = {\n",
        "        'model_0': [batch.en, batch.fr],\n",
        "        'model_1': [batch.en, vars(batch)['it'], batch.fr],\n",
        "        'TRG': batch.fr\n",
        "      }\n",
        "      loss, _ = model(model_inputs, criterions, 0)\n",
        "      epoch_loss += loss.item()\n",
        "    return epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "TsX8opIkn7y7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "MYFp2rpQUEbX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train model"
      ],
      "metadata": {
        "id": "t0QaCiat3eVK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Seq2Seq"
      ],
      "metadata": {
        "id": "VKGgTuL4tThr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For 2 langs\n",
        "EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "DROPOUT = 0.5\n",
        "\n",
        "INPUT_DIM = len(EN_FIELD.vocab)\n",
        "OUTPUT_DIM = len(FR_FIELD.vocab)\n",
        "SRC_PAD_IDX = EN_FIELD.vocab.stoi[EN_FIELD.pad_token]\n",
        "\n",
        "attn = Attention(HID_DIM, HID_DIM)\n",
        "enc = Encoder(INPUT_DIM, EMB_DIM, HID_DIM, HID_DIM, DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM, HID_DIM, DROPOUT, attn)\n",
        "\n",
        "model = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)"
      ],
      "metadata": {
        "id": "9b_1FE6ZlpEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pivot"
      ],
      "metadata": {
        "id": "TmXsuwuctVw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "DROPOUT = 0.5\n",
        "\n",
        "INPUT_DIM = len(EN_FIELD.vocab)\n",
        "PIV_DIM = len(ES_FIELD.vocab)\n",
        "OUTPUT_DIM = len(FR_FIELD.vocab)\n",
        "\n",
        "SRC_PAD_IDX = EN_FIELD.vocab.stoi[EN_FIELD.pad_token]\n",
        "attn1 = Attention(HID_DIM, HID_DIM)\n",
        "enc1 = Encoder(INPUT_DIM, EMB_DIM, HID_DIM, HID_DIM, DROPOUT)\n",
        "dec1 = Decoder(PIV_DIM, EMB_DIM, HID_DIM, HID_DIM, DROPOUT, attn1)\n",
        "model1 = Seq2Seq(enc1, dec1, SRC_PAD_IDX, device).to(device)\n",
        "\n",
        "PIV_PAD_IDX = ES_FIELD.vocab.stoi[ES_FIELD.pad_token]\n",
        "attn2 = Attention(HID_DIM, HID_DIM)\n",
        "enc2 = Encoder(PIV_DIM, EMB_DIM, HID_DIM, HID_DIM, DROPOUT)\n",
        "dec2 = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM, HID_DIM, DROPOUT, attn2)\n",
        "model2 = Seq2Seq(enc2, dec2, PIV_PAD_IDX, device).to(device)\n",
        "\n",
        "models = [model1, model2]\n",
        "fields = [EN_FIELD, ES_FIELD, FR_FIELD]\n",
        "model = PivotSeq2Seq(models, fields, device).to(device)"
      ],
      "metadata": {
        "id": "zhpJPc2DaIHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Triangulate"
      ],
      "metadata": {
        "id": "ifgtDQwvtRcw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "DROPOUT = 0.5\n",
        "\n",
        "# DIRECT\n",
        "INPUT_DIM = len(EN_FIELD.vocab)\n",
        "OUTPUT_DIM = len(FR_FIELD.vocab)\n",
        "SRC_PAD_IDX = EN_FIELD.vocab.stoi[EN_FIELD.pad_token]\n",
        "\n",
        "attn = Attention(HID_DIM, HID_DIM)\n",
        "enc = Encoder(INPUT_DIM, EMB_DIM, HID_DIM, HID_DIM, DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM, HID_DIM, DROPOUT, attn)\n",
        "\n",
        "model_direct = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)\n",
        "model_direct.load_state_dict(torch.load(f'{DIR_PATH}/seq2seq-EnFr-1.pt')['model_state_dict'])\n",
        "\n",
        "# PIVOT\n",
        "INPUT_DIM = len(EN_FIELD.vocab)\n",
        "PIV_DIM = len(FIELD_DICT['it'].vocab)\n",
        "OUTPUT_DIM = len(FR_FIELD.vocab)\n",
        "\n",
        "SRC_PAD_IDX = EN_FIELD.vocab.stoi[EN_FIELD.pad_token]\n",
        "attn1 = Attention(HID_DIM, HID_DIM)\n",
        "enc1 = Encoder(INPUT_DIM, EMB_DIM, HID_DIM, HID_DIM, DROPOUT)\n",
        "dec1 = Decoder(PIV_DIM, EMB_DIM, HID_DIM, HID_DIM, DROPOUT, attn1)\n",
        "model1 = Seq2Seq(enc1, dec1, SRC_PAD_IDX, device).to(device)\n",
        "\n",
        "PIV_PAD_IDX = FIELD_DICT['it'].vocab.stoi[FIELD_DICT['it'].pad_token]\n",
        "attn2 = Attention(HID_DIM, HID_DIM)\n",
        "enc2 = Encoder(PIV_DIM, EMB_DIM, HID_DIM, HID_DIM, DROPOUT)\n",
        "dec2 = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM, HID_DIM, DROPOUT, attn2)\n",
        "model2 = Seq2Seq(enc2, dec2, PIV_PAD_IDX, device).to(device)\n",
        "\n",
        "models = [model1, model2]\n",
        "fields = [EN_FIELD, FIELD_DICT['it'], FR_FIELD]\n",
        "model_piv = PivotSeq2Seq(models, fields, device).to(device)\n",
        "model_piv.load_state_dict(torch.load(f'{DIR_PATH}/piv-EnItFr.pt')['model_state_dict'])\n",
        "\n",
        "# TRIANGULATE\n",
        "models = [model_direct, model_piv]\n",
        "model = TriangSeq2Seq(models, OUTPUT_DIM, device, alpha=1.1, method='weighted_1', train_backbone=True).to(device)"
      ],
      "metadata": {
        "id": "6bCDFxzKtZPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train loop"
      ],
      "metadata": {
        "id": "ZhCTMvF2tbQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if isinstance(model, Seq2Seq) or isinstance(model, PivotSeq2Seq) or (isinstance(model, TriangSeq2Seq) and model.train_backbone):\n",
        "#   model.apply(init_weights) # remove if backbone models are freezed\n",
        "#   print('init_weights success')\n",
        "# elif isinstance(model, TriangSeq2Seq) and not model.train_backbone:\n",
        "#   model.head.apply(init_weights)\n",
        "#   print('init_weights head ONLY success')\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "metadata": {
        "id": "M7CeajzjlzWj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85c1b3ca-2df1-4359-f845-f9297fc724f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 87,319,499 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion2 = nn.CrossEntropyLoss(ignore_index = FR_FIELD.vocab.stoi[FR_FIELD.pad_token])\n",
        "\n",
        "if isinstance(model, PivotSeq2Seq) or isinstance(model, TriangSeq2Seq):\n",
        "  criterion1 = nn.CrossEntropyLoss(ignore_index = FIELD_DICT['it'].vocab.stoi[FIELD_DICT['it'].pad_token])\n",
        "  criterion3 = [criterion1, criterion2]\n",
        "  print('Created criterion for piv')\n",
        "\n",
        "if isinstance(model, TriangSeq2Seq):\n",
        "  criterion4 = {\n",
        "      'model_0': criterion2,\n",
        "      'model_1': criterion3,\n",
        "      'TRG': nn.CrossEntropyLoss(ignore_index = FR_FIELD.vocab.stoi[FR_FIELD.pad_token])\n",
        "  }\n",
        "  print('Created criterion for triang')"
      ],
      "metadata": {
        "id": "V_5TFOZsmPsH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f35b073-8351-44aa-8472-23adebbbb1d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created criterion for piv\n",
            "Created criterion for triang\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "N_EPOCHS = 5\n",
        "CLIP = 1\n",
        "best_valid_loss = float('inf')\n",
        "best_train_loss = float('inf')\n",
        "model_name = 'triang-EnFr-EnItFr-w1-trainall.pt'\n",
        "train_log = []"
      ],
      "metadata": {
        "id": "FIYc-s0FTPRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LR = 0.001\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "# scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[2, 5, 7, 8, 9], gamma=2.0/3.0) PIV TRAINING w/ LR = 0.0012\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[1, 2, 4], gamma=1.0/3.0)"
      ],
      "metadata": {
        "id": "edsBDaCEmQD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_path = f'{DIR_PATH}/{model_name}'\n",
        "# ckpt = torch.load(model_path)\n",
        "# model.load_state_dict(ckpt['model_state_dict'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8YS6scgUp-L",
        "outputId": "43a42385-f2cb-4941-9d3c-d2f5b9bdcb6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(N_EPOCHS):\n",
        "  if isinstance(model, Seq2Seq):\n",
        "    train_loss = trainSeq2Seq(model, train_iterator, optimizer, criterion2, CLIP)\n",
        "    valid_loss = evaluateSeq2Seq(model, valid_iterator, criterion2)\n",
        "  elif isinstance(model, PivotSeq2Seq):\n",
        "    train_loss = trainPivot(model, train_iterator, optimizer, criterion3, CLIP)\n",
        "    valid_loss = evaluatePivot(model, valid_iterator, criterion3)\n",
        "  elif isinstance(model, TriangSeq2Seq):\n",
        "    train_loss = trainTriang(model, train_iterator, optimizer, criterion4, CLIP)\n",
        "    valid_loss = evaluateTriang(model, valid_iterator, criterion4)\n",
        "  else: raise Exception('Model type is unknown')\n",
        "\n",
        "  epoch_info = [model_name, dataname, scheduler.get_last_lr()[0], BATCH_SIZE, HID_DIM, DROPOUT, epoch, N_EPOCHS, train_loss, valid_loss]\n",
        "  train_log.append([str(info) for info in epoch_info])\n",
        "  train_log = update_trainlog(train_log, f'{DIR_PATH}/training_log-OLD.txt')\n",
        "\n",
        "  scheduler.step()\n",
        "\n",
        "  # if train_loss < best_train_loss or valid_loss < best_valid_loss:\n",
        "  if valid_loss < best_valid_loss:\n",
        "    best_train_loss = train_loss\n",
        "    best_valid_loss = valid_loss\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict()\n",
        "    }, f'{DIR_PATH}/{model_name}')\n",
        "    print('SAVED MODEL')\n",
        "\n",
        "  print(f'Epoch: {epoch:02} \\t Train Loss: {train_loss:.3f} \\t Val. Loss: {valid_loss:.3f}')"
      ],
      "metadata": {
        "id": "3ngIAH5cldRm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a08121c-f508-476e-ca75-859c775994c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4000/4000 [18:29<00:00,  3.60it/s]\n",
            "100%|██████████| 200/200 [00:29<00:00,  6.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "update_trainlog SUCCESS\n",
            "SAVED MODEL\n",
            "Epoch: 00 \t Train Loss: 20.115 \t Val. Loss: 29.692\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4000/4000 [18:29<00:00,  3.60it/s]\n",
            "100%|██████████| 200/200 [00:29<00:00,  6.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "update_trainlog SUCCESS\n",
            "SAVED MODEL\n",
            "Epoch: 01 \t Train Loss: 11.040 \t Val. Loss: 21.761\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4000/4000 [18:27<00:00,  3.61it/s]\n",
            "100%|██████████| 200/200 [00:29<00:00,  6.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "update_trainlog SUCCESS\n",
            "SAVED MODEL\n",
            "Epoch: 02 \t Train Loss: 7.937 \t Val. Loss: 19.412\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4000/4000 [18:26<00:00,  3.62it/s]\n",
            "100%|██████████| 200/200 [00:29<00:00,  6.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "update_trainlog SUCCESS\n",
            "Epoch: 03 \t Train Loss: 7.719 \t Val. Loss: 19.493\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4000/4000 [18:24<00:00,  3.62it/s]\n",
            "100%|██████████| 200/200 [00:29<00:00,  6.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "update_trainlog SUCCESS\n",
            "SAVED MODEL\n",
            "Epoch: 04 \t Train Loss: 6.722 \t Val. Loss: 18.753\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LR = scheduler.get_last_lr()[0]\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[1], gamma=1.0/3.0)\n",
        "scheduler.get_last_lr()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCYHgGbFdhOv",
        "outputId": "d7d90705-4d70-4ec0-d038-b263c39b823b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3.703703703703703e-05]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(2):\n",
        "  if isinstance(model, Seq2Seq):\n",
        "    train_loss = trainSeq2Seq(model, train_iterator, optimizer, criterion2, CLIP)\n",
        "    valid_loss = evaluateSeq2Seq(model, valid_iterator, criterion2)\n",
        "  elif isinstance(model, PivotSeq2Seq):\n",
        "    train_loss = trainPivot(model, train_iterator, optimizer, criterion3, CLIP)\n",
        "    valid_loss = evaluatePivot(model, valid_iterator, criterion3)\n",
        "  elif isinstance(model, TriangSeq2Seq):\n",
        "    train_loss = trainTriang(model, train_iterator, optimizer, criterion4, CLIP)\n",
        "    valid_loss = evaluateTriang(model, valid_iterator, criterion4)\n",
        "  else: raise Exception('Model type is unknown')\n",
        "\n",
        "  epoch_info = [model_name, dataname, scheduler.get_last_lr()[0], BATCH_SIZE, HID_DIM, DROPOUT, epoch, N_EPOCHS, train_loss, valid_loss]\n",
        "  train_log.append([str(info) for info in epoch_info])\n",
        "  train_log = update_trainlog(train_log, f'{DIR_PATH}/training_log-OLD.txt')\n",
        "\n",
        "  scheduler.step()\n",
        "\n",
        "  # if train_loss < best_train_loss or valid_loss < best_valid_loss:\n",
        "  if valid_loss < best_valid_loss:\n",
        "    best_train_loss = train_loss\n",
        "    best_valid_loss = valid_loss\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict()\n",
        "    }, f'{DIR_PATH}/{model_name}')\n",
        "    print('SAVED MODEL')\n",
        "\n",
        "  print(f'Epoch: {epoch:02} \\t Train Loss: {train_loss:.3f} \\t Val. Loss: {valid_loss:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQ6j0IzqdvJz",
        "outputId": "bc355b6e-bdbb-41bd-fc74-ec9b6425bbac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 22%|██▏       | 882/4000 [04:02<18:23,  2.83it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eval"
      ],
      "metadata": {
        "id": "jRFofx7s1jJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in test_iterator:\n",
        "  break\n",
        "batch.fields"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qS7TxmFB40s9",
        "outputId": "86ef0fdc-aae2-4de1-ad17-ecd3a0df0dc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['en', 'fr', 'it'])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = f'{DIR_PATH}/piv-EnEsFr.pt'\n",
        "ckpt = torch.load(model_path)"
      ],
      "metadata": {
        "id": "g-RDSB1LkKys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
        "# scheduler.load_state_dict(ckpt['scheduler_state_dict'])\n",
        "model.load_state_dict(ckpt['model_state_dict']) # strict=False if some dimensions are different"
      ],
      "metadata": {
        "id": "PwZxT4MZ8tV3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48669f27-8a3f-46ea-e06c-38138012da98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_loss = evaluateTriang(model, valid_iterator, criterions)\n",
        "print(f'Test Loss: {valid_loss:.3f}')"
      ],
      "metadata": {
        "id": "blKOkBAe1m5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "cbS1GfWq2DmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sent2tensor(src_field, trg_field, device, max_len, sentence=None):\n",
        "  if sentence != None:\n",
        "    if isinstance(sentence, str):\n",
        "      tokens = tokenize_en(sentence)\n",
        "    else:\n",
        "      tokens = [token.lower() for token in sentence]\n",
        "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
        "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)  # [seq_len, N] w/ N=1 for batch\n",
        "    src_len_tensor = torch.LongTensor([len(src_indexes)]).to(device)\n",
        "    return src_tensor, src_len_tensor\n",
        "\n",
        "  trg_tensor = torch.LongTensor([trg_field.vocab.stoi[trg_field.init_token]] + [0 for i in range(1, max_len)]).view(-1, 1).to(device) # [seq_len, 1]\n",
        "  trg_len_tensor = torch.LongTensor([max_len]).to(device)\n",
        "  return (trg_tensor, trg_len_tensor)\n",
        "\n",
        "def idx2sent(trg_field, arr):\n",
        "  n_sents = arr.shape[1]  # arr = [seq_len, N]\n",
        "  results = []\n",
        "  for i in range(n_sents):  # for each sent\n",
        "    pred_sent = []\n",
        "    pred = arr[:, i]\n",
        "    for i in pred[1:]:  # for each word\n",
        "      pred_sent.append(trg_field.vocab.itos[i])\n",
        "      if i == trg_field.vocab.stoi[trg_field.eos_token]: break\n",
        "    results.append(pred_sent)\n",
        "  return results"
      ],
      "metadata": {
        "id": "xUMMEYoLaDr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Translate by sent"
      ],
      "metadata": {
        "id": "_3qwJt8Q1qN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sentence(sentence, src_field, trg_field, model, device, max_len):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    # create data\n",
        "    src = sent2tensor(src_field, trg_field, device, max_len, sentence)\n",
        "    trg = sent2tensor(src_field, trg_field, device, max_len)\n",
        "    # get data\n",
        "    if isinstance(model, Seq2Seq):\n",
        "      data = [src, trg]\n",
        "    elif isinstance(model, PivotSeq2Seq):\n",
        "      data = [src] + [trg for _ in range(model.num_model)]\n",
        "    elif isinstance(model, TriangSeq2Seq):\n",
        "      data = {'TRG': trg}\n",
        "      for i in range(model.num_model):\n",
        "        submodel = getattr(model, f'model_{i}')\n",
        "        if isinstance(submodel, Seq2Seq):\n",
        "          data[f'model_{i}'] = [src, trg]\n",
        "        elif isinstance(submodel, PivotSeq2Seq):\n",
        "          data[f'model_{i}'] = [src] + [trg for _ in range(submodel.num_model)]\n",
        "        else: raise Exception('Only support Seq2Seq & PivotSeq2Seq nested inside TriangSeq2Seq')\n",
        "    else: raise Exception('Model type is unknown')\n",
        "    # feed model\n",
        "    output = model(data, None, 0) # output = [trg_len, N, dec_emb_dim] w/ N=1\n",
        "    output = output.argmax(-1).detach().cpu().numpy() # output = [seq_len, N]\n",
        "    results = idx2sent(trg_field, output)\n",
        "    return results"
      ],
      "metadata": {
        "id": "c6d_Ggwz2EcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sentence_seq2seq(sentence, src_field, trg_field, model: Seq2Seq, device, max_len=64):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    # get data\n",
        "    src_tensor, src_len_tensor = sent2tensor(src_field, trg_field, device, max_len, sentence)\n",
        "    trg_tensor, trg_len_tensor = sent2tensor(src_field, trg_field, device, max_len)\n",
        "    data = [(src_tensor, src_len_tensor), (trg_tensor, trg_len_tensor)]\n",
        "    # feed model\n",
        "    output = model(data, None, 0) # output = [trg_len, N, dec_emb_dim] w/ N=1\n",
        "    output = output.argmax(-1).detach().cpu().numpy() # output = [seq_len, N]\n",
        "    results = idx2sent(trg_field, output)\n",
        "    return results\n",
        "\n",
        "def translate_sentence_pivot(sentence, src_field, trg_field, model, device, max_len=64):  # not yet modified\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    # get data\n",
        "    src_tensor, src_len_tensor = sent2tensor(src_field, trg_field, device, max_len, sentence)\n",
        "    trg_tensor, trg_len_tensor = sent2tensor(src_field, trg_field, device, max_len)\n",
        "    data = [(src_tensor, src_len_tensor)] + [(trg_tensor.clone().detach().to(device), trg_len_tensor.clone().detach().to(device)) for _ in range(model.num_model)]\n",
        "    # feed model\n",
        "    output = model(data, None, 0) # output = [trg_len, N, dec_emb_dim]\n",
        "    output = output.argmax(-1).detach().cpu().numpy()\n",
        "    results = idx2sent(trg_field, output)\n",
        "    return results\n",
        "\n",
        "def translate_batch_triang(sentence, src_field, trg_field, model, device, max_len=64):  # not yet complete. modify sentence/sentences, fields base on type of models\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    # get data\n",
        "    src_tensor, src_len_tensor = sent2tensor(src_field, trg_field, device, max_len, sentence)\n",
        "    trg_tensor, trg_len_tensor = sent2tensor(src_field, trg_field, device, max_len)\n",
        "    data = {\n",
        "      'model_0': [(src_tensor, src_len_tensor), (trg_tensor, trg_len_tensor)],\n",
        "      'model_1': [(src_tensor, src_len_tensor), (trg_tensor, trg_len_tensor), (trg_tensor, trg_len_tensor)],\n",
        "      'TRG': (trg_tensor, trg_len_tensor)}\n",
        "    # feed model\n",
        "    output = model(data, None, 0)\n",
        "    output = output.argmax(-1).detach().cpu().numpy()\n",
        "    results = idx2sent(trg_field, output)\n",
        "    return results"
      ],
      "metadata": {
        "id": "dxrWnx-ImxDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if isinstance(model, Seq2Seq):\n",
        "  translator = translate_sentence_seq2seq\n",
        "  print('Created translator for seq2seq')\n",
        "elif isinstance(model, PivotSeq2Seq):\n",
        "  translator = translate_sentence_pivot\n",
        "  print('Created translator for piv')\n",
        "elif isinstance(model, TriangSeq2Seq):\n",
        "  translator = translate_batch_triang\n",
        "  print('Created translator for triang')\n",
        "else: raise Exception('Model type is unknown')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-zvVPKEmzp9",
        "outputId": "241de002-09df-421b-bbc5-2744a6540d81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created translator for triang\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_idx = 2132\n",
        "src = vars(valid_dt.examples[example_idx])['en']\n",
        "trg = vars(valid_dt.examples[example_idx])['fr']\n",
        "pred = translate_sentence(src, EN_FIELD, FR_FIELD, model, device, 50)\n",
        "print(src)\n",
        "print(trg)\n",
        "print(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sK5gR8S7Ecle",
        "outputId": "15f9399e-f9c3-4c51-8d7c-4441fbcc9d23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['(', 'da', ')', 'mr', 'president', ',', 'i', 'merely', 'wish', 'to', 'add', 'to', 'what', 'has', 'already', 'been', 'said', '.']\n",
            "['-', '(', 'da', ')', 'monsieur', 'le', 'président', ',', 'je', 'souhaite', 'simplement', 'ajouter', 'quelques', 'mots', 'à', 'ce', 'qui', 'a', 'déjà', 'été', 'dit', '.']\n",
            "[['(', 'da', ')', 'monsieur', 'le', 'président', ',', 'je', 'voudrais', 'juste', 'ajouter', 'quelques', 'ce', 'qui', 'a', 'déjà', 'été', 'dit', '.', '<eos>']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Translate by batch"
      ],
      "metadata": {
        "id": "2_uEiN_p1ufE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_iterator_1 = BucketIterator(\n",
        "     test_dt,\n",
        "     batch_size = 4,\n",
        "     sort_within_batch = True,\n",
        "     sort_key = lambda x : len(x.fr),\n",
        "     device = device)\n",
        "\n",
        "for batch in test_iterator_1: break\n",
        "\n",
        "print(batch.fields)\n",
        "print(batch.en[1])\n",
        "print(batch.fr[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwABYGQpZoFf",
        "outputId": "1efbd7d4-0fe7-49b9-d853-1807f76ab772"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['en', 'fr'])\n",
            "tensor([21, 20, 23, 15], device='cuda:0')\n",
            "tensor([24, 24, 24, 24], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_batch_seq2seq(model_infer, iterator, trg_field, device):\n",
        "  model_infer.eval()\n",
        "  with torch.no_grad():\n",
        "    gt_sents = []\n",
        "    pred_sents = []\n",
        "    for i, batch in enumerate(tqdm(iterator)):\n",
        "      data = [batch.en, batch.fr] # modify based on model\n",
        "      output = model_infer(data, criterion=None, teacher_forcing_ratio=0)\n",
        "\n",
        "      pred = output.argmax(-1).detach().cpu().numpy() # [seq_len, N]\n",
        "      truth = batch.fr[0].detach().cpu().numpy()  # [seq_len, N]\n",
        "\n",
        "      gt_sents = gt_sents + idx2sent(trg_field, truth)\n",
        "      pred_sents = pred_sents + idx2sent(trg_field, pred)\n",
        "      if i==10: break\n",
        "    return gt_sents, pred_sents"
      ],
      "metadata": {
        "id": "oIRZYHOsi0LU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gt_sents, pred_sents = translate_batch_seq2seq(model_infer, test_iterator_1, FR_FIELD, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fhhvOpTkQkD",
        "outputId": "150dcf1d-4308-4b67-984e-d9a479e60150"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 10/1600 [00:00<00:31, 49.92it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (gt_sent, pred_sent) in enumerate(zip(gt_sents, pred_sents)):\n",
        "  print(gt_sent)\n",
        "  print(pred_sent)\n",
        "  print()\n",
        "  if i==10: break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKokrGMnlDp5",
        "outputId": "12c744a4-541a-48f6-8b52-b790e3b2c343"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1', '.', '<eos>']\n",
            "['1', '.', '<eos>']\n",
            "\n",
            "['1', '.', '<eos>']\n",
            "['1', '.', '<eos>']\n",
            "\n",
            "['certes', '.', '<eos>']\n",
            "['cela', 'ne', 'fait']\n",
            "\n",
            "['2', '.', '<eos>']\n",
            "['2', '.', '<eos>']\n",
            "\n",
            "[\"c'\", 'est', 'un', 'outil', 'important', 'pour', 'rendre', 'compte', 'aux', 'citoyens', 'de', \"l'\", 'europe', ',', 'à', 'leurs', 'représentants', 'élus', ',', 'au', 'parlement', ',', 'de', 'la', 'politique', 'monétaire', 'et', 'de', 'nos', 'activités', 'dans', 'les', 'domaines', 'de', 'notre', 'compétence', '.', '<eos>']\n",
            "['il', 's’', 'agit', 'là', 'un', 'instrument', 'important', 'pour', 'tenir', 'un', 'signe', 'de', 'citoyens', 'européens', ',', 'en', 'leurs', 'représentants', 'élus', ',', 'au', 'parlement', ',', 'de', 'la', 'politique', 'de', 'nos', 'politiques', 'en', 'matière', 'de', 'responsabilité', '.', '<eos>']\n",
            "\n",
            "['à', 'cet', 'égard', ',', 'le', 'fait', 'de', 'soutenir', \"l'\", 'appel', 'de', 'la', 'commission', 'pour', 'introduire', 'des', 'niveaux', 'de', 'référence', 'communs', 'pour', 'la', 'sécurité', 'nucléaire', 'au', 'sein', 'de', \"l'\", 'union', 'européenne', 'est', 'un', 'aspect', 'crucial', 'du', 'rapport', '.', '<eos>']\n",
            "['dans', 'cet', 'sens', ',', 'le', 'soutien', 'à', 'la', 'commission', 'de', 'la', 'commission', 'de', 'mettre', 'en', 'pied', 'une', 'sécurité', 'nucléaire', 'en', 'matière', 'de', 'sécurité', 'nucléaire', 'dans', 'l’', 'union', 'européenne', 'est', 'un', 'aspect', 'fondamental', '.', '<eos>']\n",
            "\n",
            "['comme', \"l'\", 'a', ',', 'à', 'juste', 'titre', ',', 'dit', 'la', 'commissaire', ',', 'maintenant', 'il', 'est', 'temps', 'de', 'mettre', 'en', 'œuvre', 'le', 'bon', 'travail', 'que', 'nous', 'avons', 'accompli', 'au', 'cours', 'de', 'ces', 'dernières', 'années', '.', '<eos>']\n",
            "['comme', 'l’', 'a', 'dit', 'le', 'commissaire', 'a', 'dit', ',', 'il', 'est', 'en', 'est', 'de', 'mettre', 'en', 'œuvre', 'en', 'tout', 'de', 'fait', 'que', 'ces', 'dernières', 'années', '.', '<eos>']\n",
            "\n",
            "['tout', \"d'\", 'abord', ',', 'nous', 'espérons', 'recevoir', 'bientôt', \"l'\", 'avis', 'de', 'la', 'cour', 'des', 'comptes', ',', 'ce', 'qui', 'nous', 'permettra', \"d'\", 'aboutir', 'à', 'une', 'conclusion', 'rapide', 'à', 'cet', 'égard', '.', '<eos>']\n",
            "['premièrement', ',', 'nous', 'espérons', 'que', 'l’', 'avis', 'de', 'la', 'cour', 'des', 'comptes', 'et', 'que', 'nous', 'pourrons', 'donc', 'recevoir', 'une', 'conclusion', 'à', 'cet', 'égard', '.', '<eos>']\n",
            "\n",
            "['si', 'nous', 'ne', 'pouvons', 'le', 'comprendre', 'maintenant', ',', 'quand', 'le', 'pourrons', '-nous', '?', '<eos>']\n",
            "['quand', 'on', 'ne', 'peut', 'pas', 'le', 'cas', ',', 'quand', 'quand', 'quand', 'quand', 'quand', 'quand']\n",
            "\n",
            "['je', 'souhaite', 'remercier', 'le', 'rapporteur', 'pour', 'la', 'réflexion', 'menée', 'derrière', 'le', 'rapport', '.', '<eos>']\n",
            "['je', 'voudrais', 'féliciter', 'le', 'rapporteur', 'pour', 'son', 'rapport', '.', '<eos>']\n",
            "\n",
            "['néanmoins', ',', 'le', 'texte', 'de', 'la', 'commission', 'de', \"l'\", 'environnement', 'est', 'acceptable', '.', '<eos>']\n",
            "['le', 'texte', 'de', 'la', 'commission', 'de', \"l'\", 'environnement', 'est', 'cependant', '.', '<eos>']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BLEU"
      ],
      "metadata": {
        "id": "_gNHZNQk2e4M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main"
      ],
      "metadata": {
        "id": "celagCJTtFd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_bleu(translator, data, src_field, trg_field, model, device, max_len=50):\n",
        "  trgs = []\n",
        "  pred_trgs = []\n",
        "  for i, datum in enumerate(tqdm(data)):\n",
        "    src = vars(datum)['en']\n",
        "    trg = vars(datum)['fr']\n",
        "    pred = translator(src, src_field, trg_field, model, device, max_len)\n",
        "    pred_trgs.append(pred[0][:-1])  #cut off <eos> token\n",
        "    trgs.append([trg])\n",
        "    # if i==2000: break\n",
        "  return pred_trgs, trgs"
      ],
      "metadata": {
        "id": "gTwoCAVY2iib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_trgs, trgs = calculate_bleu(translate_sentence, test_dt, EN_FIELD, FR_FIELD, model, device)"
      ],
      "metadata": {
        "id": "UDzwuc3z2sDq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c6af264-7100-4064-9708-cf1c6baf488e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6400/6400 [16:06<00:00,  6.62it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score = bleu_score(pred_trgs, trgs)\n",
        "print(f'BLEU score = {score*100:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rGPCkwK9DfB",
        "outputId": "5c648e6b-922b-4fd6-9a8b-0e6b20585c6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU score = 26.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IF error happens: index 4 is out of bounds for dimension 0 with size 4 ==> use this"
      ],
      "metadata": {
        "id": "yi7oQJcHs7KJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pred_trg_corpus(data, model_infer, device, SRC_FIELD, TRG_FIELD, PIV_FIELD=None, max_len=50):\n",
        "  candidate_corpus = []\n",
        "  references_corpus = []\n",
        "  for datum in tqdm(data):\n",
        "    src = vars(datum)['src']\n",
        "    trg = vars(datum)['trg']\n",
        "    if isinstance(model_infer, PivotSeq2Seq):\n",
        "      piv = vars(datum)['piv']\n",
        "      sent1, sent = translate_sentence_1piv(src, SRC_FIELD, PIV_FIELD, TRG_FIELD, model_infer, device, max_len=max_len)\n",
        "    else:\n",
        "      sent, attn = translate_sentence(src, SRC_FIELD, TRG_FIELD, model_infer, device, max_len=max_len)\n",
        "    candidate_corpus.append(sent[:-1])\n",
        "    references_corpus.append([trg])\n",
        "  return candidate_corpus, references_corpus"
      ],
      "metadata": {
        "id": "TTExRY2DnVC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "candidate_corpus, references_corpus = get_pred_trg_corpus(test_dt, model_infer, device, SRC_FIELD, TRG_FIELD, PIV_FIELD)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_a4WDhXZo3sJ",
        "outputId": "26aa6768-5387-4472-c33d-3c0a8865b55d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6400/6400 [02:43<00:00, 39.08it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "candidate_corpus[:1376], references_corpus[:1376]"
      ],
      "metadata": {
        "id": "CDkYu16Lq_tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 5000\n",
        "j = 100\n",
        "k = 30000\n",
        "bleu_score(candidate_corpus[: i]+candidate_corpus[i+j:k], references_corpus[: i]+references_corpus[i+j:k])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLDv3sEIo-Q9",
        "outputId": "7e0cdc70-b69a-4e5e-b7d5-0a2e88c90707"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.26010842469504236"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "candidate = ['hello', '']\n",
        "references = [['hello'], ['.']]\n",
        "bleu_score(candidate, references)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lU_iKKNCon4s",
        "outputId": "edfc8467-5ae7-48be-c8e9-3f58a6d433e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8187307530779819"
            ]
          },
          "metadata": {},
          "execution_count": 249
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Result"
      ],
      "metadata": {
        "id": "LcU3uVKbtI4X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Seq2Seq\n",
        "  * seq2seq-EnFr-1.pt: BLEU = 38.74\n",
        "1. PivotSeq2Seq\n",
        "  * piv_EnItFr.pt: BLEU = 31.37\n",
        "  * piv-EnEsFr.pt: BLEU = 32.27\n",
        "  * piv-EnRoFr.pt: BLEU = 29.51\n",
        "  * piv-EnPtFr.pt: BLEU = 31.36\n",
        "1. TriangSeq2Seq\n",
        "  * triang-EnFr-EnItFr:\n",
        "    * triang-EnFr-EnItFr-1.pt: BLEU = 26.12\n",
        "    * triang-EnFr-EnItFr.pt: BLEU = 31.95 (EnFr: BLEU=38.03, EnItFr: BLEU=24.62)\n",
        "  * triang-EnFr-EnEsFr: (Prof Neller's suggestion, method='max')\n",
        "    * BLEU = 37.80 (prod)\n",
        "    * BLEU = 37.91 (mean)\n",
        "  * triang-EnFr-EnPtFr: (Prof Neller's suggestion, method='max')\n",
        "    * BLEU = 37.81 (prod)\n",
        "    * BLEU = 37.93 (mean)\n",
        "  * triang-EnFr-EnRoFr: (Prof Neller's suggestion, method='max')\n",
        "    * BLEU = 37.98 (prod)\n",
        "    * BLEU = 38.09 (mean)\n",
        "  * triang-EnFr-EnItFr-EnEsFr: (Prof Neller's suggestion, method='max')\n",
        "    * BLEU = 36.25 (mean)"
      ],
      "metadata": {
        "id": "cui8pV4FBbx_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* attn_en-fr_32k.pt: BLEU = 12.65\n",
        "* attn_enfr_160kset.pt: BLEU = 32.18\n",
        "* piv_endefr_74kset_2.pt: BLEU = 26.33\n"
      ],
      "metadata": {
        "id": "vIw-3jBB42zA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# End"
      ],
      "metadata": {
        "id": "_0vl6ryRfWl9"
      }
    }
  ]
}