{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyPz99mMtjm0f2z233p9ocL/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["Issue: Model does not converge\n","\n","Reason:\n","  * [ ] Different dataset from tutorial [Aladdin Persson](https://www.youtube.com/watch?v=sQUqQddQtB4)\n","  * [X] Compare to [Pytorch tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html), I don't have the initHidden method --> dont need (Defaults to zeros if (h_0, c_0) is not provided)"],"metadata":{"id":"VFCvErsQ00RX"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gm4AaemB3hpJ","executionInfo":{"status":"ok","timestamp":1676847654987,"user_tz":300,"elapsed":15323,"user":{"displayName":"Quan Nguyen","userId":"14971034059264739859"}},"outputId":"1a7d454f-45dd-4906-f3d9-f9efa6fcf68b"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["!pip install spacy -q"],"metadata":{"id":"y-HSqOOZ3-Gp","executionInfo":{"status":"ok","timestamp":1676847659096,"user_tz":300,"elapsed":4119,"user":{"displayName":"Quan Nguyen","userId":"14971034059264739859"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["!python -m spacy download fr_core_news_sm -q\n","!python -m spacy download en_core_web_sm -q\n","# !python -m spacy download de_core_news_sm -q"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ExHnhY4q4ANJ","executionInfo":{"status":"ok","timestamp":1676847699905,"user_tz":300,"elapsed":40812,"user":{"displayName":"Quan Nguyen","userId":"14971034059264739859"}},"outputId":"5e1f159a-184c-4571-f196-337665b492f1"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-02-19 23:01:06.168621: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-02-19 23:01:08.949252: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2023-02-19 23:01:08.949370: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2023-02-19 23:01:08.949391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('fr_core_news_sm')\n","2023-02-19 23:01:28.618517: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-02-19 23:01:29.543744: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2023-02-19 23:01:29.543862: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2023-02-19 23:01:29.543882: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m92.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n"]}]},{"cell_type":"code","source":["# !pip install fasttext"],"metadata":{"id":"d7zB4NZ2-W9_","executionInfo":{"status":"ok","timestamp":1676847699906,"user_tz":300,"elapsed":12,"user":{"displayName":"Quan Nguyen","userId":"14971034059264739859"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"M6NjMyQP3YvM","executionInfo":{"status":"ok","timestamp":1676847715895,"user_tz":300,"elapsed":15999,"user":{"displayName":"Quan Nguyen","userId":"14971034059264739859"}},"outputId":"a9c2a678-380a-4fd8-a75b-e1b4f8e4adc6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'cuda'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":5}],"source":["from __future__ import unicode_literals, print_function, division\n","import os\n","import io\n","from io import open\n","import unicodedata\n","import string\n","import re\n","import random\n","import pickle\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import matplotlib.ticker as ticker\n","import numpy as np\n","\n","import spacy\n","import nltk\n","from tqdm import tqdm\n","import gensim\n","\n","import torch\n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","device"]},{"cell_type":"code","source":["cfg = {\n","  \"PAD_token\": 0,\n","  \"SOS_token\": 1,\n","  \"EOS_token\": 2,\n","  \"MIN_LENGTH\": 2,\n","  \"MAX_LENGTH\": 50,\n","  \"max_seq_len\": 64,\n","  \"input_pad\": \"pre\",\n","  \"input_reverse\": True,\n","  \"spacy\": {\n","      \"en\": \"en_core_web_sm\",\n","      \"fr\": \"fr_core_news_sm\",\n","      \"de\": \"de_core_news_sm\"\n","  },\n","  \"batch_size\": 128,\n","  \"epoch\": 5,\n","  \"dataset_len\": 3200,\n","  \"encoder_embedding_size\": 300,\n","  \"decoder_embedding_size\": 300,\n","  \"hidden_size\": 256,\n","  \"num_layers\": 1,\n","  \"enc_dropout\": 0.5,\n","  \"dec_dropout\": 0.5,\n","  \"learning_rate\": 0.001\n","}"],"metadata":{"id":"sI7AjqiNq3Zy","executionInfo":{"status":"ok","timestamp":1676847715895,"user_tz":300,"elapsed":6,"user":{"displayName":"Quan Nguyen","userId":"14971034059264739859"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def savePickle(input_lang, output_lang, pairs):\n","  with open('/content/gdrive/MyDrive/Colab Notebooks/eaai24/lang-en.pkl', 'wb') as f:\n","    pickle.dump(input_lang, f)\n","  with open('/content/gdrive/MyDrive/Colab Notebooks/eaai24/lang-fr.pkl', 'wb') as f:\n","    pickle.dump(output_lang, f)\n","  with open('/content/gdrive/MyDrive/Colab Notebooks/eaai24/pairs-EnFr.pkl', 'wb') as f:\n","    pickle.dump(pairs, f)\n","\n","def loadPickle():\n","  with open('/content/gdrive/MyDrive/Colab Notebooks/eaai24/lang-en.pkl', 'rb') as f:\n","    input_lang = pickle.load(f)\n","  with open('/content/gdrive/MyDrive/Colab Notebooks/eaai24/lang-fr.pkl', 'rb') as f:\n","    output_lang = pickle.load(f)\n","  with open('/content/gdrive/MyDrive/Colab Notebooks/eaai24/pairs-EnFr.pkl', 'rb') as f:\n","    pairs = pickle.load(f)\n","  return input_lang, output_lang, pairs"],"metadata":{"id":"rzeNORKvKzFE","executionInfo":{"status":"ok","timestamp":1676847715896,"user_tz":300,"elapsed":6,"user":{"displayName":"Quan Nguyen","userId":"14971034059264739859"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["# data"],"metadata":{"id":"JuceqM70ezfg"}},{"cell_type":"markdown","source":["# Dataset"],"metadata":{"id":"rJqEPLktUBz0"}},{"cell_type":"code","source":["class MyDataset(torch.utils.data.Dataset):\n","  def __init__(self, input_lang, output_lang, pairs, cfg):\n","    self.input_lang = input_lang\n","    self.output_lang = output_lang\n","    self.pairs = pairs\n","    self.cfg = cfg\n","    # self.in_tkz = spacy.load(cfg['spacy'][input_lang.name])\n","    # self.out_tkz = spacy.load(cfg['spacy'][output_lang.name])\n","  \n","  def __len__(self):\n","    return len(self.pairs)\n","  \n","  def tokenizeTxt(self, lang, text):  # Tokenizes text from a string into a list of strings (tokens)\n","    if lang.name == self.input_lang.name:\n","      return [tok.text for tok in self.input_lang.tkz.tokenizer(text)]  # self.in_tkz.tokenizer(text)\n","    return [tok.text for tok in self.output_lang.tkz.tokenizer(text)]  # self.out_tkz.tokenizer(text)\n","\n","  def indexesFromSentence(self, lang, sentence):\n","    # sent2id = [lang.word2index[word] for word in sentence.split(' ')][:self.cfg['max_seq_len']]\n","    words = self.tokenizeTxt(lang, sentence)\n","    sent2id = [lang.word2index[word] for word in words]\n","    return [self.cfg['SOS_token']] + sent2id[:self.cfg['max_seq_len']-2] + [self.cfg['EOS_token']]  # 2 for <sos> and <eos>\n","\n","  def paddingTensorFromSentence(self, lang, sentence, padding, reverse_in):\n","      indexes = self.indexesFromSentence(lang, sentence)\n","      remain_len = self.cfg['max_seq_len'] - len(indexes)\n","      if reverse_in:\n","        indexes = list(reversed(indexes))\n","      if padding == 'pre':\n","        indexes = [self.cfg['PAD_token']]*remain_len + indexes\n","      elif padding == 'post':\n","        indexes = indexes + [self.cfg['PAD_token']]*remain_len\n","      \n","      return torch.tensor(indexes, dtype=torch.long).view(-1) # output.shape = (cfg['max_seq_len']) = [64]\n","\n","  def tensorsFromPair(self, pair):\n","      input_tensor = self.paddingTensorFromSentence(self.input_lang, pair[0], self.cfg['input_pad'], reverse_in=self.cfg['input_reverse'])\n","      target_tensor = self.paddingTensorFromSentence(self.output_lang, pair[1], 'post', reverse_in=False)\n","      \n","      return (input_tensor, target_tensor)  # output.shape = (cfg['max_seq_len']) = [64]\n","\n","  def __getitem__(self, index):\n","    pair = self.pairs[index]\n","    # tkzed_in = [tok.text for tok in self.in_tkz.tokenizer(pair[0])]\n","    # tkzed_out = [tok.text for tok in self.out_tkz.tokenizer(pair[1])]\n","    return self.tensorsFromPair(pair), pair"],"metadata":{"id":"ZTpmNHS3UDjD","executionInfo":{"status":"ok","timestamp":1676847716103,"user_tz":300,"elapsed":212,"user":{"displayName":"Quan Nguyen","userId":"14971034059264739859"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["# Testing data process"],"metadata":{"id":"YwqALeRrdi8x"}},{"cell_type":"markdown","source":["# load data"],"metadata":{"id":"c91LVzRF34IZ"}},{"cell_type":"code","source":["class Lang:\n","    def __init__(self, name, cfg=cfg):\n","        self.name = name\n","        self.cfg = cfg\n","        self.tkz = spacy.load(cfg['spacy'][name])\n","        self.max_len = 0\n","        self.word2index = {\"<pad>\": cfg['PAD_token'], \"<sos>\": cfg['SOS_token'], \"<eos>\": cfg['EOS_token']}\n","        self.index2word = {cfg['PAD_token']: \"<pad>\", cfg['SOS_token']: \"<sos>\", cfg['EOS_token']: \"<eos>\"}\n","        self.word2count = {}\n","        self.n_words = 3  # Count SOS and EOS\n","\n","    def addSentence(self, sentence):\n","      words = [tok.text for tok in self.tkz.tokenizer(sentence)]\n","      # for word in sentence.split(' '):\n","      for word in words:\n","          self.addWord(word)\n","\n","    def addWord(self, word):\n","        if word not in self.word2index: # if not in dict:\n","            self.word2index[word] = self.n_words\n","            self.word2count[word] = 1\n","            self.index2word[self.n_words] = word\n","            self.n_words += 1\n","        else: # count++ if word already in dict\n","            self.word2count[word] += 1"],"metadata":{"id":"6YpjZQQdLZl6","executionInfo":{"status":"ok","timestamp":1676847716103,"user_tz":300,"elapsed":19,"user":{"displayName":"Quan Nguyen","userId":"14971034059264739859"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["Consider using only **[lower case]** or both **[upper and lower case]**."],"metadata":{"id":"9h_AZDBga7ga"}},{"cell_type":"code","source":["# Turn a Unicode string to plain ASCII, thanks to\n","# https://stackoverflow.com/a/518232/2809427\n","def unicodeToAscii(s):\n","    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n","\n","# Lowercase, trim, and remove non-letter characters\n","def normalizeStr(s):\n","    s = s.lower().strip()\n","    # s = unicodeToAscii(s)\n","    # s = re.sub(r\"([.!?])\", r\" \\1\", s)\n","    # s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n","    return s\n","\n","def load_data(filename: str):\n","    with open(filename, 'rb') as f:\n","        data = pickle.load(f)\n","        print('load_data SUCCESS')\n","        return data\n","\n","def readLangs(lang1='en', lang2='fr', reverse=False):\n","    print(\"Reading data...\")\n","\n","    data = load_data('/content/gdrive/MyDrive/Colab Notebooks/eaai24/dataset/en-fr.pkl')\n","\n","    pairs = [[normalizeStr(pair[lang1]), normalizeStr(pair[lang2])] for pair in data]   # lower case\n","    input_lang = Lang(lang1)\n","    output_lang = Lang(lang2)\n","\n","    return (input_lang, output_lang, pairs)\n","\n","def filterPair(p):\n","  # p: a pair of lang\n","    return cfg['MIN_LENGTH'] <= len(p[0].split(' ')) < cfg['MAX_LENGTH'] and \\\n","           cfg['MIN_LENGTH'] <= len(p[1].split(' ')) < cfg['MAX_LENGTH']\n","\n","def filterPairs(pairs):\n","    return [pair for pair in pairs if filterPair(pair)]"],"metadata":{"id":"h4uMlGzgLgbI","executionInfo":{"status":"ok","timestamp":1676847716104,"user_tz":300,"elapsed":18,"user":{"displayName":"Quan Nguyen","userId":"14971034059264739859"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["def prepareData(lang1='en', lang2='fr'):\n","    (input_lang, output_lang, pairs) = readLangs(lang1, lang2)\n","    print(\"Read %s sentence pairs\" % len(pairs))\n","    pairs = filterPairs(pairs)\n","    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n","    print(\"Counting words...\")\n","    for pair in tqdm(pairs):\n","        input_lang.max_len = max(input_lang.max_len, len(pair[0]))\n","        input_lang.addSentence(pair[0])\n","        output_lang.max_len = max(output_lang.max_len, len(pair[1]))\n","        output_lang.addSentence(pair[1])\n","    print(\"Counted words:\")\n","    print(input_lang.name, input_lang.n_words)\n","    print(output_lang.name, output_lang.n_words)\n","\n","    return (input_lang, output_lang, pairs)"],"metadata":{"id":"5FnOOpoqLq2p","executionInfo":{"status":"ok","timestamp":1676847716105,"user_tz":300,"elapsed":18,"user":{"displayName":"Quan Nguyen","userId":"14971034059264739859"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["# Model"],"metadata":{"id":"S27Ur8Amh46p"}},{"cell_type":"markdown","source":["## Encoder"],"metadata":{"id":"DAhfUzvqh7T1"}},{"cell_type":"code","source":["class Encoder(nn.Module):\n","  def __init__(self, input_size, cfg):  # , embedding_size, hidden_size, num_layers, p\n","    '''\n","    Args:\n","      input_size: size of in_lang\n","      embedding_size: size of vec for word2vec\n","      hidden_size: dimensionality of the hidden and cell states = 1024\n","      num_layers: number of layers in the RNN = 2\n","      p: dropout rate = 0.5\n","    '''\n","    super(Encoder, self).__init__()\n","    self.cfg = cfg\n","    self.input_size = input_size\n","    self.embedding_size = cfg['encoder_embedding_size']\n","    self.p = cfg['enc_dropout']\n","    self.hidden_size = cfg['hidden_size']\n","    self.num_layers = cfg['num_layers']\n","\n","    self.dropout = nn.Dropout(self.p)\n","\n","    self.embedding = nn.Embedding(input_size, self.embedding_size, padding_idx=cfg['PAD_token']) # output can be (batch, sent_len, embedding_size)\n","    self.rnn = nn.LSTM(self.embedding_size, self.hidden_size, self.num_layers, dropout=self.p)\n","  \n","  def forward(self, x):\n","    '''\n","    Args:\n","      x: has shape = (seq_len, batch_size)\n","    Return:\n","      hidden: shape = (D∗num_layers, batch_size, hidden_size if proj_size<=0 else proj_size)\n","      cell: shape = (D∗num_layers, bact_size, hidden_size)\n","    '''\n","    # print(f'Encoder\\t x.shape = {x.shape} \\t expect (512, batch_size)')\n","    embedding = self.dropout(self.embedding(x))\n","    # embedding shape = (seq_len, batch_size, embedding_size)\n","    \n","    # LSTM input: shape = (seq_len, batch_size, input_size)\n","    outputs, (hidden, cell) = self.rnn(embedding)\n","    # outputs.shape = [seq_len, batch size, hidden_size * num_directions]\n","    # hidden.shape = (num_layers * num_directions, batch_size, hidden_size)\n","    # cell.shape = (num_layers * num_directions, batch_size, hidden_size)\n","\n","    # note: num_directions = 1 if bidirection=False else 2\n","    # outputs are always from the top hidden layer\n","    return hidden, cell"],"metadata":{"id":"TcM0HpGjh9CV","executionInfo":{"status":"ok","timestamp":1676847716105,"user_tz":300,"elapsed":17,"user":{"displayName":"Quan Nguyen","userId":"14971034059264739859"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["## Decoder"],"metadata":{"id":"CZnjUZpBiPh3"}},{"cell_type":"code","source":["class Decoder(nn.Module):\n","  def __init__(self, output_size, cfg):  # embedding_size, hidden_size, output_size, num_layers, p |output_dim, emb_dim, hid_dim, n_layers, dropout\n","    '''\n","    embedding_size: size of vec for word2vec\n","    hidden_size: same as in Encoder\n","    output_size: size of out_lang\n","    num_layers:\n","    p: dropout rate\n","    '''\n","    super(Decoder, self).__init__()\n","    self.cfg = cfg\n","    self.hidden_size = cfg['hidden_size']\n","    self.num_layers = cfg['num_layers']\n","    self.p = cfg['dec_dropout']\n","    self.output_size = output_size\n","    self.embedding_size = cfg['decoder_embedding_size']\n","\n","    self.dropout = nn.Dropout(self.p)\n","    self.embedding = nn.Embedding(output_size, self.embedding_size, padding_idx=cfg['PAD_token'])\n","    self.rnn = nn.LSTM(self.embedding_size, self.hidden_size, self.num_layers, dropout=self.p)\n","    self.fc = nn.Linear(self.hidden_size, self.output_size)\n","\n","  def forward(self, x, hidden, cell):\n","    '''\n","    Args:\n","      x: shape = (batch_size) because we input 1 word each time\n","      hidden: shape = (num_directions * num_layers, hidden_size)\n","      cell: current state (for next pred)\n","\n","      #hidden = [n layers * n directions, batch size, hid dim]\n","      #cell = [n layers * n directions, batch size, hid dim]\n","    Return:\n","      pred: shape = (batch_size, target_vocab_len)\n","      hidden, cell: state for next pred\n","    '''\n","    x = x.unsqueeze(0)\n","    # x.shape = (1, batch_size) = (seq_len, batch_size) since we use a single word and not a sentence\n","    \n","    embedding = self.dropout(self.embedding(x))\n","    # embedding.shape = (1, batch_size, embedding_size)\n","    \n","    outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell)) # outputs shape = (1, batch_size, )\n","    # output = [seq len, batch_size, hidden_size * num_directions]\n","    # hidden = [num_layers * num_directions, batch_size, hidden_size]\n","    # cell = [num_layers * num_directions, batch_size, hidden_size]\n","\n","    # seq len and n directions will always be 1 in the decoder, therefore:\n","    # output = [1, batch_size, hidden_size]\n","    # hidden = [num_layers, batch_size, hidden_size]\n","    # cell = [num_layers, batch_size, hidden_size]\n","\n","    predictions = self.fc(outputs.squeeze(0))  # predictions.shape = (1, batch_size, vocab_len)\n","    # prediction = [batch size, output_size] or (batch_size, target_vocab_len)\n","\n","    # predictions = predictions.squeeze(0)  # predictions.shape = (batch_size, target_vocab_len) to send to loss func\n","    \n","    return predictions, hidden, cell"],"metadata":{"id":"s-3O6JMxiQ2m","executionInfo":{"status":"ok","timestamp":1676847716106,"user_tz":300,"elapsed":17,"user":{"displayName":"Quan Nguyen","userId":"14971034059264739859"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["## Seq2Seq"],"metadata":{"id":"knbVRbjwjBJ0"}},{"cell_type":"code","source":["class Seq2Seq(nn.Module):\n","  def __init__(self, encoder: Encoder, decoder: Decoder):\n","    super(Seq2Seq, self).__init__()\n","    self.encoder = encoder\n","    self.decoder = decoder\n","    assert encoder.hidden_size == decoder.hidden_size, \"Hidden dimensions of encoder and decoder must be equal!\"\n","    assert encoder.num_layers == decoder.num_layers, \"Encoder and decoder must have equal number of layers!\"\n","\n","  def forward(self, source, target, teacher_force_ratio=0.5):\n","    '''\n","    source: shape = (src_len, batch_size)\n","    target: shape = (target_len, batch_size)\n","    teacher_force_ratio: ratio b/w choosing predicted and ground_truth word to use as input for next word prediction\n","    '''\n","    batch_size = target.shape[1]\n","    target_len = target.shape[0]\n","    # target_vocab_size = target.n_words\n","    target_vocab_size = self.decoder.output_size\n","\n","    # tensor to store decoder outputs\n","    outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n","\n","    # last hidden state of the encoder is used as the initial hidden state of the decoder\n","    hidden, cell = self.encoder(source)\n","\n","    # First input to the decoder is the <sos> tokens\n","    x = target[0]\n","    # print(f'Seq2Seq\\t start x.shape = {x.shape} \\t expect (batch_size)')\n","    for t in range(1, target_len):\n","      # insert input token embedding, previous hidden and previous cell states\n","      # receive output tensor (predictions) and new hidden and cell states\n","      output, hidden, cell = self.decoder(x, hidden, cell)\n","      # output.shape = (batch_size, target_vocab_len)\n","      \n","      # print(f'Seq2Seq\\t output.shape = {output.shape} \\t expect (batch_size, target_vocab_len)')\n","\n","      # place predictions in a tensor holding predictions for each token\n","      outputs[t] = output\n","\n","      #decide if we are going to use teacher forcing or not\n","      teacher_force = random.random() < teacher_force_ratio\n","\n","      # Get the best word the Decoder predicted (index in the vocabulary)\n","      best_guess = output.argmax(1) # best_guess.shape = (batch_size)\n","      # print(f'Seq2Seq\\t best_guess.shape = {best_guess.shape} \\t expect (batch_size)')\n","\n","      # With probability of teacher_force_ratio we take the actual next word\n","      # otherwise we take the word that the Decoder predicted it to be.\n","      # Teacher Forcing is used so that the model gets used to seeing\n","      # similar inputs at training and testing time, if teacher forcing is 1\n","      # then inputs at test time might be completely different than what the\n","      # network is used to. This was a long comment.\n","      x = target[t] if teacher_force else best_guess\n","\n","    return outputs"],"metadata":{"id":"N8PWznzJjCuo","executionInfo":{"status":"ok","timestamp":1676847716106,"user_tz":300,"elapsed":16,"user":{"displayName":"Quan Nguyen","userId":"14971034059264739859"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["# Model Attn"],"metadata":{"id":"fXMKQcf_eX7k"}},{"cell_type":"markdown","source":["## Encoder"],"metadata":{"id":"8QCdp-X_eZZi"}},{"cell_type":"code","source":["class Encoder_Attn(nn.Module):\n","  def __init__(self, input_size, cfg):  # embedding_size, hidden_size, num_layers, p, \n","    '''\n","    Args:\n","      input_size: size of in_lang\n","      embedding_size: size of vec for word2vec\n","      hidden_size: dimensionality of the hidden and cell states = 1024\n","      num_layers: number of layers in the RNN = 1 --> no dropout\n","      p: dropout rate = 0.5\n","    '''\n","    super(Encoder_Attn, self).__init__()\n","    self.cfg = cfg\n","    self.input_size = input_size\n","    self.embedding_size = cfg['encoder_embedding_size']\n","    self.p = cfg['enc_dropout']\n","    self.hidden_size = cfg['hidden_size']\n","    self.num_layers = cfg['num_layers']\n","\n","    self.embedding = nn.Embedding(self.input_size, self.embedding_size, padding_idx=self.cfg['PAD_token']) # output can be (batch, sent_len, embedding_size)\n","    self.rnn = nn.LSTM(self.embedding_size, self.hidden_size, self.num_layers, bidirectional=True)\n","    self.fc_hidden = nn.Linear(self.hidden_size*2, self.hidden_size)  # *2 'cause bidirection\n","    self.fc_cell = nn.Linear(self.hidden_size*2, self.hidden_size)\n","    self.dropout = nn.Dropout(self.p)\n","\n","  def forward(self, x):\n","    '''\n","    Args:\n","      x: has shape = (batch_size, seq_len)\n","    Return:\n","      hidden: shape = (batch_size, hidden_size)\n","      cell: shape = (batch_size, hidden_size)\n","    '''   \n","    embedding = self.dropout(self.embedding(x)) # embedding shape = (seq_len, batch_size, embedding_size)\n","    \n","    encoder_states, (hidden, cell) = self.rnn(embedding)  # LSTM input: shape = (seq_len, batch_size, input_size)\n","    # encoder_states.shape = (seq_len, N, hidden_size * num_directions)\n","    # hidden.shape = (num_layers * num_directions, N, hidden_size)\n","    # cell.shape = (num_layers * num_directions, N, hidden_size)\n","\n","    # hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n","    # hidden [-2, :, : ] is the last of the forwards RNN\n","    # hidden [-1, :, : ] is the last of the backwards RNN\n","\n","    # hidden = self.fc_hidden(torch.cat((hidden[-2], hidden[-1]), dim=1)) # (N, hidden_size) -- concat on hidden_size dim\n","    # cell = self.fc_cell(torch.cat((cell[-2], cell[-1]), dim=1)) # (N, hidden_size) -- concat on hidden_size dim\n","    hidden = self.fc_hidden(torch.cat((hidden[0:1], hidden[1:2]), dim=2))\n","    cell = self.fc_cell(torch.cat((cell[0:1], cell[1:2]), dim=2))\n","\n","    # return encoder_states, hidden.unsqueeze(0), cell.unsqueeze(0)\n","    return encoder_states, hidden, cell"],"metadata":{"id":"66NxEcqAeaiS","executionInfo":{"status":"ok","timestamp":1676847716107,"user_tz":300,"elapsed":15,"user":{"displayName":"Quan Nguyen","userId":"14971034059264739859"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["## Decoder"],"metadata":{"id":"MRs4n4uRegJw"}},{"cell_type":"code","source":["class Decoder_Attn(nn.Module):\n","  def __init__(self, output_size, cfg):  # embedding_size, hidden_size, output_size, num_layers, p | output_dim, emb_dim, hid_dim, n_layers, dropout\n","    '''\n","    embedding_size: size of vec for word2vec\n","    hidden_size: same as in Encoder\n","    output_size: size of out_lang\n","    num_layers: should be 1 --> no dropout\n","    p: dropout rate\n","    '''\n","    super(Decoder_Attn, self).__init__()\n","    self.cfg = cfg\n","    self.hidden_size = cfg['hidden_size']\n","    self.num_layers = cfg['num_layers']\n","    self.p = cfg['dec_dropout']\n","    self.output_size = output_size\n","    self.embedding_size = cfg['decoder_embedding_size']\n","\n","    self.embedding = nn.Embedding(self.output_size, self.embedding_size, padding_idx=self.cfg['PAD_token'])\n","    self.rnn = nn.LSTM(self.hidden_size*2 + self.embedding_size, self.hidden_size, 1)  # num_layers = 1 is a must\n","    self.energy = nn.Linear(self.hidden_size*3, 1) # hidden_states from encoder + prev step from decoder\n","    self.dropout = nn.Dropout(self.p)\n","    self.fc = nn.Linear(self.hidden_size, self.output_size)\n","\n","  def forward(self, x, encoder_states, hidden, cell):\n","    '''\n","    Args:\n","      x: shape = (batch_size) because we input 1 word each time\n","      encoder_states: shape = (seq_len, batch_size, hidden_size * num_directions) --> correct\n","      hidden: shape = (batch_size, hidden_size)\n","      cell: shape = (batch_size, hidden_size)\n","\n","      # hidden = [n layers * n directions, batch size, hid dim]\n","      # cell = [n layers * n directions, batch size, hid dim]\n","    Return:\n","      pred: shape = (batch_size, target_vocab_len)\n","      hidden, cell: state for next pred\n","    '''\n","    # From VIDEO\n","    # x = x.unsqueeze(0)  # (1, N)\n","    # embedding = self.dropout(self.embedding(x)) # (1, N, embedding_size)\n","    # seq_len = encoder_states.shape[0]\n","    # h_reshape = hidden.repeat(seq_len, 1, 1)  # (seq_length, N, hidden_size)\n","    \n","    # # torch.cat shape = (seq_length, N, hidden_size*3)\n","    # energy = F.relu(self.energy(torch.cat((h_reshape, encoder_states), dim=2))) # (seq_length, N, 1)\n","    # attention = F.softmax(energy, dim=0).permute(1, 2, 0) # (seq_length, N, 1) --> (N, 1, seq_len)\n","    # encoder_states = encoder_states.permute(1, 0, 2)  # (N, seq_len, hidden_size*2)\n","    # context_vector = torch.bmm(attention, encoder_states).permute(1, 0, 2) # (N, 1, hidden_size*2) --> (1, N, hidden_size*2)\n","    # rnn_input = torch.cat((context_vector, embedding), dim=2) # (1, N, hidden_size*2 + 300)\n","\n","    # outputs, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n","    # # output = (seq_len, N, hidden_size * num_directions)\n","    # # hidden = (num_layers * num_directions, N, hidden_size)\n","    # # cell = (num_layers * num_directions, N, hidden_size)\n","    #   # seq_len and n directions will always be 1 in the decoder, therefore:\n","    #   # output = (1, N, hidden_size)\n","    #   # hidden = (num_layers * num_directions, N, hidden_size)\n","    #   # cell = (num_layers * num_directions, N, hidden_size)\n","    # predictions = self.fc(outputs).squeeze(0)  # (N, vocab_len)\n","    \n","    # From GITHUB https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/more_advanced/Seq2Seq_attention/seq2seq_attention.py\n","    x = x.unsqueeze(0)\n","    # x: (1, N) where N is the batch size\n","\n","    embedding = self.dropout(self.embedding(x)) # (1, N, embedding_size)\n","    sequence_length = encoder_states.shape[0]\n","    h_reshaped = hidden.repeat(sequence_length, 1, 1) # (seq_length, N, hidden_size*2)\n","\n","    energy = F.relu(self.energy(torch.cat((h_reshaped, encoder_states), dim=2))) # (seq_length, N, 1)\n","\n","    attention = F.softmax(energy, dim=0)  # (seq_length, N, 1)\n","\n","    # attention: (seq_length, N, 1), snk\n","    # encoder_states: (seq_length, N, hidden_size*2), snl\n","    # we want context_vector: (1, N, hidden_size*2), i.e knl\n","    context_vector = torch.einsum(\"snk,snl->knl\", attention, encoder_states)\n","\n","    rnn_input = torch.cat((context_vector, embedding), dim=2) # (1, N, hidden_size*2 + embedding_size)\n","\n","    outputs, (hidden, cell) = self.rnn(rnn_input, (hidden, cell)) # outputs shape: (1, N, hidden_size)\n","\n","    predictions = self.fc(outputs).squeeze(0) # (N, hidden_size)\n","\n","    return predictions, hidden, cell"],"metadata":{"id":"6vIT6_pEehOW","executionInfo":{"status":"ok","timestamp":1676848049361,"user_tz":300,"elapsed":6,"user":{"displayName":"Quan Nguyen","userId":"14971034059264739859"}}},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":["## Seq2Seq"],"metadata":{"id":"fknNDE0jejK_"}},{"cell_type":"code","source":["class Seq2Seq_Attn(nn.Module):\n","  def __init__(self, encoder: Encoder_Attn, decoder: Decoder_Attn, cfg, device):\n","    super(Seq2Seq_Attn, self).__init__()\n","    self.cfg = cfg\n","    self.device = device\n","    self.encoder = encoder.to(self.device)\n","    self.decoder = decoder.to(self.device)\n","\n","    # assert encoder.hidden_size == decoder.hidden_size, \"Hidden dimensions of encoder and decoder must be equal!\"\n","    # assert encoder.num_layers == decoder.num_layers, \"Encoder and decoder must have equal number of layers!\"\n","\n","  def forward(self, source, target, teacher_force_ratio=0.5):\n","    '''\n","    source: shape = (batch_size, src_len)\n","    target: shape = (batch_size, target_len)\n","    teacher_force_ratio: ratio b/w choosing predicted and ground_truth word to use as input for next word prediction\n","    '''\n","    source = source.permute(1, 0)\n","    target = target.permute(1, 0)\n","\n","    batch_size = source.shape[1]\n","    target_len = target.shape[0]\n","    \n","    target_vocab_size = self.decoder.output_size  # (= target.n_words)\n","\n","    # tensor to store decoder outputs\n","    outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(self.device)\n","    \n","    # last hidden state of the encoder is used as the initial hidden state of the decoder\n","    encoder_states, hidden, cell = self.encoder(source)\n","\n","    # First input to the decoder is the <sos> tokens\n","    x = target[0]\n","    for t in range(1, target_len):\n","      # insert input token embedding, previous hidden and previous cell states\n","      # receive output tensor (predictions) and new hidden and cell states\n","      prediction, decode_hidden, decode_cell = self.decoder(x, encoder_states, hidden, cell)\n","      # prediction.shape = (batch_size, target_vocab_len)\n","      \n","      # place predictions in a tensor holding predictions for each token\n","      outputs[t] = prediction\n","\n","      # Get the best word the Decoder predicted (index in the vocabulary)\n","      best_guess = prediction.argmax(1) # best_guess.shape = (batch_size)\n","\n","      teacher_force = random.random() < teacher_force_ratio\n","      x = target[t] if teacher_force else best_guess\n","\n","    return outputs"],"metadata":{"id":"L9gPGKMrelKy","executionInfo":{"status":"ok","timestamp":1676847716109,"user_tz":300,"elapsed":15,"user":{"displayName":"Quan Nguyen","userId":"14971034059264739859"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# with torch.no_grad():\n","#   x = 1\n","#   test_tensor = tensors_out[:x].view(x, -1)\n","#   mask = (test_tensor!=0)\n","#   extract_tensor = torch.masked_select(test_tensor, mask)\n","#   flip_tensor = torch.flip(extract_tensor, (0,))\n","#   new_tensor = \n","#   print(test_tensor)\n","#   print(mask)\n","#   print(extract_tensor)\n","#   print(flip_tensor)\n","#   # print(new_tensor)"],"metadata":{"id":"YPnLFatVg_Z_","executionInfo":{"status":"ok","timestamp":1676847716258,"user_tz":300,"elapsed":163,"user":{"displayName":"Quan Nguyen","userId":"14971034059264739859"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["def init_weights(m):\n","    for name, param in m.named_parameters():\n","        if 'weight' in name:\n","            nn.init.normal_(param.data, mean=0, std=0.01)\n","        else:\n","            nn.init.constant_(param.data, 0)"],"metadata":{"id":"FclY87VpnhU2","executionInfo":{"status":"ok","timestamp":1676847716259,"user_tz":300,"elapsed":6,"user":{"displayName":"Quan Nguyen","userId":"14971034059264739859"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["# Testing model attn"],"metadata":{"id":"lBFIUWTRffJy"}},{"cell_type":"code","source":["cfg = {\n","  \"PAD_token\": 0,\n","  \"SOS_token\": 1,\n","  \"EOS_token\": 2,\n","  \"MIN_LENGTH\": 2,\n","  \"MAX_LENGTH\": 50,\n","  \"max_seq_len\": 64,\n","  \"input_pad\": \"post\",\n","  \"input_reverse\": False,\n","  \"spacy\": {\n","      \"en\": \"en_core_web_sm\",\n","      \"fr\": \"fr_core_news_sm\",\n","      \"de\": \"de_core_news_sm\"\n","  },\n","  \"batch_size\": 32,\n","  \"epoch\": 5,\n","  \"dataset_len\": 3200,\n","  \"encoder_embedding_size\": 300,\n","  \"decoder_embedding_size\": 300,\n","  \"hidden_size\": 1024, # Needs to be the same for both RNN's\n","  \"num_layers\": 1,\n","  \"enc_dropout\": 0.0,\n","  \"dec_dropout\": 0.0,\n","  \"learning_rate\": .01\n","}"],"metadata":{"id":"Ef1LjTCDfhZl","executionInfo":{"status":"ok","timestamp":1676848641050,"user_tz":300,"elapsed":140,"user":{"displayName":"Quan Nguyen","userId":"14971034059264739859"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["# input_lang, output_lang, pairs = prepareData(lang1='en', lang2='fr')\n","# savePickle(input_lang, output_lang, pairs)\n","input_lang, output_lang, pairs = loadPickle()"],"metadata":{"id":"aNhmsTTsLxEl","executionInfo":{"status":"ok","timestamp":1676847754170,"user_tz":300,"elapsed":31436,"user":{"displayName":"Quan Nguyen","userId":"14971034059264739859"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["input_size_enc = input_lang.n_words\n","input_size_dec = output_lang.n_words"],"metadata":{"id":"9BiXNNlTkp_F","executionInfo":{"status":"ok","timestamp":1676847754171,"user_tz":300,"elapsed":45,"user":{"displayName":"Quan Nguyen","userId":"14971034059264739859"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["encoder_attn = Encoder_Attn(input_size_enc, cfg)\n","decoder_attn = Decoder_Attn(input_size_dec, cfg)\n","model_attn = Seq2Seq_Attn(encoder_attn, decoder_attn, cfg, device)\n","# model_attn.apply(init_weights);"],"metadata":{"id":"f2Wo4gftksFF","executionInfo":{"status":"ok","timestamp":1676848648731,"user_tz":300,"elapsed":2413,"user":{"displayName":"Quan Nguyen","userId":"14971034059264739859"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["optimizer = optim.Adam(model_attn.parameters(), lr=cfg['learning_rate'])"],"metadata":{"id":"2ro7P6bWlHw9","executionInfo":{"status":"ok","timestamp":1676848648732,"user_tz":300,"elapsed":6,"user":{"displayName":"Quan Nguyen","userId":"14971034059264739859"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["criterion = nn.CrossEntropyLoss(ignore_index=cfg['PAD_token'], label_smoothing=0.0)"],"metadata":{"id":"Qr7E1z5xlj7Z","executionInfo":{"status":"ok","timestamp":1676848651381,"user_tz":300,"elapsed":157,"user":{"displayName":"Quan Nguyen","userId":"14971034059264739859"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["def train_fn(model: nn.Module, dataloader: torch.utils.data.DataLoader, optimizer: torch.optim, criterion: nn):\n","  model.train()\n","  total_loss = 0.0\n","  for i, ((en_vec, fr_vec), (en, fr)) in tqdm(enumerate(dataloader)):\n","    en_vec = en_vec.to(device)\n","    fr_vec = fr_vec.to(device)\n","\n","    optimizer.zero_grad()\n","\n","    # Forward prop\n","    output = model(en_vec, fr_vec)  # (seq_len, N, target_vocab_size)\n","\n","    # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n","    # doesn't take input in that form. For example if we have MNIST we want to have\n","    # output to be: (N, 10) and targets just (N). Here we can view it in a similar\n","    # way that we have output_words * batch_size that we want to send in into\n","    # our cost function, so we need to do some reshapin. While we're at it\n","    output = output[1:].reshape(-1, output.shape[-1])  # shape = (trg_len * N, target_vocab_size)\n","    \n","    fr_vec = fr_vec.permute(1, 0) # (N, seq_len) --> (seq_len, N)\n","    target = fr_vec[1:].reshape(-1) # shape = (trg_len * batch_size)\n","    # output[1:]: ignore SOS_token\n","    \n","    # print('output', output.shape)\n","    # print(output)\n","    # print('target', target.shape)\n","    # print(target)\n","\n","    loss = criterion(output, target)\n","    loss.backward()\n","\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","    optimizer.step()\n","    \n","    total_loss += loss.item()\n","    \n","  return total_loss/len(dataloader)"],"metadata":{"id":"PlnBhWmt2nkb","executionInfo":{"status":"ok","timestamp":1676847962181,"user_tz":300,"elapsed":145,"user":{"displayName":"Quan Nguyen","userId":"14971034059264739859"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["def evaluate(model: nn.Module, dataloader: torch.utils.data.DataLoader, criterion: nn):\n","  model.eval()\n","  epoch_loss = 0\n","  \n","  with torch.no_grad():\n","    for i, ((en_vec, fr_vec), (en, fr)) in tqdm(enumerate(dataloader)):\n","      en_vec = en_vec.to(device)\n","      fr_vec = fr_vec.to(device)\n","\n","      output = model(en_vec, fr_vec, 0)  # (seq_len, N, target_vocab_size)\n","      \n","      #trg = [trg len, batch size]\n","      #output = [trg len, batch size, output dim]\n","\n","      output = output[1:].reshape(-1, output.shape[-1])\n","      fr_vec = fr_vec.permute(1, 0) # (N, seq_len) --> (seq_len, N)\n","      target = fr_vec[1:].reshape(-1) # shape = (trg_len * batch_size)\n","      #trg = [(trg len - 1) * batch size]\n","      #output = [(trg len - 1) * batch size, output dim]\n","\n","      loss = criterion(output, target)\n","      epoch_loss += loss.item()\n","      \n","  return epoch_loss / len(dataloader)"],"metadata":{"id":"C9Cno1Tp9UgE","executionInfo":{"status":"ok","timestamp":1676848629939,"user_tz":300,"elapsed":308,"user":{"displayName":"Quan Nguyen","userId":"14971034059264739859"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["def update_trainlog(data: list, filename: str='./log/training_log.txt'):\n","  print('update_trainlog SUCCESS')\n","  return []\n","def save_model(model: nn.Module):\n","  print('SAVED MODEL SUCCESS')"],"metadata":{"id":"HRTW-nZZAvsw","executionInfo":{"status":"ok","timestamp":1676840962856,"user_tz":300,"elapsed":4,"user":{"displayName":"Quan Nguyen","userId":"14971034059264739859"}}},"execution_count":54,"outputs":[]},{"cell_type":"code","source":["trainset = MyDataset(input_lang, output_lang, pairs[:cfg['dataset_len']], cfg)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=cfg['batch_size'])\n","validset = MyDataset(input_lang, output_lang, pairs[cfg['dataset_len']:320], cfg)\n","validloader = torch.utils.data.DataLoader(validset, batch_size=cfg['batch_size'])"],"metadata":{"id":"VKGWZkHpqbpy","executionInfo":{"status":"ok","timestamp":1676848715251,"user_tz":300,"elapsed":137,"user":{"displayName":"Quan Nguyen","userId":"14971034059264739859"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["best_train_loss = float('inf')\n","train_log = []\n","model_name = 'abc.pt'\n","\n","for epoch in range(10):\n","  train_loss = train_fn(model_attn, trainloader, optimizer, criterion)\n","  valid_loss = train_fn(model_attn, validloader, optimizer, criterion)\n","  print(f'EPOCH: {epoch} \\t train_loss = {train_loss} \\t valid_loss = {valid_loss}')  \n","\n","  # valid_loss = 0.0\n","  # epoch_info = (model_name, cfg['learning_rate'], cfg['batch_size'], cfg['hidden_size'], cfg['num_layers'],\n","  #               cfg['enc_dropout'], cfg['dec_dropout'], epoch, cfg['epoch'], train_loss, valid_loss)\n","  # train_log.append(epoch_info)\n","  # if train_loss < best_train_loss:\n","  #   best_train_loss = train_loss\n","  #   train_log = update_trainlog(train_log)\n","  #   save_model(model_attn)\n","  \n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":415},"id":"UVpfUtcj33Zk","executionInfo":{"status":"error","timestamp":1676848762325,"user_tz":300,"elapsed":4307,"user":{"displayName":"Quan Nguyen","userId":"14971034059264739859"}},"outputId":"50f2c8e9-dae7-4170-9707-1c4f3395b13c"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stderr","text":["1it [00:04,  4.13s/it]\n"]},{"output_type":"error","ename":"OutOfMemoryError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m<ipython-input-45-f2839211ad9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_attn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_attn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'EPOCH: {epoch} \\t train_loss = {train_loss} \\t valid_loss = {valid_loss}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-30-dc6a0b286a84>\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(model, dataloader, optimizer, criterion)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Forward prop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfr_vec\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (seq_len, N, target_vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-17-0313df785dcc>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, source, target, teacher_force_ratio)\u001b[0m\n\u001b[1;32m     35\u001b[0m       \u001b[0;31m# insert input token embedding, previous hidden and previous cell states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m       \u001b[0;31m# receive output tensor (predictions) and new hidden and cell states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m       \u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m       \u001b[0;31m# prediction.shape = (batch_size, target_vocab_len)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-36-0df6450f1deb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, encoder_states, hidden, cell)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mrnn_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (1, N, hidden_size*2 + embedding_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# outputs shape: (1, N, hidden_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (N, hidden_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    772\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[1;32m    775\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    776\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 14.76 GiB total capacity; 13.06 GiB already allocated; 21.88 MiB free; 13.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}]},{"cell_type":"code","source":["# reverse then pre padding\n","# 200it [03:28,  1.04s/it]\n","# update_trainlog SUCCESS\n","# SAVED MODEL SUCCESS\n","# EPOCH: 0 \t train_loss = 7.335598840713501\n","# 200it [03:26,  1.03s/it]\n","# update_trainlog SUCCESS\n","# SAVED MODEL SUCCESS\n","# EPOCH: 1 \t train_loss = 6.436497392654419\n","# 200it [03:27,  1.04s/it]\n","# update_trainlog SUCCESS\n","# SAVED MODEL SUCCESS\n","# EPOCH: 2 \t train_loss = 6.407515485286712\n","# 200it [03:28,  1.04s/it]\n","# update_trainlog SUCCESS\n","# SAVED MODEL SUCCESS\n","# EPOCH: 3 \t train_loss = 6.4074364233016965\n","# 200it [03:26,  1.03s/it]EPOCH: 4 \t train_loss = 6.408039071559906"],"metadata":{"id":"j5-sjvysY1Uu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# reverse then post padding\n","# 200it [03:26,  1.03s/it]\n","# update_trainlog SUCCESS\n","# SAVED MODEL SUCCESS\n","# EPOCH: 0 \t train_loss = 7.323721137046814\n","# 200it [03:27,  1.04s/it]\n","# update_trainlog SUCCESS\n","# SAVED MODEL SUCCESS\n","# EPOCH: 1 \t train_loss = 6.433749363422394\n","# 200it [03:24,  1.02s/it]\n","# update_trainlog SUCCESS\n","# SAVED MODEL SUCCESS\n","# EPOCH: 2 \t train_loss = 6.409488241672516\n","# 200it [03:25,  1.03s/it]\n","# update_trainlog SUCCESS\n","# SAVED MODEL SUCCESS\n","# EPOCH: 3 \t train_loss = 6.407776315212249\n","# 200it [03:25,  1.03s/it]EPOCH: 4 \t train_loss = 6.408242735862732"],"metadata":{"id":"bQCp4LM_Udgd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Test Model"],"metadata":{"id":"HJT2dWTWipkn"}},{"cell_type":"code","source":["cfg = {\n","  \"PAD_token\": 0,\n","  \"SOS_token\": 1,\n","  \"EOS_token\": 2,\n","  \"MIN_LENGTH\": 2,\n","  \"MAX_LENGTH\": 50,\n","  \"max_seq_len\": 64,\n","  \"input_pad\": \"pre\",\n","  \"input_reverse\": True,\n","  \"spacy\": {\n","      \"en\": \"en_core_web_sm\",\n","      \"fr\": \"fr_core_news_sm\",\n","      \"de\": \"de_core_news_sm\"\n","  },\n","  \"batch_size\": 16,\n","  \"epoch\": 5,\n","  \"dataset_len\": 320,\n","  \"encoder_embedding_size\": 300,\n","  \"decoder_embedding_size\": 300,\n","  \"hidden_size\": 128, # Needs to be the same for both RNN's\n","  \"num_layers\": 1,\n","  \"enc_dropout\": 0.5,\n","  \"dec_dropout\": 0.5,\n","  \"learning_rate\": 0.001\n","}"],"metadata":{"id":"VO2FZgmKi1ZC","executionInfo":{"status":"ok","timestamp":1676841702594,"user_tz":300,"elapsed":208,"user":{"displayName":"Quan Nguyen","userId":"14971034059264739859"}}},"execution_count":59,"outputs":[]},{"cell_type":"code","source":["# input_lang, output_lang, pairs = prepareData(lang1='en', lang2='fr')\n","# savePickle(input_lang, output_lang, pairs)\n","input_lang, output_lang, pairs = loadPickle()"],"metadata":{"id":"KiM4saKAizno","executionInfo":{"status":"ok","timestamp":1676841711491,"user_tz":300,"elapsed":7097,"user":{"displayName":"Quan Nguyen","userId":"14971034059264739859"}}},"execution_count":60,"outputs":[]},{"cell_type":"code","source":["input_size_enc = input_lang.n_words\n","input_size_dec = output_lang.n_words"],"metadata":{"id":"MgbbFA3qi3qI","executionInfo":{"status":"ok","timestamp":1676841711492,"user_tz":300,"elapsed":42,"user":{"displayName":"Quan Nguyen","userId":"14971034059264739859"}}},"execution_count":61,"outputs":[]},{"cell_type":"code","source":["encoder_net = Encoder(input_size_enc, cfg).to(device)\n","decoder_net = Decoder(input_size_dec, cfg).to(device)\n","\n","model_net = Seq2Seq(encoder_net, decoder_net).to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fes7A_0jivdn","executionInfo":{"status":"ok","timestamp":1676841725187,"user_tz":300,"elapsed":922,"user":{"displayName":"Quan Nguyen","userId":"14971034059264739859"}},"outputId":"5861fc74-3c5b-4396-ae59-4ed1ffdf06b3"},"execution_count":66,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n"]}]},{"cell_type":"code","source":["optimizer1 = optim.Adam(model_net.parameters(), lr=cfg['learning_rate'])\n","criterion1 = nn.CrossEntropyLoss(ignore_index=cfg['PAD_token'], label_smoothing=0.5)"],"metadata":{"id":"ySClaOsdjMNa","executionInfo":{"status":"ok","timestamp":1676841728080,"user_tz":300,"elapsed":6,"user":{"displayName":"Quan Nguyen","userId":"14971034059264739859"}}},"execution_count":67,"outputs":[]},{"cell_type":"code","source":["def train_fn1(model: nn.Module, dataloader, optimizer, criterion):\n","  model.train()\n","  total_loss = 0.0\n","  for i, ((en_vec, fr_vec), (en, fr)) in tqdm(enumerate(dataloader)):\n","    en_vec, fr_vec = en_vec.permute(1, 0), fr_vec.permute(1, 0) # (N, seq_len) ---> (seq_len, N)\n","    en_vec = en_vec.to(device)\n","    fr_vec = fr_vec.to(device)\n","\n","    # Forward prop\n","    output = model(en_vec, fr_vec)\n","\n","    # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n","    # doesn't take input in that form. For example if we have MNIST we want to have\n","    # output to be: (N, 10) and targets just (N). Here we can view it in a similar\n","    # way that we have output_words * batch_size that we want to send in into\n","    # our cost function, so we need to do some reshapin. While we're at it\n","    # Let's also remove the start token while we're at it\n","    output = output[1:].reshape(-1, output.shape[2])  # shape = (trg_len * N, output_dim)\n","    target = fr_vec[1:].reshape(-1) # shape = (trg_len * N)\n","    # output[1:]: ignore SOS_token\n","\n","    optimizer.zero_grad()\n","    loss = criterion(output, target)\n","    loss.backward()\n","\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","    optimizer.step()\n","    \n","    total_loss += loss.item()\n","\n","  return total_loss/len(dataloader)"],"metadata":{"id":"6_FhMuX7jYbE","executionInfo":{"status":"ok","timestamp":1676841940356,"user_tz":300,"elapsed":2,"user":{"displayName":"Quan Nguyen","userId":"14971034059264739859"}}},"execution_count":77,"outputs":[]},{"cell_type":"code","source":["dataset1 = MyDataset(input_lang, output_lang, pairs[:cfg['dataset_len']], cfg)\n","dataloader1 = torch.utils.data.DataLoader(dataset1, batch_size=cfg['batch_size'])"],"metadata":{"id":"DSsAMN2Gj1xb","executionInfo":{"status":"ok","timestamp":1676841819840,"user_tz":300,"elapsed":155,"user":{"displayName":"Quan Nguyen","userId":"14971034059264739859"}}},"execution_count":69,"outputs":[]},{"cell_type":"code","source":["for epoch in range(10):\n","  train_loss = train_fn1(model_net, dataloader1, optimizer1, criterion1)\n","  print(f'EPOCH: {epoch+20} \\t train_loss = {train_loss}')"],"metadata":{"id":"Y_EB-HLKjsC9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 20it [00:20,  1.02s/it]\n","# EPOCH: 0 \t train_loss = 10.77676510810852\n","# 20it [00:20,  1.03s/it]\n","# EPOCH: 1 \t train_loss = 9.547325325012206\n","# 20it [00:20,  1.05s/it]\n","# EPOCH: 2 \t train_loss = 9.472969818115235\n","# 20it [00:21,  1.08s/it]\n","# EPOCH: 3 \t train_loss = 9.449314308166503\n","# 20it [00:21,  1.05s/it]\n","# EPOCH: 4 \t train_loss = 9.429299545288085\n","# 20it [00:20,  1.03s/it]\n","# EPOCH: 5 \t train_loss = 9.412078332901\n","# 20it [00:20,  1.04s/it]\n","# EPOCH: 6 \t train_loss = 9.392915678024291\n","# 20it [00:20,  1.02s/it]\n","# EPOCH: 7 \t train_loss = 9.375585794448853\n","# 20it [00:20,  1.02s/it]\n","# EPOCH: 8 \t train_loss = 9.359159421920776\n","# 20it [00:20,  1.01s/it]\n","# EPOCH: 9 \t train_loss = 9.341047334671021\n","# 20it [00:21,  1.08s/it]\n","# EPOCH: 10 \t train_loss = 9.321704816818237\n","# 20it [00:20,  1.01s/it]\n","# EPOCH: 11 \t train_loss = 9.308528852462768\n","# 20it [00:20,  1.02s/it]\n","# EPOCH: 12 \t train_loss = 9.29764266014099\n","# 20it [00:20,  1.01s/it]\n","# EPOCH: 13 \t train_loss = 9.279660081863403\n","# 20it [00:20,  1.03s/it]\n","# EPOCH: 14 \t train_loss = 9.257467269897461\n","# 20it [00:20,  1.01s/it]\n","# EPOCH: 15 \t train_loss = 9.242575073242188\n","# 20it [00:20,  1.02s/it]\n","# EPOCH: 16 \t train_loss = 9.229005479812622\n","# 20it [00:20,  1.02s/it]\n","# EPOCH: 17 \t train_loss = 9.210863161087037\n","# 20it [00:20,  1.02s/it]\n","# EPOCH: 18 \t train_loss = 9.189864492416381\n","# 20it [00:20,  1.02s/it]\n","# EPOCH: 19 \t train_loss = 9.18324818611145\n","# 20it [00:21,  1.08s/it]\n","# EPOCH: 20 \t train_loss = 9.164757680892944\n","# 20it [00:20,  1.01s/it]\n","# EPOCH: 21 \t train_loss = 9.150789070129395\n","# 20it [00:20,  1.02s/it]\n","# EPOCH: 22 \t train_loss = 9.133882522583008\n","# 20it [00:20,  1.02s/it]\n","# EPOCH: 23 \t train_loss = 9.120397186279297\n","# 20it [00:20,  1.02s/it]\n","# EPOCH: 24 \t train_loss = 9.103279829025269\n","# 20it [00:20,  1.02s/it]\n","# EPOCH: 25 \t train_loss = 9.084403371810913\n","# 20it [00:20,  1.04s/it]\n","# EPOCH: 26 \t train_loss = 9.061267852783203"],"metadata":{"id":"JpXFB3ovlKJP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# END"],"metadata":{"id":"3k4JWjHVrR6H"}}]}