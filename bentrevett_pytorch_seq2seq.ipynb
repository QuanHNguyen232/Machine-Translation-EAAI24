{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "limqRnayUnj5",
        "vJm8VpgbUsbK",
        "7M3M0cKNUsfb"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Improvements:\n",
        "* [X] Reverse input.\n",
        "  * Method 1: Added in tkzer\n",
        "  * Method 2: Added in `seq2seq` model's `forward` method\n",
        "* [X] Train on smaller dataset (currently using just 32k/2M pairs) (lower epoch instead). New trainset with 64k/160k-pairs and 2k5 most freq words from original dataset\n",
        "* [ ] Change init model to normal distribution from $-0.8$ to $0.8$"
      ],
      "metadata": {
        "id": "WuVizTD46IT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/bentrevett/pytorch-seq2seq/blob/master/4%20-%20Packed%20Padded%20Sequences%2C%20Masking%2C%20Inference%20and%20BLEU.ipynb\n",
        "# based on https://gmihaila.github.io/tutorial_notebooks/pytorchtext_bucketiterator/#dataset-class"
      ],
      "metadata": {
        "id": "V6P0jGmKybf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "DEdOoJ93e008"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install torch==1.8.2 torchvision==0.9.2 torchaudio==0.8.2 --extra-index-url https://download.pytorch.org/whl/lts/1.8/cu111 -q"
      ],
      "metadata": {
        "id": "SA0jFbKHbfHP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2f9b1ac-cf5d-43d2-9c48-8b3bb737f6e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 GB\u001b[0m \u001b[31m804.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.5/17.5 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.14.1 requires torch==1.13.1, but you have torch 1.8.2+cu111 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1754nP4obkXv",
        "outputId": "e395c594-18c8-467a-8f6d-137f877e9a76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.8.2+cu111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMlSEzldbP3q",
        "outputId": "bdf89228-ba27-48a9-f2a8-fe011f2c9fd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m735.5/735.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.9.2+cu111 requires torch==1.8.2, but you have torch 1.8.0 which is incompatible.\n",
            "torchaudio 0.8.2 requires torch==1.8.2, but you have torch 1.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install torchtext==0.9 -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm -q\n",
        "!python -m spacy download de_core_news_sm -q\n",
        "!python -m spacy download fr_core_news_sm -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dw6ZjirTbTE8",
        "outputId": "bcaddddb-48d8-4ff3-9b63-e27b08c768b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-03-04 23:32:43.536755: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-04 23:32:48.743236: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-03-04 23:32:48.743702: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-03-04 23:32:48.743740: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2023-03-04 23:32:54.056363: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "2023-03-04 23:33:05.951822: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-04 23:33:06.937172: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-03-04 23:33:06.937292: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-03-04 23:33:06.937314: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2023-03-04 23:33:08.461132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n",
            "2023-03-04 23:33:16.489565: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-04 23:33:17.919138: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-03-04 23:33:17.919251: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-03-04 23:33:17.919273: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2023-03-04 23:33:19.954915: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# from torchtext.legacy.datasets import Multi30k\n",
        "from torchtext.legacy.data import Field, BucketIterator\n",
        "from torchtext.legacy.data import Dataset, Example\n",
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import pickle\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "6N_lgaPWd4AX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Work"
      ],
      "metadata": {
        "id": "uf5Zguxbe2pz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# My Section"
      ],
      "metadata": {
        "id": "IZIH_L_HeyEJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "f_Qg27JlTHS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOnOF0x_PM0F",
        "outputId": "81262b85-a910-4d2d-b211-7f5bf3b9663f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spacy_de = spacy.load('de_core_news_sm')\n",
        "def tokenize_de(text):\n",
        "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
        "\n",
        "spacy_fr = spacy.load('fr_core_news_sm')\n",
        "def tokenize_fr(text):\n",
        "    return [tok.text for tok in spacy_fr.tokenizer(text)]\n",
        "\n",
        "spacy_en = spacy.load('en_core_web_sm')\n",
        "def tokenize_en(text):\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]"
      ],
      "metadata": {
        "id": "aXSlRvZqPRqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EN_FIELD = Field(tokenize = tokenize_en, \n",
        "            init_token = '<sos>',\n",
        "            eos_token = '<eos>',\n",
        "            lower = True,\n",
        "            include_lengths = True\n",
        "            )\n",
        "DE_FIELD = Field(tokenize = tokenize_de,\n",
        "            init_token = '<sos>',\n",
        "            eos_token = '<eos>',\n",
        "            lower = True,\n",
        "            include_lengths = True\n",
        "            )\n",
        "FR_FIELD = Field(tokenize = tokenize_fr,\n",
        "            init_token = '<sos>',\n",
        "            eos_token = '<eos>',\n",
        "            lower = True,\n",
        "            include_lengths = True\n",
        "            )"
      ],
      "metadata": {
        "id": "a_ikcgwxfLvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "jopehmrVTRSm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_vocab(vocab, path):\n",
        "  with open(path, 'w+', encoding='utf-8') as f:     \n",
        "    for token, index in vocab.stoi.items():\n",
        "      f.write(f'{index}\\t{token}\\n')\n",
        "def read_vocab(path):\n",
        "  vocab = dict()\n",
        "  with open(path, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "      index, token = line.split('\\t')\n",
        "      vocab[token] = int(index)\n",
        "  return vocab\n",
        "# https://discuss.pytorch.org/t/how-to-save-and-load-torchtext-data-field-build-vocab-result/50407/3\n",
        "# save_vocab(TRG_FIELD.vocab, '/content/gdrive/MyDrive/Colab Notebooks/eaai24/Datasets/seq2seq-enfr-trg_vocab.txt')\n",
        "# LOAD_FIELD.vocab = read_vocab('/content/gdrive/MyDrive/Colab Notebooks/eaai24/Datasets/seq2seq-enfr-src_vocab.txt')"
      ],
      "metadata": {
        "id": "lqFlyKPUq1Bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with open('/content/gdrive/MyDrive/Colab Notebooks/eaai24/Datasets/enfr_160kpairs_2k5-freq-words.pkl', 'rb') as f:\n",
        "#   data = pickle.load(f)\n",
        "# data[80], len(data)"
      ],
      "metadata": {
        "id": "pHT8AUMJe7PF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8eae8908-eba0-4f1c-c343-55c65e97f14f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'en': 'It should continue along this path.',\n",
              "  'fr': 'Elle devrait poursuivre dans cette voie.'},\n",
              " 158629)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/gdrive/MyDrive/Colab Notebooks/eaai24/Datasets/endefr_75kpairs_2k5-freq-words.pkl', 'rb') as f:\n",
        "  data = pickle.load(f)\n",
        "data[8], len(data)"
      ],
      "metadata": {
        "id": "g8r6JHsEXCDI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e95f46fe-a37f-462a-8cf1-69aaaa0ebbc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'en': 'I would urge you to endorse this.',\n",
              "  'de': 'Ich bitte Sie um Zustimmung.',\n",
              "  'fr': 'Je vous demande de voter en faveur de son adoption.'},\n",
              " 75002)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_len = 64000\n",
        "valid_len = 3200\n",
        "test_len = 6400\n",
        "\n",
        "train_pt = train_len\n",
        "valid_pt = train_pt + valid_len\n",
        "test_pt = valid_pt + test_len"
      ],
      "metadata": {
        "id": "T5DlQGmUoKoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For 2 langs\n",
        "# data_set = [[pair['en'].lower(), pair['fr'].lower()] for pair in data]\n",
        "# FIELDS = [('src', SRC_FIELD), ('trg', TRG_FIELD)]\n",
        "# train_examples = list(map(lambda x: Example.fromlist(list(x), fields=FIELDS), data_set[: train_pt]))\n",
        "# valid_examples = list(map(lambda x: Example.fromlist(list(x), fields=FIELDS), data_set[train_pt : valid_pt]))\n",
        "# test_examples = list(map(lambda x: Example.fromlist(list(x), fields=FIELDS), data_set[valid_pt : test_pt]))\n",
        "\n",
        "# For 3 langs\n",
        "data_set = [[pair['en'], pair['de'], pair['fr']] for pair in data]\n",
        "FIELDS = [('en', EN_FIELD), ('de', DE_FIELD), ('fr', FR_FIELD)]\n",
        "train_examples = list(map(lambda x: Example.fromlist(list(x), fields=FIELDS), data_set[: train_pt]))\n",
        "valid_examples = list(map(lambda x: Example.fromlist(list(x), fields=FIELDS), data_set[train_pt : valid_pt]))\n",
        "test_examples = list(map(lambda x: Example.fromlist(list(x), fields=FIELDS), data_set[valid_pt : test_pt]))"
      ],
      "metadata": {
        "id": "cyFzD7FQfHBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dt = Dataset(train_examples, fields=FIELDS)\n",
        "valid_dt = Dataset(valid_examples, fields=FIELDS)\n",
        "test_dt = Dataset(test_examples, fields=FIELDS)"
      ],
      "metadata": {
        "id": "Mron6AMwfH6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EN_FIELD.build_vocab(train_dt, min_freq = 1) # choose 1 since data is already filter w/ most freq words\n",
        "DE_FIELD.build_vocab(train_dt, min_freq = 1)\n",
        "FR_FIELD.build_vocab(train_dt, min_freq = 1)\n",
        "len(EN_FIELD.vocab), len(DE_FIELD.vocab), len(FR_FIELD.vocab)"
      ],
      "metadata": {
        "id": "NfMdkYZSfS52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2699445-4267-40f0-b0ff-c977ee5dd7c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2267, 2474, 2390)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_dt, valid_dt, test_dt),\n",
        "     batch_size = BATCH_SIZE,\n",
        "     sort_within_batch = True,\n",
        "     sort_key = lambda x : len(x.en),\n",
        "     device = device)"
      ],
      "metadata": {
        "id": "9ZrebZgnkiuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, batch in enumerate(train_iterator):\n",
        "  print(batch.en[0].shape, batch.en[1], batch.de[0].shape, batch.de[1], batch.fr[0].shape, batch.fr[1])\n",
        "  if i==2: break\n",
        "batch.fields"
      ],
      "metadata": {
        "id": "XxntiGWVlA4G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce5d1bfb-6760-4f2e-f5b0-e15800ec4f8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 64]) tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
            "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
            "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]) torch.Size([8, 64]) tensor([5, 5, 5, 5, 8, 5, 5, 5, 5, 7, 5, 5, 4, 4, 5, 5, 5, 6, 4, 5, 5, 5, 6, 5,\n",
            "        5, 5, 5, 5, 4, 5, 5, 7, 5, 5, 5, 5, 5, 5, 7, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
            "        5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5]) torch.Size([11, 64]) tensor([10,  5, 10,  5, 10,  5,  5,  5,  5,  8,  5,  5,  5, 11,  5,  5,  5,  6,\n",
            "        11,  5,  5,  5,  6,  5,  5,  5,  5,  5, 11,  5,  5,  7,  5,  5,  5,  5,\n",
            "         5,  5,  9,  4,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
            "        11,  5,  5,  5,  5,  5,  5,  5,  5,  5])\n",
            "torch.Size([11, 64]) tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
            "        11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
            "        11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
            "        11, 11, 11, 11, 11, 11, 11, 11, 11, 11]) torch.Size([16, 64]) tensor([11, 16,  8, 10,  9, 11, 11, 10, 12, 10, 11,  9, 14, 11, 13,  9, 10, 10,\n",
            "        11, 15, 12, 10, 11,  9, 12, 12, 16, 11, 10, 11,  9, 13, 11, 16, 11, 10,\n",
            "        15, 10, 10,  9, 10, 14,  9, 11,  9, 12, 10, 14,  8, 10, 11, 12, 10,  8,\n",
            "        10,  9, 11, 11,  9, 13, 10, 15,  9,  8]) torch.Size([17, 64]) tensor([ 8, 13,  9, 11, 12, 14, 10, 15, 15, 11, 15, 11, 10, 11, 13,  9, 14, 10,\n",
            "        11, 14, 11, 14,  7, 13, 10, 10, 11, 15, 10, 11,  9, 13, 12, 13, 13, 13,\n",
            "        16, 10, 12, 12, 13, 15,  8, 11, 17, 15, 12,  7,  9, 11, 12, 12, 10,  9,\n",
            "         9, 12, 12, 13, 16, 11,  9, 13, 11, 13])\n",
            "torch.Size([4, 64]) tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
            "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
            "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]) torch.Size([11, 64]) tensor([ 4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4, 11,  7,  4,  4,  4,\n",
            "         4,  6,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  8,  4,  4,  4,  4,  4,\n",
            "         4,  4,  7,  4,  4,  4,  4,  6,  4,  4,  4,  4,  4,  4,  5,  4,  4,  4,\n",
            "         4,  4,  4,  6,  6,  4,  4,  4,  4,  4]) torch.Size([27, 64]) tensor([20, 20, 20,  4, 20, 20, 20, 20, 20, 20, 20, 20, 20, 21,  4, 20, 20, 20,\n",
            "         4,  4, 20, 20, 20, 20, 20, 20, 20,  4, 20,  4, 27,  4, 20, 20, 20, 20,\n",
            "        20, 20,  4,  4, 20, 20, 20,  4, 20, 20, 20, 20,  4, 20,  5, 20, 20, 20,\n",
            "        20, 20, 20,  4,  4, 20, 20, 20, 20,  5])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['en', 'de', 'fr'])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# batch.src"
      ],
      "metadata": {
        "id": "-e5emJChOLnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batch.trg"
      ],
      "metadata": {
        "id": "jHYT4FVsORlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# src_sent, piv_sent, trg_sent = [], [], []\n",
        "# for i in batch.src[0][: , 0]:\n",
        "#   src_sent.append(SRC_FIELD.vocab.itos[i])\n",
        "# for i in batch.piv[0][: , 0]:\n",
        "#   piv_sent.append(PIV_FIELD.vocab.itos[i])\n",
        "# for i in batch.trg[0][:, 0]:\n",
        "#   trg_sent.append(TRG_FIELD.vocab.itos[i])\n",
        "# print(' '.join(src_sent))\n",
        "# print(' '.join(piv_sent))\n",
        "# print(' '.join(trg_sent))"
      ],
      "metadata": {
        "id": "m7IO5L0nvEeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in range(5):\n",
        "#   print(SRC_FIELD.vocab.itos[i], PIV_FIELD.vocab.itos[i], TRG_FIELD.vocab.itos[i])"
      ],
      "metadata": {
        "id": "-XROmZP0olgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batch.src[0][:, 0]"
      ],
      "metadata": {
        "id": "YAMpLzcKoc7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "3Cp4hrsvTlPy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder"
      ],
      "metadata": {
        "id": "limqRnayUnj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "    self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
        "    self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, src, src_len):\n",
        "    #src = [src len, batch size]\n",
        "    #src_len = [batch size]\n",
        "    embedded = self.dropout(self.embedding(src))  #embedded = [src len, batch size, emb dim]\n",
        "\n",
        "    #need to explicitly put lengths on cpu!\n",
        "    packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len.to('cpu'))\n",
        "\n",
        "    #  when the input is a pad token are all zeros\n",
        "    packed_outputs, hidden = self.rnn(packed_embedded)\n",
        "    #packed_outputs is a packed sequence containing all hidden states\n",
        "    #hidden is now from the final non-padded element in the batch\n",
        "\n",
        "    outputs, len_list = nn.utils.rnn.pad_packed_sequence(packed_outputs) #outputs is now a non-packed sequence, all hidden states obtained\n",
        "    #  when the input is a pad token are all zeros\n",
        "    \n",
        "    #outputs = [src len, batch size, hid dim * num directions]\n",
        "    #hidden = [n layers * num directions, batch size, hid dim]\n",
        "    \n",
        "    #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
        "    #outputs are always from the last layer\n",
        "    \n",
        "    #hidden [-2, :, : ] is the last of the forwards RNN \n",
        "    #hidden [-1, :, : ] is the last of the backwards RNN\n",
        "    \n",
        "    #initial decoder hidden is final hidden state of the forwards and backwards \n",
        "    #  encoder RNNs fed through a linear layer\n",
        "    hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
        "\n",
        "    #outputs = [src len, batch size, enc hid dim * 2]\n",
        "    #hidden = [batch size, dec hid dim]\n",
        "    return outputs, hidden"
      ],
      "metadata": {
        "id": "kYO9-yg4UpEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attn"
      ],
      "metadata": {
        "id": "vJm8VpgbUsbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "  def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "    super().__init__()\n",
        "    self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
        "    self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
        "      \n",
        "  def forward(self, hidden, encoder_outputs, mask):\n",
        "    #hidden = [batch size, dec hid dim]\n",
        "    #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
        "    batch_size = encoder_outputs.shape[1]\n",
        "    src_len = encoder_outputs.shape[0]\n",
        "\n",
        "    #repeat decoder hidden state src_len times\n",
        "    hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)  #hidden = [batch size, src len, dec hid dim]\n",
        "    encoder_outputs = encoder_outputs.permute(1, 0, 2)  #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
        "    energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) #energy = [batch size, src len, dec hid dim]\n",
        "\n",
        "    attention = self.v(energy).squeeze(2) #attention = [batch size, src len]\n",
        "    attention = attention.masked_fill(mask == 0, -1e10)\n",
        "    return F.softmax(attention, dim = 1)"
      ],
      "metadata": {
        "id": "hthEkeIXU1lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder"
      ],
      "metadata": {
        "id": "7M3M0cKNUsfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
        "    super().__init__()\n",
        "    self.output_dim = output_dim\n",
        "    self.attention = attention\n",
        "    self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "    self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
        "    self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "      \n",
        "  def forward(self, input, hidden, encoder_outputs, mask):\n",
        "    #input = [batch size]\n",
        "    #hidden = [batch size, dec hid dim]\n",
        "    #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
        "    #mask = [batch size, src len]\n",
        "    input = input.unsqueeze(0)  #input = [1, batch size]\n",
        "    embedded = self.dropout(self.embedding(input))  #embedded = [1, batch size, emb dim]\n",
        "\n",
        "    a = self.attention(hidden, encoder_outputs, mask) #a = [batch size, src len]\n",
        "    a = a.unsqueeze(1)  #a = [batch size, 1, src len]\n",
        "\n",
        "    encoder_outputs = encoder_outputs.permute(1, 0, 2)  #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
        "\n",
        "    weighted = torch.bmm(a, encoder_outputs)  #weighted = [batch size, 1, enc hid dim * 2]\n",
        "    weighted = weighted.permute(1, 0, 2)  #weighted = [1, batch size, enc hid dim * 2]\n",
        "\n",
        "    rnn_input = torch.cat((embedded, weighted), dim = 2)  #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n",
        "\n",
        "    output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
        "    #output = [seq len, batch size, dec hid dim * n directions]\n",
        "    #hidden = [n layers * n directions, batch size, dec hid dim]\n",
        "    \n",
        "    #seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
        "    #output = [1, batch size, dec hid dim]\n",
        "    #hidden = [1, batch size, dec hid dim]\n",
        "    #this also means that output == hidden\n",
        "    assert (output == hidden).all()\n",
        "    \n",
        "    embedded = embedded.squeeze(0)\n",
        "    output = output.squeeze(0)\n",
        "    weighted = weighted.squeeze(0)\n",
        "    \n",
        "    prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))  #prediction = [batch size, output dim]\n",
        "    return prediction, hidden.squeeze(0), a.squeeze(1)"
      ],
      "metadata": {
        "id": "JVIADpjwU48J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Seq2Seq"
      ],
      "metadata": {
        "id": "WwSO5W3KUsjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, encoder, decoder, src_pad_idx, device):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.src_pad_idx = src_pad_idx\n",
        "    self.device = device\n",
        "      \n",
        "  def create_mask(self, src):\n",
        "    mask = (src != self.src_pad_idx).permute(1, 0)\n",
        "    return mask\n",
        "      \n",
        "  def forward(self, datas, criterion=None, teacher_forcing_ratio = 0.5):\n",
        "    #src = [src len, batch size]\n",
        "    #src_len = [batch size]\n",
        "    #trg = [trg len, batch size]\n",
        "    #trg_len = [batch size]\n",
        "    #teacher_forcing_ratio is probability of using trg to be input else prev output to be input for next prediction.\n",
        "    src, src_len, trg, trg_len = datas\n",
        "    batch_size = src.shape[1]\n",
        "    trg_len = trg.shape[0]\n",
        "    trg_vocab_size = self.decoder.output_dim\n",
        "    \n",
        "    #tensor to store decoder outputs\n",
        "    outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "    \n",
        "    #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
        "    #hidden is the final forward and backward hidden states, passed through a linear layer\n",
        "    encoder_outputs, hidden = self.encoder(src, src_len)\n",
        "\n",
        "    #first input to the decoder is the <sos> tokens\n",
        "    input = trg[0,:]\n",
        "    \n",
        "    mask = self.create_mask(src)  #mask = [batch size, src len]\n",
        "            \n",
        "    for t in range(1, trg_len):\n",
        "      #insert input token embedding, previous hidden state, all encoder hidden states and mask\n",
        "      #receive output tensor (predictions) and new hidden state\n",
        "      output, hidden, _ = self.decoder(input, hidden, encoder_outputs, mask)\n",
        "      \n",
        "      #place predictions in a tensor holding predictions for each token\n",
        "      outputs[t] = output\n",
        "      \n",
        "      #if teacher forcing, use actual next token as next input. Else, use predicted token\n",
        "      input = trg[t] if random.random() < teacher_forcing_ratio else output.argmax(1)\n",
        "    \n",
        "    if criterion != None:\n",
        "      loss = self.compute_loss(outputs, trg, criterion)\n",
        "      return loss, outputs\n",
        "    return outputs\n",
        "  \n",
        "  def compute_loss(self, output, trg, criterion):\n",
        "    #output = (trg_len, batch_size, trg_vocab_size)\n",
        "    #trg = [trg len, batch size]\n",
        "    output = output[1:].view(-1, output.shape[-1])  #output = [(trg len - 1) * batch size, output dim]\n",
        "    trg = trg[1:].view(-1)  #trg = [(trg len - 1) * batch size]\n",
        "    loss = criterion(output, trg)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "Pyz0ZITCU9r5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pivot model (update)"
      ],
      "metadata": {
        "id": "fdjE61pw7CBy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**still need to reorganize code to use for infer (no criterions, only 1 data: src, src_len)**"
      ],
      "metadata": {
        "id": "qt2bmz1GTTnj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PivotSeq2Seq(nn.Module):\n",
        "  def __init__(self, models: list, fields: list, device, lamda=0.75):\n",
        "    super().__init__()\n",
        "    self.num_model = len(models)\n",
        "    self.fields = fields\n",
        "    self.num_field = len(fields)\n",
        "    self.device = device\n",
        "    self.lamda = lamda\n",
        "    \n",
        "    for i in range(self.num_model):\n",
        "      self.add_module(f'model_{i}', models[i])\n",
        "    \n",
        "    assert len(models)+1 == len(fields), f\"Not enough Fields for models: num_field={len(fields)} != {len(models)+1}\"\n",
        "      \n",
        "  def forward(self, datas: list, criterions=None, teacher_forcing_ratio=0.5):\n",
        "    '''\n",
        "    datas: list of data: [(src, src_len), (piv1, piv_len1), ... , (pivM, piv_lenM), (trg, trg_len)] given M models\n",
        "      src = [src len, batch_size]\n",
        "      src_len = [batch_size]\n",
        "      ...\n",
        "      trg = [trg len, batch_size]\n",
        "      trg_len = [batch_size]\n",
        "    criterions: list of criterion for each model\n",
        "    '''\n",
        "    if criterions != None:\n",
        "      loss_list, output_list = self.run(datas, criterions, teacher_forcing_ratio)\n",
        "      total_loss = self.compute_loss(loss_list)\n",
        "      return total_loss, output_list[-1]\n",
        "    else:\n",
        "      criterions = [None for _ in range(self.num_model)]\n",
        "      _, output_list = self.run(datas, criterions, teacher_forcing_ratio)\n",
        "      return output_list[-1]\n",
        "    \n",
        "  def run(self, datas, criterions, teacher_forcing_ratio):\n",
        "    assert self.num_model+1 == len(datas), f\"Not enough datas for models: data_len={len(datas)} != {self.num_model+1}\"\n",
        "    assert self.num_model == len(criterions), f'Criterions must have for each model: num_criterion={len(criterions)} != {self.num_model}'\n",
        "\n",
        "    output_list = []\n",
        "    loss_list = []\n",
        "    for i in range(self.num_model):\n",
        "      # 1st model must always use src\n",
        "      isForceOn = True if i==0 else random.random() < teacher_forcing_ratio\n",
        "\n",
        "      src, src_len = datas[i] if isForceOn else self.process_output(output_list[-1], self.fields[i+1])      \n",
        "      src, src_len, datas = self.sort_by_src_len(src, src_len, datas)\n",
        "      trg, trg_len = datas[i+1]\n",
        "      data = [src, src_len, trg, trg_len]\n",
        "      model = getattr(self, f'model_{i}')\n",
        "      criterion = criterions[i]\n",
        "      output = model(data, criterion, 0 if criterion==None else teacher_forcing_ratio)\n",
        "      \n",
        "      if criterions[i] == None:\n",
        "        output_list.append(output)\n",
        "      else:\n",
        "        assert len(output) == 2, 'With criterion, model should return loss & prediction'\n",
        "        loss_list.append(output[0])\n",
        "        output_list.append(output[1])\n",
        "    \n",
        "    return loss_list, output_list\n",
        "\n",
        "  def compute_loss(self, loss_list):\n",
        "    total_loss = 0.0\n",
        "    for loss in loss_list:\n",
        "      total_loss += loss\n",
        "    return total_loss + self.lamda*self.compute_embed_loss()\n",
        "  \n",
        "  def compute_embed_loss(self):\n",
        "    # decoder.embedding.weight = [piv_vocab_dim, emb_dim]\n",
        "    # F.pairwise_distance = [piv_vocab_dim] --> distance for each vector along emb_dim\n",
        "    embed_loss = 0.0\n",
        "    for i in range(1, self.num_model):\n",
        "      model1 = getattr(self, f'model_{i-1}')\n",
        "      model2 = getattr(self, f'model_{i}')\n",
        "      embed_loss += torch.sum(F.pairwise_distance(model1.decoder.embedding.weight, model2.encoder.embedding.weight, p=2))\n",
        "    return embed_loss\n",
        "  \n",
        "  def sort_by_src_len(self, piv, piv_len, datas): # must sort all datas, otherwise will cause mismatch in src-trg pairs\n",
        "    # piv = [piv_len, batch_size]\n",
        "    piv_len, sorted_ids = piv_len.sort(descending=True)\n",
        "    sorted_datas = [(sent[:, sorted_ids], sent_len[sorted_ids]) for (sent, sent_len) in datas]\n",
        "    return piv[:, sorted_ids], piv_len, sorted_datas  # piv sorted along batch_size\n",
        "\n",
        "  def process_output(self, output, piv_field):\n",
        "    # output = [trg len, batch size, output dim]\n",
        "    # trg = [trg len, batch size]\n",
        "    # Process output1 to be input for model2\n",
        "    seq_len, N, _ = output.shape\n",
        "    tmp_out = output.argmax(2)  # tmp_out = [seq_len, batch_size]\n",
        "    # re-create pivot as src for model2\n",
        "    piv = torch.zeros_like(tmp_out).type(torch.long).to(output.device)\n",
        "    piv[0, :] = torch.full_like(piv[0, :], piv_field.vocab.stoi[piv_field.init_token])  # fill all first idx with sos_token\n",
        "    \n",
        "    for i in range(1, tmp_out.shape[0]):  # for each i in seq_len\n",
        "      # if tmp_out's prev is eos_token, replace w/ pad_token, else current value\n",
        "      eos_mask = (tmp_out[i-1, :] == piv_field.vocab.stoi[piv_field.eos_token])\n",
        "      piv[i, :] = torch.where(eos_mask, piv_field.vocab.stoi[piv_field.pad_token], tmp_out[i, :])\n",
        "      # if piv's prev is pad_token, replace w/ pad_token, else current value\n",
        "      pad_mask = (piv[i-1, :] == piv_field.vocab.stoi[piv_field.pad_token])\n",
        "      piv[i, :] = torch.where(pad_mask, piv_field.vocab.stoi[piv_field.pad_token], piv[i, :])\n",
        "    \n",
        "    # Trim down extra pad tokens\n",
        "    tensor_list = [piv[i] for i in range(seq_len) if not all(piv[i] == piv_field.vocab.stoi[piv_field.pad_token])]  # tensor_list = [new_seq_len, batch_size]\n",
        "    piv = torch.stack([x for x in tensor_list], dim=0).type(torch.long).to(output.device)\n",
        "    assert not all(piv[-1] == piv_field.vocab.stoi[piv_field.pad_token]), 'Not completely trim down tensor'\n",
        "\n",
        "    # get seq_id + eos_tok id of each sequence\n",
        "    piv_ids, eos_ids = (piv.permute(1, 0) == piv_field.vocab.stoi[piv_field.eos_token]).nonzero(as_tuple=True)  # piv_len = [N]\n",
        "    piv_len = torch.full_like(piv[0], piv.shape[0]).type(torch.long)  # init w/ longest seq\n",
        "    piv_len[piv_ids] = eos_ids + 1 # seq_len = eos_tok + 1\n",
        "    \n",
        "    return piv, piv_len"
      ],
      "metadata": {
        "id": "kdQAxGHj7E2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Triangulate model"
      ],
      "metadata": {
        "id": "KgwBXdjq3IkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TriangSeq2Seq(nn.Module):\n",
        "  def __init__(self, models: list, output_dim, device):\n",
        "    # output_dim = trg vocab size\n",
        "    super().__init__()\n",
        "    self.num_model = len(models)\n",
        "    self.device = device\n",
        "    self.output_dim = output_dim\n",
        "    self.final_fc = torch.nn.Linear(output_dim*len(models), output_dim)\n",
        "    \n",
        "    for i in range(self.num_model):\n",
        "      self.add_module(f'model_{i}', models[i])\n",
        "          \n",
        "  def forward(self, datas: dict, criterions=None, teacher_forcing_ratio=0.5):\n",
        "    '''\n",
        "    datas: dict of data:\n",
        "      {\"model_0\": (src, src_len, trg, trg_len), \"model_1\": [(src, src_len), (piv, piv_len), (trg, trg_len)], ..., \"TRG\": (trg, trg_len)}\n",
        "      src = [src len, batch size]\n",
        "      src_len = [batch size]\n",
        "      ...\n",
        "      trg = [trg len, batch size]\n",
        "      trg_len = [batch size]\n",
        "    criterions: dict of criterions\n",
        "      {\"model_0\": criterion_0, \"model_1\": criterion_1, ..., \"TRG\": criterion_M}\n",
        "    '''\n",
        "    if criterions != None:\n",
        "      loss_list, output_list = self.run(datas, criterions, teacher_forcing_ratio)\n",
        "      assert len(loss_list)==len(output_list) and len(output_list)==self.num_model, 'DO NOT MATCH: len(loss_list)=len(output_list) OR len(output_list)=self.num_model'\n",
        "      # TODO: calculate final_out from all outputs\n",
        "      final_out = self.get_final_pred(output_list)\n",
        "      # TODO: submodels_loss + final output's loss ==> total_loss\n",
        "      total_loss = self.compute_final_pred_loss(final_out, datas[\"TRG\"], criterions[\"TRG\"]) + self.compute_submodels_loss(loss_list)\n",
        "      return total_loss, final_out\n",
        "    else:\n",
        "      criterions = [None for _ in range(self.num_model)]\n",
        "      loss_list, output_list = self.run(datas, criterions, teacher_forcing_ratio)\n",
        "      assert len(loss_list)==0 and len(output_list)==self.num_model, 'DO NOT MATCH: len(loss_list)=0 OR len(output_list)=self.num_model'\n",
        "      # TODO: calculate final_out from all outputs\n",
        "      final_out = self.get_final_pred(output_list)\n",
        "      return final_out\n",
        "\n",
        "  def run(self, datas, criterions, teacher_forcing_ratio):\n",
        "    assert self.num_model == len(datas), f\"Not enough datas for models: data_len={len(datas)} != {self.num_model+1}\"\n",
        "    assert self.num_model == len(criterions), f'Criterions must have for each model: num_criterion={len(criterions)} != {self.num_model}'\n",
        "\n",
        "    output_list = []\n",
        "    loss_list = []\n",
        "    for i in range(self.num_model):\n",
        "      # 1st model must always use src\n",
        "      isForceOn = True if i==0 else random.random() < teacher_forcing_ratio\n",
        "\n",
        "      data = datas[f'model_{i}']\n",
        "      model = getattr(self, f'model_{i}')\n",
        "      criterion = criterions[f'model_{i}']\n",
        "      output = model(data, criterion, 0 if criterion==None else teacher_forcing_ratio)\n",
        "      \n",
        "      if criterions[i] == None:\n",
        "        output_list.append(output)\n",
        "      else:\n",
        "        assert len(output) == 2, 'With criterion, model should return loss & prediction'\n",
        "        loss_list.append(output[0])\n",
        "        output_list.append(output[1])\n",
        "    \n",
        "    return loss_list, output_list\n",
        "\n",
        "  def compute_submodels_loss(self, loss_list):\n",
        "    total_loss = 0.0\n",
        "    for loss in loss_list:\n",
        "      total_loss += loss\n",
        "    return total_loss + self.lamda*self.compute_embed_loss()\n",
        "\n",
        "  def compute_final_pred_loss(self, output, trg, criterion):\n",
        "    #output = (trg_len, batch_size, trg_vocab_size)\n",
        "    #trg = [trg len, batch size]\n",
        "    output = output[1:].view(-1, output.shape[-1])  #output = [(trg len - 1) * batch size, output dim]\n",
        "    trg = trg[1:].view(-1)  #trg = [(trg len - 1) * batch size]\n",
        "    loss = criterion(output, trg)\n",
        "    return loss\n",
        "\n",
        "  def get_final_pred(self, output_list, method='weighted'):\n",
        "    # output_list[0] shape = [seq_len, N, out_dim]\n",
        "    # outputs must match shape because use the same trg, trg_len\n",
        "    assert all([output_list[i].shape == output_list[i-1].shape for i in range(1, len(output_list))]), 'all outputs must match shape [seq_len, N, out_dim]'\n",
        "    # MAX method: get max along seq_len b/w all outputs\n",
        "    if method=='max':\n",
        "      stack_dim = 2\n",
        "      all_outputs = torch.stack([out for out in output_list], dim=stack_dim)  # all_outputs = [seq_len, N, stack_dim, out_dim]\n",
        "      final_out, max_idx = torch.max(all_outputs, dim=stack_dim)  # final_out = [seq_len, N, out_dim]\n",
        "      return final_out\n",
        "    elif method=='weighted':\n",
        "      linear_in = torch.cat([out for out in output_list], dim=-1) # linear_in = [seq_len, N, out_dim * num_model]. Note that num_model = len(output_list)\n",
        "      final_out = self.final_fc(linear_in)  # final_out = [seq_len, N, out_dim]\n",
        "      return final_out\n",
        "    else:\n",
        "      return output_list[0]"
      ],
      "metadata": {
        "id": "FKCumOEK3T6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train func"
      ],
      "metadata": {
        "id": "2cDqYW-LTnGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trainSeq2Seq(model, iterator, optimizer, criterion, clip):\n",
        "  model.train()\n",
        "  epoch_loss = 0.0\n",
        "  for batch in tqdm(iterator):\n",
        "    optimizer.zero_grad()\n",
        "    src, src_len = batch.src\n",
        "    trg, trg_len = batch.trg # trg = [trg len, batch size]\n",
        "    datas = [src, src_len, trg, trg_len]\n",
        "    loss, output = model(datas, criterion)\n",
        "\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "    optimizer.step()\n",
        "    epoch_loss += loss.item()\n",
        "  return epoch_loss / len(iterator)\n",
        "\n",
        "def evaluateSeq2Seq(model, iterator, criterion):\n",
        "  model.eval()\n",
        "  epoch_loss = 0.0\n",
        "  with torch.no_grad():\n",
        "    for batch in tqdm(iterator):\n",
        "      src, src_len = batch.src\n",
        "      trg, trg_len = batch.trg # trg = [trg len, batch size]\n",
        "      datas = [src, src_len, trg, trg_len]\n",
        "      loss, output = model(datas, criterion, 0) # turn off teacher forcing\n",
        "      epoch_loss += loss.item()\n",
        "    return epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "PyiHo_rj7b3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trainPivot(model, iterator, optimizer, criterions, clip):\n",
        "  model.train()\n",
        "  epoch_loss = 0.0\n",
        "  for batch in tqdm(iterator):\n",
        "    optimizer.zero_grad()\n",
        "    model_inputs = [batch.en, batch.de, batch.fr]\n",
        "    loss, output = model(model_inputs, criterions)\n",
        "\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "    optimizer.step()\n",
        "    epoch_loss += loss.item()\n",
        "  return epoch_loss / len(iterator)\n",
        "\n",
        "def evaluatePivot(model, iterator, criterions):\n",
        "  model.eval()\n",
        "  epoch_loss = 0.0\n",
        "  with torch.no_grad():\n",
        "    for batch in tqdm(iterator):\n",
        "      model_inputs = [batch.en, batch.de, batch.fr]\n",
        "      loss, output = model(model_inputs, criterions, 0)\n",
        "      epoch_loss += loss.item()\n",
        "    return epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "Au60zQ85n8No"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for triangulate model"
      ],
      "metadata": {
        "id": "TsX8opIkn7y7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_trainlog(data: list, filename: str='/content/gdrive/MyDrive/Colab Notebooks/eaai24/training_log.txt'):\n",
        "  ''' Update training log w/ new losses\n",
        "  Args:\n",
        "      data (List): a list of infor for many epochs as tuple, each tuple has model_name, loss, etc.\n",
        "      filename (String): path + file_name\n",
        "  Return:\n",
        "      None: new data is appended into train-log\n",
        "  '''\n",
        "  with open(filename, 'a') as f: # save\n",
        "    for epoch in data:\n",
        "      f.write(','.join(epoch))\n",
        "      f.write(\"\\n\")\n",
        "  print('update_trainlog SUCCESS')\n",
        "  return []"
      ],
      "metadata": {
        "id": "iaocGp1dS3RJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "MYFp2rpQUEbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # For 2 langs\n",
        "# INPUT_DIM = 2500  #len(SRC_FIELD.vocab) since vocab is selected from most 2k5 freq words\n",
        "# OUTPUT_DIM = 2500 #len(TRG_FIELD.vocab)\n",
        "# ENC_EMB_DIM = 256\n",
        "# DEC_EMB_DIM = 256\n",
        "# ENC_HID_DIM = 512\n",
        "# DEC_HID_DIM = 512\n",
        "# ENC_DROPOUT = 0.5\n",
        "# DEC_DROPOUT = 0.5\n",
        "# LR = 0.001\n",
        "# SRC_PAD_IDX = SRC_FIELD.vocab.stoi[SRC_FIELD.pad_token]\n",
        "\n",
        "# attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "# enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "# dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "\n",
        "# model = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)"
      ],
      "metadata": {
        "id": "9b_1FE6ZlpEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For 3 langs\n",
        "INPUT_DIM = 2500  #len(SRC_FIELD.vocab) since vocab is selected from most 2k5 freq words\n",
        "PIV_DIM = 2500  #len(PIV_FIELD.vocab)\n",
        "OUTPUT_DIM = 2500 #len(TRG_FIELD.vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "ENC_HID_DIM = 512\n",
        "DEC_HID_DIM = 512\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "LR = 0.001\n",
        "\n",
        "SRC_PAD_IDX = EN_FIELD.vocab.stoi[EN_FIELD.pad_token]\n",
        "PIV_PAD_IDX = DE_FIELD.vocab.stoi[DE_FIELD.pad_token]\n",
        "\n",
        "attn1 = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "enc1 = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "dec1 = Decoder(PIV_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn1)\n",
        "model1 = Seq2Seq(enc1, dec1, SRC_PAD_IDX, device).to(device)\n",
        "\n",
        "attn2 = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "enc2 = Encoder(PIV_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "dec2 = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn2)\n",
        "model2 = Seq2Seq(enc2, dec2, PIV_PAD_IDX, device).to(device)\n",
        "\n",
        "models = [model1, model2]\n",
        "fields = [EN_FIELD, DE_FIELD, FR_FIELD]\n",
        "model = PivotSeq2Seq(models, fields, device).to(device)"
      ],
      "metadata": {
        "id": "zhpJPc2DaIHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(m):\n",
        "  for name, param in m.named_parameters():\n",
        "    if 'weight' in name:\n",
        "      nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "    else:\n",
        "      nn.init.constant_(param.data, 0)\n",
        "            \n",
        "model.apply(init_weights)"
      ],
      "metadata": {
        "id": "M7CeajzjlzWj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6727ac32-183f-41d2-d284-49c476555826"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PivotSeq2Seq(\n",
              "  (model_0): Seq2Seq(\n",
              "    (encoder): Encoder(\n",
              "      (embedding): Embedding(2500, 256)\n",
              "      (rnn): GRU(256, 512, bidirectional=True)\n",
              "      (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
              "      (dropout): Dropout(p=0.5, inplace=False)\n",
              "    )\n",
              "    (decoder): Decoder(\n",
              "      (attention): Attention(\n",
              "        (attn): Linear(in_features=1536, out_features=512, bias=True)\n",
              "        (v): Linear(in_features=512, out_features=1, bias=False)\n",
              "      )\n",
              "      (embedding): Embedding(2500, 256)\n",
              "      (rnn): GRU(1280, 512)\n",
              "      (fc_out): Linear(in_features=1792, out_features=2500, bias=True)\n",
              "      (dropout): Dropout(p=0.5, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (model_1): Seq2Seq(\n",
              "    (encoder): Encoder(\n",
              "      (embedding): Embedding(2500, 256)\n",
              "      (rnn): GRU(256, 512, bidirectional=True)\n",
              "      (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
              "      (dropout): Dropout(p=0.5, inplace=False)\n",
              "    )\n",
              "    (decoder): Decoder(\n",
              "      (attention): Attention(\n",
              "        (attn): Linear(in_features=1536, out_features=512, bias=True)\n",
              "        (v): Linear(in_features=512, out_features=1, bias=False)\n",
              "      )\n",
              "      (embedding): Embedding(2500, 256)\n",
              "      (rnn): GRU(1280, 512)\n",
              "      (fc_out): Linear(in_features=1792, out_features=2500, bias=True)\n",
              "      (dropout): Dropout(p=0.5, inplace=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=LR)"
      ],
      "metadata": {
        "id": "edsBDaCEmQD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html#torch.optim.lr_scheduler.StepLR\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.333)"
      ],
      "metadata": {
        "id": "TQzaKEN2bX1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion1 = nn.CrossEntropyLoss(ignore_index = DE_FIELD.vocab.stoi[DE_FIELD.pad_token])\n",
        "criterion2 = nn.CrossEntropyLoss(ignore_index = FR_FIELD.vocab.stoi[FR_FIELD.pad_token])\n",
        "criterions = (criterion1, criterion2)"
      ],
      "metadata": {
        "id": "V_5TFOZsmPsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_EPOCHS = 1\n",
        "CLIP = 1\n",
        "best_valid_loss = float('inf')\n",
        "best_train_loss = float('inf')\n",
        "model_name = 'attn_enfr_160kset_3.pt'\n",
        "train_log = []"
      ],
      "metadata": {
        "id": "FIYc-s0FTPRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(N_EPOCHS):\n",
        "  # For 2 langs\n",
        "  # train_loss = train(model, train_iterator, optimizer, criterion2, CLIP)\n",
        "  # valid_loss = evaluate(model, valid_iterator, criterion2)\n",
        "\n",
        "  # For 3 langs\n",
        "  train_loss = trainPivot(model, train_iterator, optimizer, criterions, CLIP)\n",
        "  valid_loss = evaluatePivot(model, valid_iterator, criterions)\n",
        "  \n",
        "  scheduler.step()\n",
        "  \n",
        "  epoch_info = [model_name, scheduler.get_last_lr()[0], BATCH_SIZE, ENC_HID_DIM, 'no-num_layers', ENC_DROPOUT, DEC_DROPOUT, epoch, N_EPOCHS, train_loss, valid_loss]\n",
        "  train_log.append([str(ele) for ele in epoch_info])\n",
        "\n",
        "  # if train_loss < best_train_loss or valid_loss < best_valid_loss:\n",
        "  #   best_train_loss = train_loss\n",
        "  #   best_valid_loss = valid_loss\n",
        "  #   torch.save({\n",
        "  #       'model_state_dict': model.state_dict(),\n",
        "  #       'optimizer_state_dict': optimizer.state_dict(),\n",
        "  #       'scheduler_state_dict': scheduler.state_dict()\n",
        "  #   }, f'/content/gdrive/MyDrive/Colab Notebooks/eaai24/{model_name}')\n",
        "  #   print('SAVED MODEL')\n",
        "  #   train_log = update_trainlog(train_log)\n",
        "  \n",
        "  print(f'Epoch: {epoch:02} \\t Train Loss: {train_loss:.3f} \\t Val. Loss: {valid_loss:.3f}')"
      ],
      "metadata": {
        "id": "3ngIAH5cldRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eval"
      ],
      "metadata": {
        "id": "jRFofx7s1jJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FIELDS = [('src', EN_SRC), ('trg', FR_TRG)]\n",
        "# start_idx = train_len + valid_len\n",
        "# end_idx = start_idx + 6400\n",
        "# test_examples = list(map(lambda x: Example.fromlist(list(x), fields=FIELDS), data_set[start_idx : end_idx]))\n",
        "# test_dt = Dataset(test_examples, fields=FIELDS)\n",
        "# test_iterator1 = BucketIterator(\n",
        "#     test_dt,\n",
        "#      batch_size = BATCH_SIZE,\n",
        "#      sort_within_batch = True,\n",
        "#      sort_key = lambda x : len(x.src),\n",
        "#      device = device)"
      ],
      "metadata": {
        "id": "wEb4SPEC4bnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in test_iterator:\n",
        "  break\n",
        "batch.fields"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qS7TxmFB40s9",
        "outputId": "0b71f167-0c53-40a9-9494-7ca1e7299721"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['src', 'trg'])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # For 2 langs\n",
        "INPUT_DIM = 2463  #len(SRC_FIELD.vocab)\n",
        "OUTPUT_DIM = 2495 #len(TRG_FIELD.vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "ENC_HID_DIM = 512\n",
        "DEC_HID_DIM = 512\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "LR = 0.001\n",
        "SRC_PAD_IDX = EN_FIELD.vocab.stoi[EN_FIELD.pad_token]\n",
        "\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "\n",
        "model_infer = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)\n",
        "model_path = f'/content/gdrive/MyDrive/Colab Notebooks/eaai24/attn_enfr_160kset.pt'\n",
        "model_infer.load_state_dict(torch.load(model_path)['model_state_dict'])"
      ],
      "metadata": {
        "id": "ZM8Z2hbY37WQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f458674-12d8-452d-f6e7-1180e324757b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For 3 langs\n",
        "# INPUT_DIM = 2500  #len(SRC_FIELD.vocab)\n",
        "# PIV_DIM = 2500  #len(PIV_FIELD.vocab)\n",
        "# OUTPUT_DIM = 2500 #len(TRG_FIELD.vocab)\n",
        "# ENC_EMB_DIM = 256\n",
        "# DEC_EMB_DIM = 256\n",
        "# ENC_HID_DIM = 512\n",
        "# DEC_HID_DIM = 512\n",
        "# ENC_DROPOUT = 0.5\n",
        "# DEC_DROPOUT = 0.5\n",
        "# LR = 0.001\n",
        "\n",
        "# SRC_PAD_IDX = SRC_FIELD.vocab.stoi[SRC_FIELD.pad_token]\n",
        "# PIV_PAD_IDX = PIV_FIELD.vocab.stoi[PIV_FIELD.pad_token]\n",
        "\n",
        "# attn1 = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "# enc1 = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "# dec1 = Decoder(PIV_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn1)\n",
        "# model1 = Seq2Seq(enc1, dec1, SRC_PAD_IDX, device).to(device)\n",
        "\n",
        "# attn2 = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "# enc2 = Encoder(PIV_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "# dec2 = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn2)\n",
        "# model2 = Seq2Seq(enc2, dec2, PIV_PAD_IDX, device).to(device)\n",
        "\n",
        "# model_infer = PivotSeq2Seq(model1, model2, SRC_FIELD, PIV_FIELD, TRG_FIELD, device).to(device)\n",
        "# model_path = '/content/gdrive/MyDrive/Colab Notebooks/eaai24/piv_endefr_74kset_2.pt'\n",
        "# model_infer.load_state_dict(torch.load(model_path)['model_state_dict'])"
      ],
      "metadata": {
        "id": "OrZyj-ZxRaZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_path = f'/content/gdrive/MyDrive/Colab Notebooks/eaai24/attn_en-fr_32k_160kset_inverse.pt'\n",
        "# ckpt = torch.load(model_path)\n",
        "# model.load_state_dict(ckpt['model_state_dict']) # strict=False if some dimensions are different\n",
        "# optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
        "# scheduler.load_state_dict(ckpt['scheduler_state_dict'])"
      ],
      "metadata": {
        "id": "PwZxT4MZ8tV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss = evaluate(model_infer, test_iterator, criterion2, isPivot=False, force=0)\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "metadata": {
        "id": "blKOkBAe1m5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "cbS1GfWq2DmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sentence_seq2seq(sentence, src_field, trg_field, model: Seq2Seq, device, max_len = 50):\n",
        "  model.eval()\n",
        "  if isinstance(sentence, str):\n",
        "    tokens = tokenize_en(sentence)\n",
        "  else:\n",
        "    tokens = [token.lower() for token in sentence]\n",
        "  tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
        "  src_indexes = [src_field.vocab.stoi[token] for token in tokens]    \n",
        "  src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)  # [seq_len, N] w/ N=1 for batch\n",
        "  src_len_tensor = torch.LongTensor([len(src_indexes)]).to(device)\n",
        "  trg_tensor = torch.LongTensor([trg_field.vocab.stoi[trg_field.init_token]] + [0 for i in range(1, max_len)]).view(-1, 1).to(device) # [seq_len, 1]\n",
        "\n",
        "  output = model(src_tensor, src_len_tensor, trg_tensor, criterion=None, teacher_forcing_ratio=0) # output = [trg_len, N, dec_emb_dim]\n",
        "  output = output.argmax(-1).squeeze(1).detach().cpu().numpy()\n",
        "  result = []\n",
        "  for i in output[1:]:\n",
        "    result.append(trg_field.vocab.itos[i])\n",
        "    if i == trg_field.vocab.stoi[trg_field.eos_token]: break\n",
        "  return result\n",
        "\n",
        "def translate_sentence_pivot(sentence, src_field, trg_field, model: PivotSeq2Seq, device, max_len = 50):\n",
        "  model.eval()\n",
        "  if isinstance(sentence, str):\n",
        "    tokens = tokenize_en(sentence)\n",
        "  else:\n",
        "    tokens = [token.lower() for token in sentence]\n",
        "  with torch.no_grad():\n",
        "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
        "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)  # [seq_len, N] w/ N=1 for batch_size\n",
        "    src_len_tensor = torch.LongTensor([len(src_indexes)]).to(device)\n",
        "    \n",
        "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]] + [0 for i in range(1, max_len)]\n",
        "    trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(1).to(device) # [seq_len, 1]\n",
        "    trg_len_tensor = torch.LongTensor([len(trg_indexes)]).to(device)\n",
        "\n",
        "    datas = [(src_tensor, src_len_tensor)] + [(trg_tensor.clone().detach().to(device), trg_len_tensor.clone().detach().to(device)) for _ in range(model.num_model)]\n",
        "    output = model(datas, criterions=None, teacher_forcing_ratio=0) # output = [trg_len, N, dec_emb_dim]\n",
        "    output = output.argmax(-1).squeeze(1).detach().cpu().numpy()\n",
        "    result = []\n",
        "    for i in output[1:]:\n",
        "      result.append(trg_field.vocab.itos[i])\n",
        "      if i == trg_field.vocab.stoi[trg_field.eos_token]: break\n",
        "    return result"
      ],
      "metadata": {
        "id": "c6d_Ggwz2EcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_idx = 432\n",
        "src = vars(valid_dt.examples[example_idx])['en']\n",
        "trg = vars(valid_dt.examples[example_idx])['fr']\n",
        "print(f'src = {src}')\n",
        "print(f'trg = {trg}')\n",
        "pred = translate_sentence_pivot(src, EN_FIELD, FR_FIELD, model, device)"
      ],
      "metadata": {
        "id": "M3bCCUKEDeZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BLEU"
      ],
      "metadata": {
        "id": "_gNHZNQk2e4M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main"
      ],
      "metadata": {
        "id": "celagCJTtFd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_bleu_old(data, src_field, trg_field, model, device, max_len = 50):\n",
        "  trgs = []\n",
        "  pred_trgs = []\n",
        "  for i, datum in tqdm(enumerate(data)):\n",
        "    src = vars(datum)['src']\n",
        "    trg = vars(datum)['trg']\n",
        "    pred_trg = translate_sentence_seq2seq(src, src_field, trg_field, model, device, max_len)\n",
        "    #cut off <eos> token\n",
        "    pred_trg = pred_trg[:-1]\n",
        "    pred_trgs.append(pred_trg)\n",
        "    trgs.append([trg])\n",
        "    if i==1500: break\n",
        "  return bleu_score(pred_trgs, trgs)\n",
        "\n",
        "def calculate_bleu(translator, data, src_field, trg_field, model, device, max_len = 50):\n",
        "  trgs = []\n",
        "  pred_trgs = []\n",
        "  for i, datum in tqdm(enumerate(data)):\n",
        "    src = vars(datum)['src']\n",
        "    trg = vars(datum)['trg']\n",
        "    pred = translator(src, src_field, trg_field, model, device, max_len)\n",
        "    #cut off <eos> token\n",
        "    pred_trgs.append(pred[:-1])\n",
        "    trgs.append([trg])\n",
        "    if i==2000: break\n",
        "  return bleu_score(pred_trgs, trgs)"
      ],
      "metadata": {
        "id": "gTwoCAVY2iib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_score = calculate_bleu(test_dt, EN_FIELD, FR_FIELD, model_infer, device)\n",
        "print(f'BLEU score = {bleu_score*100:.2f}')"
      ],
      "metadata": {
        "id": "UDzwuc3z2sDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_score = calculate_bleu_1piv(test_dt, EN_FIELD, DE_FIELD, FR_FIELD, model_infer, device, max_len = 50)\n",
        "print(f'BLEU score = {bleu_score*100:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMUJ2XL3ahPy",
        "outputId": "cbef3280-5e46-4c3a-d6db-0fff9d75d6b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2000it [00:54, 36.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU score = 26.33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IF error happens: index 4 is out of bounds for dimension 0 with size 4 ==> use this"
      ],
      "metadata": {
        "id": "yi7oQJcHs7KJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pred_trg_corpus(data, model_infer, device, SRC_FIELD, TRG_FIELD, PIV_FIELD=None, max_len=50):\n",
        "  candidate_corpus = []\n",
        "  references_corpus = []\n",
        "  for datum in tqdm(data):\n",
        "    src = vars(datum)['src']\n",
        "    trg = vars(datum)['trg']\n",
        "    if isinstance(model_infer, PivotSeq2Seq):\n",
        "      piv = vars(datum)['piv']\n",
        "      sent1, sent = translate_sentence_1piv(src, SRC_FIELD, PIV_FIELD, TRG_FIELD, model_infer, device, max_len=max_len)\n",
        "    else:\n",
        "      sent, attn = translate_sentence(src, SRC_FIELD, TRG_FIELD, model_infer, device, max_len=max_len)\n",
        "    candidate_corpus.append(sent[:-1])\n",
        "    references_corpus.append([trg])\n",
        "  return candidate_corpus, references_corpus"
      ],
      "metadata": {
        "id": "TTExRY2DnVC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "candidate_corpus, references_corpus = get_pred_trg_corpus(test_dt, model_infer, device, SRC_FIELD, TRG_FIELD, PIV_FIELD)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_a4WDhXZo3sJ",
        "outputId": "26aa6768-5387-4472-c33d-3c0a8865b55d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6400/6400 [02:43<00:00, 39.08it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "candidate_corpus[:1376], references_corpus[:1376]"
      ],
      "metadata": {
        "id": "CDkYu16Lq_tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 5000\n",
        "j = 100\n",
        "k = 30000\n",
        "bleu_score(candidate_corpus[: i]+candidate_corpus[i+j:k], references_corpus[: i]+references_corpus[i+j:k])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLDv3sEIo-Q9",
        "outputId": "7e0cdc70-b69a-4e5e-b7d5-0a2e88c90707"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.26010842469504236"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "candidate = ['hello', '']\n",
        "references = [['hello'], ['.']]\n",
        "bleu_score(candidate, references)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lU_iKKNCon4s",
        "outputId": "edfc8467-5ae7-48be-c8e9-3f58a6d433e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8187307530779819"
            ]
          },
          "metadata": {},
          "execution_count": 249
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Result"
      ],
      "metadata": {
        "id": "LcU3uVKbtI4X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* attn_en-fr_32k.pt: BLEU = 12.65\n",
        "* attn_enfr_160kset.pt: BLEU = 32.18\n",
        "* piv_endefr_74kset_2.pt: BLEU = 26.33\n"
      ],
      "metadata": {
        "id": "vIw-3jBB42zA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# End"
      ],
      "metadata": {
        "id": "_0vl6ryRfWl9"
      }
    }
  ]
}