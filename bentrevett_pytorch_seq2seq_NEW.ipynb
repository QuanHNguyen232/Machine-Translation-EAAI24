{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "f_Qg27JlTHS1",
        "jopehmrVTRSm",
        "limqRnayUnj5",
        "vJm8VpgbUsbK",
        "7M3M0cKNUsfb",
        "fdjE61pw7CBy",
        "KgwBXdjq3IkB"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "NEW TASKS:\n",
        "* [X] Seq2Seq: sort by src_len and unsort output --> ensure output matches with trg\n",
        "* [ ] Pivot model: ensure it works for $n$ seq2seq models\n",
        "* [ ] Trian model: ensure outputs from all submodels match"
      ],
      "metadata": {
        "id": "FCN31jhjnCzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# piv_endefr_74kset_2.pt using PivotModel in bentrevett/pytorch-seq2seq-OLD.ipynb"
      ],
      "metadata": {
        "id": "yTMYDLiGey7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/bentrevett/pytorch-seq2seq/blob/master/4%20-%20Packed%20Padded%20Sequences%2C%20Masking%2C%20Inference%20and%20BLEU.ipynb\n",
        "# based on https://gmihaila.github.io/tutorial_notebooks/pytorchtext_bucketiterator/#dataset-class"
      ],
      "metadata": {
        "id": "V6P0jGmKybf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "DEdOoJ93e008"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install torch==1.8.2 torchvision==0.9.2 torchaudio==0.8.2 --extra-index-url https://download.pytorch.org/whl/lts/1.8/cu111 -q"
      ],
      "metadata": {
        "id": "SA0jFbKHbfHP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a40a85d5-4019-4f44-ec16-00b263fc2bf8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 GB\u001b[0m \u001b[31m904.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.5/17.5 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.14.1 requires torch==1.13.1, but you have torch 1.8.2+cu111 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1754nP4obkXv",
        "outputId": "dda60a73-560b-4350-8151-89dedf35b3eb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.8.2+cu111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMlSEzldbP3q",
        "outputId": "c18f3902-6924-4ceb-e94c-53f9227ef8b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m735.5/735.5 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.9.2+cu111 requires torch==1.8.2, but you have torch 1.8.0 which is incompatible.\n",
            "torchaudio 0.8.2 requires torch==1.8.2, but you have torch 1.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install torchtext==0.9 -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm -q\n",
        "!python -m spacy download de_core_news_sm -q\n",
        "!python -m spacy download fr_core_news_sm -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dw6ZjirTbTE8",
        "outputId": "d8b5b02c-5f0d-4fe9-a56f-3f68e5d2a0f7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-03-07 00:11:30.245789: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-07 00:11:36.495366: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-07 00:11:36.495761: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-07 00:11:36.495792: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "2023-03-07 00:12:03.046992: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-07 00:12:04.507473: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-07 00:12:04.507597: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-07 00:12:04.507618: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n",
            "2023-03-07 00:12:21.747888: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-07 00:12:22.620507: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-07 00:12:22.620626: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-07 00:12:22.620646: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# from torchtext.legacy.datasets import Multi30k\n",
        "from torchtext.legacy.data import Field, BucketIterator\n",
        "from torchtext.legacy.data import Dataset, Example\n",
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import pickle\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "6N_lgaPWd4AX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# My Section"
      ],
      "metadata": {
        "id": "IZIH_L_HeyEJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "f_Qg27JlTHS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOnOF0x_PM0F",
        "outputId": "d4e7a4dc-3034-4fd0-e6ce-9a4e45e99b5c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spacy_de = spacy.load('de_core_news_sm')\n",
        "def tokenize_de(text):\n",
        "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
        "\n",
        "spacy_fr = spacy.load('fr_core_news_sm')\n",
        "def tokenize_fr(text):\n",
        "    return [tok.text for tok in spacy_fr.tokenizer(text)]\n",
        "\n",
        "spacy_en = spacy.load('en_core_web_sm')\n",
        "def tokenize_en(text):\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]"
      ],
      "metadata": {
        "id": "aXSlRvZqPRqU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EN_FIELD = Field(tokenize = tokenize_en, \n",
        "            init_token = '<sos>',\n",
        "            eos_token = '<eos>',\n",
        "            lower = True,\n",
        "            include_lengths = True\n",
        "            )\n",
        "DE_FIELD = Field(tokenize = tokenize_de,\n",
        "            init_token = '<sos>',\n",
        "            eos_token = '<eos>',\n",
        "            lower = True,\n",
        "            include_lengths = True\n",
        "            )\n",
        "FR_FIELD = Field(tokenize = tokenize_fr,\n",
        "            init_token = '<sos>',\n",
        "            eos_token = '<eos>',\n",
        "            lower = True,\n",
        "            include_lengths = True\n",
        "            )"
      ],
      "metadata": {
        "id": "a_ikcgwxfLvk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "jopehmrVTRSm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_vocab(vocab, path):\n",
        "  with open(path, 'w+', encoding='utf-8') as f:     \n",
        "    for token, index in vocab.stoi.items():\n",
        "      f.write(f'{index}\\t{token}\\n')\n",
        "def read_vocab(path):\n",
        "  vocab = dict()\n",
        "  with open(path, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "      index, token = line.split('\\t')\n",
        "      vocab[token] = int(index)\n",
        "  return vocab\n",
        "# https://discuss.pytorch.org/t/how-to-save-and-load-torchtext-data-field-build-vocab-result/50407/3\n",
        "# save_vocab(TRG_FIELD.vocab, '/content/gdrive/MyDrive/Colab Notebooks/eaai24/Datasets/seq2seq-enfr-trg_vocab.txt')\n",
        "# LOAD_FIELD.vocab = read_vocab('/content/gdrive/MyDrive/Colab Notebooks/eaai24/Datasets/seq2seq-enfr-src_vocab.txt')"
      ],
      "metadata": {
        "id": "lqFlyKPUq1Bp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/gdrive/MyDrive/Colab Notebooks/eaai24/Datasets/enfr_160kpairs_2k5-freq-words.pkl', 'rb') as f:\n",
        "  data = pickle.load(f)\n",
        "data[80], len(data)"
      ],
      "metadata": {
        "id": "pHT8AUMJe7PF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c257a7cc-00c6-4ab1-c3e9-9b0fcfd681bf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'en': 'It should continue along this path.',\n",
              "  'fr': 'Elle devrait poursuivre dans cette voie.'},\n",
              " 158629)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# with open('/content/gdrive/MyDrive/Colab Notebooks/eaai24/Datasets/endefr_75kpairs_2k5-freq-words.pkl', 'rb') as f:\n",
        "#   data = pickle.load(f)\n",
        "# data[8], len(data)"
      ],
      "metadata": {
        "id": "g8r6JHsEXCDI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_len = 64000\n",
        "valid_len = 3200\n",
        "test_len = 6400\n",
        "\n",
        "train_pt = train_len\n",
        "valid_pt = train_pt + valid_len\n",
        "test_pt = valid_pt + test_len"
      ],
      "metadata": {
        "id": "T5DlQGmUoKoF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For 2 langs\n",
        "data_set = [[pair['en'].lower(), pair['fr'].lower()] for pair in data]\n",
        "FIELDS = [('en', EN_FIELD), ('fr', FR_FIELD)]\n",
        "train_examples = list(map(lambda x: Example.fromlist(list(x), fields=FIELDS), data_set[: train_pt]))\n",
        "valid_examples = list(map(lambda x: Example.fromlist(list(x), fields=FIELDS), data_set[train_pt : valid_pt]))\n",
        "test_examples = list(map(lambda x: Example.fromlist(list(x), fields=FIELDS), data_set[valid_pt : test_pt]))\n",
        "\n",
        "# For 3 langs\n",
        "# data_set = [[pair['en'], pair['de'], pair['fr']] for pair in data]\n",
        "# FIELDS = [('en', EN_FIELD), ('de', DE_FIELD), ('fr', FR_FIELD)]\n",
        "# train_examples = list(map(lambda x: Example.fromlist(list(x), fields=FIELDS), data_set[: train_pt]))\n",
        "# valid_examples = list(map(lambda x: Example.fromlist(list(x), fields=FIELDS), data_set[train_pt : valid_pt]))\n",
        "# test_examples = list(map(lambda x: Example.fromlist(list(x), fields=FIELDS), data_set[valid_pt : test_pt]))"
      ],
      "metadata": {
        "id": "cyFzD7FQfHBC"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dt = Dataset(train_examples, fields=FIELDS)\n",
        "valid_dt = Dataset(valid_examples, fields=FIELDS)\n",
        "test_dt = Dataset(test_examples, fields=FIELDS)"
      ],
      "metadata": {
        "id": "Mron6AMwfH6y"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EN_FIELD.build_vocab(train_dt, min_freq = 1) # choose 1 since data is already filter w/ most freq words\n",
        "DE_FIELD.build_vocab(train_dt, min_freq = 1)\n",
        "FR_FIELD.build_vocab(train_dt, min_freq = 1)\n",
        "len(EN_FIELD.vocab), len(DE_FIELD.vocab), len(FR_FIELD.vocab)"
      ],
      "metadata": {
        "id": "NfMdkYZSfS52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6e4fae0-b641-4d3e-8afa-747cf0e4321a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2463, 4, 2495)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_dt, valid_dt, test_dt),\n",
        "     batch_size = BATCH_SIZE,\n",
        "     sort_within_batch = True,\n",
        "     sort_key = lambda x : len(x.en),\n",
        "     device = device)"
      ],
      "metadata": {
        "id": "9ZrebZgnkiuF"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, batch in enumerate(train_iterator):\n",
        "  print(batch.en[0].shape, batch.en[1])\n",
        "  # print(batch.de[0].shape, batch.de[1])\n",
        "  print(batch.fr[0].shape, batch.fr[1])\n",
        "  if i==0: break\n",
        "batch.fields"
      ],
      "metadata": {
        "id": "XxntiGWVlA4G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32a0ae73-a141-4d65-ef1b-0c651fa2e3ec"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([19, 64]) tensor([19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
            "        19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
            "        19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
            "        19, 19, 19, 19, 19, 19, 19, 18, 18, 18], device='cuda:0')\n",
            "torch.Size([29, 64]) tensor([17, 22, 23, 21, 17, 22, 14, 19, 26, 21, 18, 16, 18, 20, 20, 18, 21, 21,\n",
            "        29, 20, 24, 23, 21, 22, 18, 16, 22, 20, 17, 21, 17, 22, 22, 22, 24, 21,\n",
            "        17, 20, 20, 22, 23, 25, 27, 20, 16, 17, 20, 18, 19, 24, 20, 26, 18, 20,\n",
            "        21, 23, 22, 27, 17, 20, 14, 16, 19, 21], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['en', 'fr'])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# batch.src"
      ],
      "metadata": {
        "id": "-e5emJChOLnU"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batch.trg"
      ],
      "metadata": {
        "id": "jHYT4FVsORlO"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# src_sent, piv_sent, trg_sent = [], [], []\n",
        "# for i in batch.src[0][: , 0]:\n",
        "#   src_sent.append(SRC_FIELD.vocab.itos[i])\n",
        "# for i in batch.piv[0][: , 0]:\n",
        "#   piv_sent.append(PIV_FIELD.vocab.itos[i])\n",
        "# for i in batch.trg[0][:, 0]:\n",
        "#   trg_sent.append(TRG_FIELD.vocab.itos[i])\n",
        "# print(' '.join(src_sent))\n",
        "# print(' '.join(piv_sent))\n",
        "# print(' '.join(trg_sent))"
      ],
      "metadata": {
        "id": "m7IO5L0nvEeM"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in range(5):\n",
        "#   print(SRC_FIELD.vocab.itos[i], PIV_FIELD.vocab.itos[i], TRG_FIELD.vocab.itos[i])"
      ],
      "metadata": {
        "id": "-XROmZP0olgb"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batch.src[0][:, 0]"
      ],
      "metadata": {
        "id": "YAMpLzcKoc7W"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "3Cp4hrsvTlPy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder"
      ],
      "metadata": {
        "id": "limqRnayUnj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "    self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
        "    self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, src, src_len):\n",
        "    #src = [src len, batch size]\n",
        "    #src_len = [batch size]\n",
        "    embedded = self.dropout(self.embedding(src))  #embedded = [src len, batch size, emb dim]\n",
        "\n",
        "    #need to explicitly put lengths on cpu!\n",
        "    packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len.to('cpu'))\n",
        "\n",
        "    #  when the input is a pad token are all zeros\n",
        "    packed_outputs, hidden = self.rnn(packed_embedded)\n",
        "    #packed_outputs is a packed sequence containing all hidden states\n",
        "    #hidden is now from the final non-padded element in the batch\n",
        "\n",
        "    outputs, len_list = nn.utils.rnn.pad_packed_sequence(packed_outputs) #outputs is now a non-packed sequence, all hidden states obtained\n",
        "    #  when the input is a pad token are all zeros\n",
        "    \n",
        "    #outputs = [src len, batch size, hid dim * num directions]\n",
        "    #hidden = [n layers * num directions, batch size, hid dim]\n",
        "    \n",
        "    #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
        "    #outputs are always from the last layer\n",
        "    \n",
        "    #hidden [-2, :, : ] is the last of the forwards RNN \n",
        "    #hidden [-1, :, : ] is the last of the backwards RNN\n",
        "    \n",
        "    #initial decoder hidden is final hidden state of the forwards and backwards \n",
        "    #  encoder RNNs fed through a linear layer\n",
        "    hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
        "\n",
        "    #outputs = [src len, batch size, enc hid dim * 2]\n",
        "    #hidden = [batch size, dec hid dim]\n",
        "    return outputs, hidden"
      ],
      "metadata": {
        "id": "kYO9-yg4UpEo"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attn"
      ],
      "metadata": {
        "id": "vJm8VpgbUsbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "  def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "    super().__init__()\n",
        "    self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
        "    self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
        "      \n",
        "  def forward(self, hidden, encoder_outputs, mask):\n",
        "    #hidden = [batch size, dec hid dim]\n",
        "    #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
        "    batch_size = encoder_outputs.shape[1]\n",
        "    src_len = encoder_outputs.shape[0]\n",
        "\n",
        "    #repeat decoder hidden state src_len times\n",
        "    hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)  #hidden = [batch size, src len, dec hid dim]\n",
        "    encoder_outputs = encoder_outputs.permute(1, 0, 2)  #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
        "    energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) #energy = [batch size, src len, dec hid dim]\n",
        "\n",
        "    attention = self.v(energy).squeeze(2) #attention = [batch size, src len]\n",
        "    attention = attention.masked_fill(mask == 0, -1e10)\n",
        "    return F.softmax(attention, dim = 1)"
      ],
      "metadata": {
        "id": "hthEkeIXU1lp"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder"
      ],
      "metadata": {
        "id": "7M3M0cKNUsfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
        "    super().__init__()\n",
        "    self.output_dim = output_dim\n",
        "    self.attention = attention\n",
        "    self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "    self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
        "    self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "      \n",
        "  def forward(self, input, hidden, encoder_outputs, mask):\n",
        "    #input = [batch size]\n",
        "    #hidden = [batch size, dec hid dim]\n",
        "    #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
        "    #mask = [batch size, src len]\n",
        "    input = input.unsqueeze(0)  #input = [1, batch size]\n",
        "    embedded = self.dropout(self.embedding(input))  #embedded = [1, batch size, emb dim]\n",
        "\n",
        "    a = self.attention(hidden, encoder_outputs, mask) #a = [batch size, src len]\n",
        "    a = a.unsqueeze(1)  #a = [batch size, 1, src len]\n",
        "\n",
        "    encoder_outputs = encoder_outputs.permute(1, 0, 2)  #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
        "\n",
        "    weighted = torch.bmm(a, encoder_outputs)  #weighted = [batch size, 1, enc hid dim * 2]\n",
        "    weighted = weighted.permute(1, 0, 2)  #weighted = [1, batch size, enc hid dim * 2]\n",
        "\n",
        "    rnn_input = torch.cat((embedded, weighted), dim = 2)  #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n",
        "\n",
        "    output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
        "    #output = [seq len, batch size, dec hid dim * n directions]\n",
        "    #hidden = [n layers * n directions, batch size, dec hid dim]\n",
        "    \n",
        "    #seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
        "    #output = [1, batch size, dec hid dim]\n",
        "    #hidden = [1, batch size, dec hid dim]\n",
        "    #this also means that output == hidden\n",
        "    assert (output == hidden).all()\n",
        "    \n",
        "    embedded = embedded.squeeze(0)\n",
        "    output = output.squeeze(0)\n",
        "    weighted = weighted.squeeze(0)\n",
        "    \n",
        "    prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))  #prediction = [batch size, output dim]\n",
        "    return prediction, hidden.squeeze(0), a.squeeze(1)"
      ],
      "metadata": {
        "id": "JVIADpjwU48J"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Seq2Seq"
      ],
      "metadata": {
        "id": "WwSO5W3KUsjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, encoder, decoder, src_pad_idx, device):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.src_pad_idx = src_pad_idx\n",
        "    self.device = device\n",
        "      \n",
        "  def create_mask(self, src):\n",
        "    mask = (src != self.src_pad_idx).permute(1, 0)\n",
        "    return mask\n",
        "      \n",
        "  def forward(self, datas, criterion=None, teacher_forcing_ratio = 0.5):\n",
        "    #src = [src len, batch size]\n",
        "    #src_len = [batch size]\n",
        "    #trg = [trg len, batch size]\n",
        "    #trg_len = [batch size]\n",
        "    #teacher_forcing_ratio is probability of using trg to be input else prev output to be input for next prediction.\n",
        "    (src, src_len), (trg, _) = datas\n",
        "    batch_size = src.shape[1]\n",
        "    trg_len = trg.shape[0]\n",
        "    trg_vocab_size = self.decoder.output_dim\n",
        "    \n",
        "    # SORT\n",
        "    sort_ids, unsort_ids = self.sort_by_sent_len(src_len)\n",
        "    src, src_len, trg = src[:, sort_ids], src_len[sort_ids], trg[:, sort_ids]\n",
        "\n",
        "    #tensor to store decoder outputs\n",
        "    outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "    \n",
        "    #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
        "    #hidden is the final forward and backward hidden states, passed through a linear layer\n",
        "    encoder_outputs, hidden = self.encoder(src, src_len)\n",
        "\n",
        "    #first input to the decoder is the <sos> tokens\n",
        "    input = trg[0,:]\n",
        "    \n",
        "    mask = self.create_mask(src)  #mask = [batch size, src len]\n",
        "            \n",
        "    for t in range(1, trg_len):\n",
        "      #insert input token embedding, previous hidden state, all encoder hidden states and mask\n",
        "      #receive output tensor (predictions) and new hidden state\n",
        "      output, hidden, _ = self.decoder(input, hidden, encoder_outputs, mask)\n",
        "      \n",
        "      #place predictions in a tensor holding predictions for each token\n",
        "      outputs[t] = output\n",
        "      \n",
        "      #if teacher forcing, use actual next token as next input. Else, use predicted token\n",
        "      input = trg[t] if random.random() < teacher_forcing_ratio else output.argmax(1)\n",
        "    \n",
        "    if criterion != None:\n",
        "      loss = self.compute_loss(outputs, trg, criterion)\n",
        "      return loss, outputs[:, unsort_ids, :]\n",
        "    return outputs[:, unsort_ids, :]\n",
        "  \n",
        "  def compute_loss(self, output, trg, criterion):\n",
        "    #output = (trg_len, batch_size, trg_vocab_size)\n",
        "    #trg = [trg len, batch size]\n",
        "    output = output[1:].view(-1, output.shape[-1])  #output = [(trg len - 1) * batch size, output dim]\n",
        "    trg = trg[1:].view(-1)  #trg = [(trg len - 1) * batch size]\n",
        "    loss = criterion(output, trg)\n",
        "    return loss\n",
        "  \n",
        "  # NEWLY ADDED ##########################\n",
        "  def sort_by_sent_len(self, sent_len):\n",
        "    _, sort_ids = sent_len.sort(descending=True)\n",
        "    unsort_ids = sort_ids.argsort()\n",
        "    return sort_ids, unsort_ids\n",
        "  # END ADDED ############################"
      ],
      "metadata": {
        "id": "Pyz0ZITCU9r5"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pivot model (update)"
      ],
      "metadata": {
        "id": "fdjE61pw7CBy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**still need to reorganize code to use for infer (no criterions, only 1 data: src, src_len)**"
      ],
      "metadata": {
        "id": "qt2bmz1GTTnj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PivotSeq2Seq(nn.Module):\n",
        "  def __init__(self, models: list, fields: list, device, lamda=0.75):\n",
        "    super().__init__()\n",
        "    self.num_model = len(models)\n",
        "    self.fields = fields\n",
        "    self.num_field = len(fields)\n",
        "    self.device = device\n",
        "    self.lamda = lamda\n",
        "    \n",
        "    for i in range(self.num_model):\n",
        "      self.add_module(f'model_{i}', models[i])\n",
        "    \n",
        "    assert len(models)+1 == len(fields), f\"Not enough Fields for models: num_field={len(fields)} != {len(models)+1}\"\n",
        "      \n",
        "  def forward(self, datas: list, criterions=None, teacher_forcing_ratio=0.5):\n",
        "    '''\n",
        "    datas: list of data: [(src, src_len), (piv1, piv_len1), ... , (pivM, piv_lenM), (trg, trg_len)] given M models\n",
        "      src = [src len, batch_size]\n",
        "      src_len = [batch_size]\n",
        "      ...\n",
        "      trg = [trg len, batch_size]\n",
        "      trg_len = [batch_size]\n",
        "    criterions: list of criterion for each model\n",
        "    '''\n",
        "    if criterions != None:\n",
        "      loss_list, output_list = self.run(datas, criterions, teacher_forcing_ratio)\n",
        "      total_loss = self.compute_loss(loss_list)\n",
        "      return total_loss, output_list[-1]\n",
        "    else:\n",
        "      criterions = [None for _ in range(self.num_model)]\n",
        "      _, output_list = self.run(datas, criterions, teacher_forcing_ratio)\n",
        "      return output_list[-1]\n",
        "    \n",
        "  def run(self, datas, criterions, teacher_forcing_ratio):\n",
        "    assert self.num_model+1 == len(datas), f\"Not enough datas for models: data_len={len(datas)} != {self.num_model+1}\"\n",
        "    assert self.num_model == len(criterions), f'Criterions must have for each model: num_criterion={len(criterions)} != {self.num_model}'\n",
        "\n",
        "    output_list, loss_list = [], []\n",
        "    for i in range(self.num_model):\n",
        "      # 1st model must always use src\n",
        "      isForceOn = True if i==0 else random.random() < teacher_forcing_ratio\n",
        "\n",
        "      # GET NEW INPUT\n",
        "      src, src_len = datas[i] if isForceOn else self.process_output(output_list[-1], self.fields[i+1])\n",
        "      trg, trg_len = datas[i+1]\n",
        "\n",
        "      # FORWARD MODEL\n",
        "      model = getattr(self, f'model_{i}') # Seq2Seq model already sort src by src_len in forward\n",
        "      data = [(src, src_len), (trg, trg_len)]\n",
        "      criterion = criterions[i]\n",
        "      output = model(data, criterion, 0 if criterion==None else teacher_forcing_ratio)\n",
        "      \n",
        "      if criterion == None:\n",
        "        output_list.append(output)\n",
        "      else:\n",
        "        assert len(output) == 2, 'With criterion, model should return loss & prediction'\n",
        "        loss, out = output\n",
        "        loss_list.append(loss)\n",
        "        output_list.append(out)\n",
        "    \n",
        "    return loss_list, output_list\n",
        "\n",
        "  def compute_loss(self, loss_list):\n",
        "    total_loss = 0.0\n",
        "    for loss in loss_list:\n",
        "      total_loss += loss\n",
        "    return total_loss + self.lamda*self.compute_embed_loss()\n",
        "  \n",
        "  def compute_embed_loss(self):\n",
        "    embed_loss = 0.0\n",
        "    for i in range(1, self.num_model):\n",
        "      model1 = getattr(self, f'model_{i-1}')\n",
        "      model2 = getattr(self, f'model_{i}')\n",
        "      embed_loss += torch.sum(F.pairwise_distance(model1.decoder.embedding.weight, model2.encoder.embedding.weight, p=2))\n",
        "    return embed_loss\n",
        "  \n",
        "  def sort_by_src_len(self, piv, piv_len, datas): # piv = [piv_len, batch_size]\n",
        "    piv_len, sorted_ids = piv_len.sort(descending=True)\n",
        "    sorted_datas = [(sent[:, sorted_ids], sent_len[sorted_ids]) for (sent, sent_len) in datas]\n",
        "    return piv[:, sorted_ids], piv_len, sorted_datas  # piv sorted along batch_size\n",
        "  \n",
        "  def process_output(self, output, piv_field):\n",
        "    # output = [trg len, batch size, output dim]\n",
        "    # trg = [trg len, batch size]\n",
        "    # Process output1 to be input for model2\n",
        "    seq_len, N, _ = output.shape\n",
        "    tmp_out = output.argmax(2)  # tmp_out = [seq_len, batch_size]\n",
        "    # re-create pivot as src for model2\n",
        "    piv = torch.zeros_like(tmp_out).type(torch.long).to(output.device)\n",
        "    piv[0, :] = torch.full_like(piv[0, :], piv_field.vocab.stoi[piv_field.init_token])  # fill all first idx with sos_token\n",
        "    \n",
        "    for i in range(1, seq_len):  # for each i in seq_len\n",
        "      # if tmp_out's prev is eos_token, replace w/ pad_token, else current value\n",
        "      eos_mask = (tmp_out[i-1, :] == piv_field.vocab.stoi[piv_field.eos_token])\n",
        "      piv[i, :] = torch.where(eos_mask, piv_field.vocab.stoi[piv_field.pad_token], tmp_out[i, :])\n",
        "      # if piv's prev is pad_token, replace w/ pad_token, else current value\n",
        "      pad_mask = (piv[i-1, :] == piv_field.vocab.stoi[piv_field.pad_token])\n",
        "      piv[i, :] = torch.where(pad_mask, piv_field.vocab.stoi[piv_field.pad_token], piv[i, :])\n",
        "    \n",
        "    # Trim down extra pad tokens\n",
        "    tensor_list = [piv[i] for i in range(seq_len) if not all(piv[i] == piv_field.vocab.stoi[piv_field.pad_token])]  # tensor_list = [new_seq_len, batch_size]\n",
        "    piv = torch.stack([x for x in tensor_list], dim=0).type(torch.long).to(output.device)\n",
        "    assert not all(piv[-1] == piv_field.vocab.stoi[piv_field.pad_token]), 'Not completely trim down tensor'\n",
        "\n",
        "    # get seq_id + eos_tok id of each sequence\n",
        "    piv_ids, eos_ids = (piv.permute(1, 0) == piv_field.vocab.stoi[piv_field.eos_token]).nonzero(as_tuple=True)  # piv_len = [N]\n",
        "    piv_len = torch.full_like(piv[0], seq_len).type(torch.long)  # init w/ longest seq\n",
        "    piv_len[piv_ids] = eos_ids + 1 # seq_len = eos_tok + 1\n",
        "    \n",
        "    return piv, piv_len"
      ],
      "metadata": {
        "id": "kdQAxGHj7E2c"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Triangulate model"
      ],
      "metadata": {
        "id": "KgwBXdjq3IkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TriangSeq2Seq(nn.Module):\n",
        "  def __init__(self, models: list, output_dim, device):\n",
        "    # output_dim = trg vocab size\n",
        "    super().__init__()\n",
        "    self.num_model = len(models)\n",
        "    self.device = device\n",
        "    self.output_dim = output_dim\n",
        "    self.final_fc = torch.nn.Linear(output_dim*len(models), output_dim)\n",
        "    \n",
        "    for i in range(self.num_model):\n",
        "      self.add_module(f'model_{i}', models[i])\n",
        "          \n",
        "  def forward(self, datas: dict, criterions=None, teacher_forcing_ratio=0.5):\n",
        "    '''\n",
        "    datas: dict of data:\n",
        "      {\"model_0\": (src, src_len, trg, trg_len), \"model_1\": [(src, src_len), (piv, piv_len), (trg, trg_len)], ..., \"TRG\": (trg, trg_len)}\n",
        "      src = [src len, batch size]\n",
        "      src_len = [batch size]\n",
        "      ...\n",
        "      trg = [trg len, batch size]\n",
        "      trg_len = [batch size]\n",
        "    criterions: dict of criterions\n",
        "      {\"model_0\": criterion_0, \"model_1\": criterion_1, ..., \"TRG\": criterion_M}\n",
        "    '''\n",
        "    if criterions != None:\n",
        "      loss_list, output_list = self.run(datas, criterions, teacher_forcing_ratio)\n",
        "      assert len(loss_list)==len(output_list) and len(output_list)==self.num_model, 'DO NOT MATCH: len(loss_list)=len(output_list) OR len(output_list)=self.num_model'\n",
        "      # TODO: calculate final_out from all outputs\n",
        "      final_out = self.get_final_pred(output_list)\n",
        "      # TODO: submodels_loss + final output's loss ==> total_loss\n",
        "      total_loss = self.compute_final_pred_loss(final_out, datas[\"TRG\"], criterions[\"TRG\"]) + self.compute_submodels_loss(loss_list)\n",
        "      return total_loss, final_out\n",
        "    else:\n",
        "      criterions = {f'model_{i}':None for i in range(self.num_model)}\n",
        "      loss_list, output_list = self.run(datas, criterions, teacher_forcing_ratio)\n",
        "      assert len(loss_list)==0 and len(output_list)==self.num_model, 'DO NOT MATCH: len(loss_list)=0 OR len(output_list)=self.num_model'\n",
        "      # TODO: calculate final_out from all outputs\n",
        "      final_out = self.get_final_pred(output_list)\n",
        "      return final_out\n",
        "\n",
        "  def run(self, datas, criterions, teacher_forcing_ratio):\n",
        "    assert self.num_model+1 == len(datas), f\"Not enough datas for models: data_len={len(datas)} != {self.num_model+1}\"\n",
        "    assert self.num_model == len(criterions), f'Criterions must have for each model: num_criterion={len(criterions)} != {self.num_model}'\n",
        "\n",
        "    output_list = []\n",
        "    loss_list = []\n",
        "    for i in range(self.num_model):\n",
        "      # 1st model must always use src\n",
        "      isForceOn = True if i==0 else random.random() < teacher_forcing_ratio\n",
        "\n",
        "      data = datas[f'model_{i}']\n",
        "      model = getattr(self, f'model_{i}')\n",
        "      criterion = criterions[f'model_{i}']\n",
        "      output = model(data, criterion, 0 if criterion==None else teacher_forcing_ratio)\n",
        "      \n",
        "      if criterion == None:\n",
        "        output_list.append(output)\n",
        "      else:\n",
        "        assert len(output) == 2, 'With criterion, model should return loss & prediction'\n",
        "        loss_list.append(output[0])\n",
        "        output_list.append(output[1])\n",
        "    \n",
        "    return loss_list, output_list\n",
        "\n",
        "  def compute_submodels_loss(self, loss_list):\n",
        "    total_loss = 0.0\n",
        "    for loss in loss_list:\n",
        "      total_loss += loss\n",
        "    return total_loss + self.lamda*self.compute_embed_loss()\n",
        "\n",
        "  def compute_final_pred_loss(self, output, trg, criterion):\n",
        "    #output = (trg_len, batch_size, trg_vocab_size)\n",
        "    #trg = [trg len, batch size]\n",
        "    output = output[1:].view(-1, output.shape[-1])  #output = [(trg len - 1) * batch size, output dim]\n",
        "    trg = trg[1:].view(-1)  #trg = [(trg len - 1) * batch size]\n",
        "    loss = criterion(output, trg)\n",
        "    return loss\n",
        "\n",
        "  def get_final_pred(self, output_list, method='weighted'):\n",
        "    # output_list[0] shape = [seq_len, N, out_dim]\n",
        "    # outputs must match shape because use the same trg, trg_len\n",
        "    assert all([output_list[i].shape == output_list[i-1].shape for i in range(1, len(output_list))]), 'all outputs must match shape [seq_len, N, out_dim]'\n",
        "    # MAX method: get max along seq_len b/w all outputs\n",
        "    if method=='max':\n",
        "      stack_dim = 2\n",
        "      all_outputs = torch.stack([out for out in output_list], dim=stack_dim)  # all_outputs = [seq_len, N, stack_dim, out_dim]\n",
        "      final_out, max_idx = torch.max(all_outputs, dim=stack_dim)  # final_out = [seq_len, N, out_dim]\n",
        "      return final_out\n",
        "    elif method=='weighted':\n",
        "      linear_in = torch.cat([out for out in output_list], dim=-1) # linear_in = [seq_len, N, out_dim * num_model]. Note that num_model = len(output_list)\n",
        "      final_out = self.final_fc(linear_in)  # final_out = [seq_len, N, out_dim]\n",
        "      return final_out\n",
        "    else:\n",
        "      return output_list[0]"
      ],
      "metadata": {
        "id": "FKCumOEK3T6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train func"
      ],
      "metadata": {
        "id": "2cDqYW-LTnGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trainSeq2Seq(model, iterator, optimizer, criterion, clip):\n",
        "  model.train()\n",
        "  epoch_loss = 0.0\n",
        "  for batch in tqdm(iterator):\n",
        "    optimizer.zero_grad()\n",
        "    datas = [batch.en, batch.fr]\n",
        "    loss, output = model(datas, criterion)\n",
        "\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "    optimizer.step()\n",
        "    epoch_loss += loss.item()\n",
        "  return epoch_loss / len(iterator)\n",
        "\n",
        "def evaluateSeq2Seq(model, iterator, criterion):\n",
        "  model.eval()\n",
        "  epoch_loss = 0.0\n",
        "  with torch.no_grad():\n",
        "    for batch in tqdm(iterator):\n",
        "      datas = [batch.en, batch.fr]\n",
        "      loss, output = model(datas, criterion, 0) # turn off teacher forcing\n",
        "      epoch_loss += loss.item()\n",
        "    return epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "PyiHo_rj7b3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trainPivot(model, iterator, optimizer, criterions, clip):\n",
        "  model.train()\n",
        "  epoch_loss = 0.0\n",
        "  for batch in tqdm(iterator):\n",
        "    optimizer.zero_grad()\n",
        "    model_inputs = [batch.en, batch.de, batch.fr]\n",
        "    loss, output = model(model_inputs, criterions)\n",
        "\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "    optimizer.step()\n",
        "    epoch_loss += loss.item()\n",
        "  return epoch_loss / len(iterator)\n",
        "\n",
        "def evaluatePivot(model, iterator, criterions):\n",
        "  model.eval()\n",
        "  epoch_loss = 0.0\n",
        "  with torch.no_grad():\n",
        "    for batch in tqdm(iterator):\n",
        "      model_inputs = [batch.en, batch.de, batch.fr]\n",
        "      loss, output = model(model_inputs, criterions, 0)\n",
        "      epoch_loss += loss.item()\n",
        "    return epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "Au60zQ85n8No"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for triangulate model\n"
      ],
      "metadata": {
        "id": "TsX8opIkn7y7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.train()\n",
        "# epoch_loss = 0.0\n",
        "# iterator = train_iterator\n",
        "# for batch in tqdm(iterator):\n",
        "#   optimizer.zero_grad()\n",
        "#   (en_sent, en_len), (de_sent, de_len), (fr_sent, fr_len) = batch.en, batch.de, batch.fr\n",
        "\n",
        "#     # prep for model_1\n",
        "#   _, sorted_ids = de_len.sort(descending=True)\n",
        "#   model_inputs = {\"model_0\": [en_sent, en_len, fr_sent, fr_len],\n",
        "#                   \"model_1\": [de_sent[:, sorted_ids], de_len[sorted_ids], fr_sent[:, sorted_ids], fr_len[sorted_ids]],\n",
        "#                   \"TRG\": (fr_sent, fr_len)}\n",
        "#   loss, output = model(model_inputs, criterions)\n",
        "  \n",
        "#   loss.backward()\n",
        "#   torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "#   optimizer.step()\n",
        "#   epoch_loss += loss.item()\n",
        "#   break"
      ],
      "metadata": {
        "id": "doIMy1Eoiprv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_trainlog(data: list, filename: str='/content/gdrive/MyDrive/Colab Notebooks/eaai24/training_log.txt'):\n",
        "  ''' Update training log w/ new losses\n",
        "  Args:\n",
        "      data (List): a list of infor for many epochs as tuple, each tuple has model_name, loss, etc.\n",
        "      filename (String): path + file_name\n",
        "  Return:\n",
        "      None: new data is appended into train-log\n",
        "  '''\n",
        "  with open(filename, 'a') as f: # save\n",
        "    for epoch in data:\n",
        "      f.write(','.join(epoch))\n",
        "      f.write(\"\\n\")\n",
        "  print('update_trainlog SUCCESS')\n",
        "  return []"
      ],
      "metadata": {
        "id": "iaocGp1dS3RJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "MYFp2rpQUEbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # For 2 langs\n",
        "INPUT_DIM = 2500  #len(SRC_FIELD.vocab) since vocab is selected from most 2k5 freq words\n",
        "OUTPUT_DIM = 2500 #len(TRG_FIELD.vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "ENC_HID_DIM = 512\n",
        "DEC_HID_DIM = 512\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "LR = 0.001\n",
        "SRC_PAD_IDX = DE_FIELD.vocab.stoi[DE_FIELD.pad_token]\n",
        "\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "\n",
        "model = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)"
      ],
      "metadata": {
        "id": "9b_1FE6ZlpEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For 3 langs\n",
        "# INPUT_DIM = 2500  #len(SRC_FIELD.vocab) since vocab is selected from most 2k5 freq words\n",
        "# PIV_DIM = 2500  #len(PIV_FIELD.vocab)\n",
        "# OUTPUT_DIM = 2500 #len(TRG_FIELD.vocab)\n",
        "# ENC_EMB_DIM = 256\n",
        "# DEC_EMB_DIM = 256\n",
        "# ENC_HID_DIM = 512\n",
        "# DEC_HID_DIM = 512\n",
        "# ENC_DROPOUT = 0.5\n",
        "# DEC_DROPOUT = 0.5\n",
        "# LR = 0.001\n",
        "\n",
        "# SRC_PAD_IDX = EN_FIELD.vocab.stoi[EN_FIELD.pad_token]\n",
        "# PIV_PAD_IDX = DE_FIELD.vocab.stoi[DE_FIELD.pad_token]\n",
        "\n",
        "# attn1 = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "# enc1 = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "# dec1 = Decoder(PIV_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn1)\n",
        "# model1 = Seq2Seq(enc1, dec1, SRC_PAD_IDX, device).to(device)\n",
        "\n",
        "# attn2 = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "# enc2 = Encoder(PIV_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "# dec2 = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn2)\n",
        "# model2 = Seq2Seq(enc2, dec2, PIV_PAD_IDX, device).to(device)\n",
        "\n",
        "# models = [model1, model2]\n",
        "# # fields = [EN_FIELD, DE_FIELD, FR_FIELD]\n",
        "# # model = PivotSeq2Seq(models, fields, device).to(device)\n",
        "\n",
        "# # For triangulate\n",
        "# model = TriangSeq2Seq(models, OUTPUT_DIM, device).to(device)"
      ],
      "metadata": {
        "id": "zhpJPc2DaIHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(m):\n",
        "  for name, param in m.named_parameters():\n",
        "    if 'weight' in name:\n",
        "      nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "    else:\n",
        "      nn.init.constant_(param.data, 0)\n",
        "            \n",
        "model.apply(init_weights);"
      ],
      "metadata": {
        "id": "M7CeajzjlzWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=LR)"
      ],
      "metadata": {
        "id": "edsBDaCEmQD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html#torch.optim.lr_scheduler.StepLR\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.333)"
      ],
      "metadata": {
        "id": "TQzaKEN2bX1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion1 = nn.CrossEntropyLoss(ignore_index = DE_FIELD.vocab.stoi[DE_FIELD.pad_token])\n",
        "criterion2 = nn.CrossEntropyLoss(ignore_index = FR_FIELD.vocab.stoi[FR_FIELD.pad_token])\n",
        "# criterions = (criterion1, criterion2)\n",
        "\n",
        "# for triangulate\n",
        "criterions = {\n",
        "    'model_0': criterion1,\n",
        "    'model_1': criterion2,\n",
        "}"
      ],
      "metadata": {
        "id": "V_5TFOZsmPsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "best_valid_loss = float('inf')\n",
        "best_train_loss = float('inf')\n",
        "model_name = 'attn_enfr_160kset_3.pt'\n",
        "train_log = []"
      ],
      "metadata": {
        "id": "FIYc-s0FTPRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(N_EPOCHS):\n",
        "  # For 2 langs\n",
        "  train_loss = trainSeq2Seq(model, train_iterator, optimizer, criterion2, CLIP)\n",
        "  valid_loss = evaluateSeq2Seq(model, valid_iterator, criterion2)\n",
        "\n",
        "  # For 3 langs\n",
        "  # train_loss = trainPivot(model, train_iterator, optimizer, criterions, CLIP)\n",
        "  # valid_loss = evaluatePivot(model, valid_iterator, criterions)\n",
        "  \n",
        "  scheduler.step()\n",
        "  \n",
        "  epoch_info = [model_name, scheduler.get_last_lr()[0], BATCH_SIZE, ENC_HID_DIM, 'no-num_layers', ENC_DROPOUT, DEC_DROPOUT, epoch, N_EPOCHS, train_loss, valid_loss]\n",
        "  train_log.append([str(ele) for ele in epoch_info])\n",
        "\n",
        "  # if train_loss < best_train_loss or valid_loss < best_valid_loss:\n",
        "  #   best_train_loss = train_loss\n",
        "  #   best_valid_loss = valid_loss\n",
        "  #   torch.save({\n",
        "  #       'model_state_dict': model.state_dict(),\n",
        "  #       'optimizer_state_dict': optimizer.state_dict(),\n",
        "  #       'scheduler_state_dict': scheduler.state_dict()\n",
        "  #   }, f'/content/gdrive/MyDrive/Colab Notebooks/eaai24/{model_name}')\n",
        "  #   print('SAVED MODEL')\n",
        "  #   train_log = update_trainlog(train_log)\n",
        "  \n",
        "  print(f'Epoch: {epoch:02} \\t Train Loss: {train_loss:.3f} \\t Val. Loss: {valid_loss:.3f}')"
      ],
      "metadata": {
        "id": "3ngIAH5cldRm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d287d49-d4da-4abb-caad-13fda865dffa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [01:43<00:00,  9.70it/s]\n",
            "100%|██████████| 50/50 [00:03<00:00, 16.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 00 \t Train Loss: 4.065 \t Val. Loss: 3.470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [01:48<00:00,  9.20it/s]\n",
            "100%|██████████| 50/50 [00:01<00:00, 29.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 \t Train Loss: 2.754 \t Val. Loss: 2.994\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [01:41<00:00,  9.82it/s]\n",
            "100%|██████████| 50/50 [00:01<00:00, 34.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 02 \t Train Loss: 2.353 \t Val. Loss: 2.833\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [01:44<00:00,  9.59it/s]\n",
            "100%|██████████| 50/50 [00:01<00:00, 28.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 03 \t Train Loss: 2.129 \t Val. Loss: 2.866\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [01:45<00:00,  9.46it/s]\n",
            "100%|██████████| 50/50 [00:01<00:00, 27.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 04 \t Train Loss: 1.967 \t Val. Loss: 2.867\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [01:55<00:00,  8.64it/s]\n",
            "100%|██████████| 50/50 [00:01<00:00, 34.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 05 \t Train Loss: 1.855 \t Val. Loss: 2.909\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [01:51<00:00,  8.97it/s]\n",
            "100%|██████████| 50/50 [00:01<00:00, 28.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 06 \t Train Loss: 1.739 \t Val. Loss: 2.956\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [01:49<00:00,  9.09it/s]\n",
            "100%|██████████| 50/50 [00:01<00:00, 30.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 07 \t Train Loss: 1.496 \t Val. Loss: 2.914\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [01:36<00:00, 10.32it/s]\n",
            "100%|██████████| 50/50 [00:01<00:00, 30.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 08 \t Train Loss: 1.394 \t Val. Loss: 3.012\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [01:35<00:00, 10.43it/s]\n",
            "100%|██████████| 50/50 [00:01<00:00, 34.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 09 \t Train Loss: 1.328 \t Val. Loss: 3.007\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eval"
      ],
      "metadata": {
        "id": "jRFofx7s1jJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FIELDS = [('src', EN_SRC), ('trg', FR_TRG)]\n",
        "# start_idx = train_len + valid_len\n",
        "# end_idx = start_idx + 6400\n",
        "# test_examples = list(map(lambda x: Example.fromlist(list(x), fields=FIELDS), data_set[start_idx : end_idx]))\n",
        "# test_dt = Dataset(test_examples, fields=FIELDS)\n",
        "# test_iterator1 = BucketIterator(\n",
        "#     test_dt,\n",
        "#      batch_size = BATCH_SIZE,\n",
        "#      sort_within_batch = True,\n",
        "#      sort_key = lambda x : len(x.src),\n",
        "#      device = device)"
      ],
      "metadata": {
        "id": "wEb4SPEC4bnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in test_iterator:\n",
        "  break\n",
        "batch.fields"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qS7TxmFB40s9",
        "outputId": "065400e0-103f-4f78-be50-7e0a95b81290"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['en', 'fr'])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # For 2 langs\n",
        "INPUT_DIM = 2463  #len(SRC_FIELD.vocab)\n",
        "OUTPUT_DIM = 2495 #len(TRG_FIELD.vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "ENC_HID_DIM = 512\n",
        "DEC_HID_DIM = 512\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "LR = 0.001\n",
        "SRC_PAD_IDX = EN_FIELD.vocab.stoi[EN_FIELD.pad_token]\n",
        "\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "\n",
        "model_infer = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)\n",
        "model_path = f'/content/gdrive/MyDrive/Colab Notebooks/eaai24/attn_enfr_160kset.pt'\n",
        "model_infer.load_state_dict(torch.load(model_path)['model_state_dict'])"
      ],
      "metadata": {
        "id": "ZM8Z2hbY37WQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50ced86f-c305-4d7f-a361-3fe7b96a74e2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For 3 langs\n",
        "# INPUT_DIM = 2500  #len(SRC_FIELD.vocab)\n",
        "# PIV_DIM = 2500  #len(PIV_FIELD.vocab)\n",
        "# OUTPUT_DIM = 2500 #len(TRG_FIELD.vocab)\n",
        "# ENC_EMB_DIM = 256\n",
        "# DEC_EMB_DIM = 256\n",
        "# ENC_HID_DIM = 512\n",
        "# DEC_HID_DIM = 512\n",
        "# ENC_DROPOUT = 0.5\n",
        "# DEC_DROPOUT = 0.5\n",
        "# LR = 0.001\n",
        "\n",
        "# SRC_PAD_IDX = SRC_FIELD.vocab.stoi[SRC_FIELD.pad_token]\n",
        "# PIV_PAD_IDX = PIV_FIELD.vocab.stoi[PIV_FIELD.pad_token]\n",
        "\n",
        "# attn1 = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "# enc1 = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "# dec1 = Decoder(PIV_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn1)\n",
        "# model1 = Seq2Seq(enc1, dec1, SRC_PAD_IDX, device).to(device)\n",
        "\n",
        "# attn2 = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "# enc2 = Encoder(PIV_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "# dec2 = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn2)\n",
        "# model2 = Seq2Seq(enc2, dec2, PIV_PAD_IDX, device).to(device)\n",
        "\n",
        "# model_infer = PivotSeq2Seq(model1, model2, SRC_FIELD, PIV_FIELD, TRG_FIELD, device).to(device)\n",
        "# model_path = '/content/gdrive/MyDrive/Colab Notebooks/eaai24/piv_endefr_74kset_2.pt'\n",
        "# model_infer.load_state_dict(torch.load(model_path)['model_state_dict'])"
      ],
      "metadata": {
        "id": "OrZyj-ZxRaZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_path = f'/content/gdrive/MyDrive/Colab Notebooks/eaai24/attn_en-fr_32k_160kset_inverse.pt'\n",
        "# ckpt = torch.load(model_path)\n",
        "# model.load_state_dict(ckpt['model_state_dict']) # strict=False if some dimensions are different\n",
        "# optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
        "# scheduler.load_state_dict(ckpt['scheduler_state_dict'])"
      ],
      "metadata": {
        "id": "PwZxT4MZ8tV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_loss = evaluate(model_infer, test_iterator, criterion2, isPivot=False, force=0)\n",
        "# print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "metadata": {
        "id": "blKOkBAe1m5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "cbS1GfWq2DmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sent2tensor(src_field, trg_field, device, max_len, sentence=None):\n",
        "  if sentence != None:\n",
        "    if isinstance(sentence, str):\n",
        "      tokens = tokenize_en(sentence)\n",
        "    else:\n",
        "      tokens = [token.lower() for token in sentence]\n",
        "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
        "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]    \n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)  # [seq_len, N] w/ N=1 for batch\n",
        "    src_len_tensor = torch.LongTensor([len(src_indexes)]).to(device)\n",
        "    return src_tensor, src_len_tensor\n",
        "  \n",
        "  trg_tensor = torch.LongTensor([trg_field.vocab.stoi[trg_field.init_token]] + [0 for i in range(1, max_len)]).view(-1, 1).to(device) # [seq_len, 1]\n",
        "  trg_len_tensor = torch.LongTensor([max_len]).to(device)\n",
        "  return trg_tensor, trg_len_tensor\n",
        "\n",
        "def idx2sent(trg_field, arr):\n",
        "  n_sents = arr.shape[1]  # arr = [seq_len, N]\n",
        "  results = []\n",
        "  for i in range(n_sents):  # for each sent\n",
        "    pred_sent = []\n",
        "    pred = arr[:, i]\n",
        "    for i in pred[1:]:  # for each word\n",
        "      pred_sent.append(trg_field.vocab.itos[i])\n",
        "      if i == trg_field.vocab.stoi[trg_field.eos_token]: break\n",
        "    results.append(pred_sent)\n",
        "  return results"
      ],
      "metadata": {
        "id": "xUMMEYoLaDr_"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sentence_seq2seq(sentence, src_field, trg_field, model: Seq2Seq, device, max_len=50):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    # get data\n",
        "    src_tensor, src_len_tensor = sent2tensor(src_field, trg_field, device, max_len, sentence)\n",
        "    trg_tensor, trg_len_tensor = sent2tensor(src_field, trg_field, device, max_len)\n",
        "    data = [(src_tensor, src_len_tensor), (trg_tensor, trg_len_tensor)]\n",
        "    # feed model\n",
        "    output = model(data, criterion=None, teacher_forcing_ratio=0) # output = [trg_len, N, dec_emb_dim] w/ N=1\n",
        "    output = output.argmax(-1).detach().cpu().numpy() # output = [seq_len, N]\n",
        "    results = idx2sent(trg_field, output)\n",
        "    return results\n",
        "\n",
        "def translate_sentence_pivot(sentence, src_field, trg_field, model, device, max_len=50):  # not yet modified\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    # get data\n",
        "    src_tensor, src_len_tensor = sent2tensor(src_field, trg_field, device, max_len, sentence)\n",
        "    trg_tensor, trg_len_tensor = sent2tensor(src_field, trg_field, device, max_len)\n",
        "    data = [(src_tensor, src_len_tensor)] + [(trg_tensor.clone().detach().to(device), trg_len_tensor.clone().detach().to(device)) for _ in range(model.num_model)]\n",
        "    # feed model\n",
        "    output = model(data, criterions=None, teacher_forcing_ratio=0) # output = [trg_len, N, dec_emb_dim]\n",
        "    output = output.argmax(-1).detach().cpu().numpy()\n",
        "    results = idx2sent(trg_field, output)\n",
        "    return results"
      ],
      "metadata": {
        "id": "c6d_Ggwz2EcZ"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_idx = 432\n",
        "src = vars(valid_dt.examples[example_idx])['en']\n",
        "trg = vars(valid_dt.examples[example_idx])['fr']\n",
        "pred = translate_sentence_seq2seq(src, EN_FIELD, FR_FIELD, model_infer, device)\n",
        "print(src)\n",
        "print(trg)\n",
        "print(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sK5gR8S7Ecle",
        "outputId": "91ca2b22-5bca-433f-aa27-71de3ffa89a8"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['that', 'is', 'the', 'future', '.']\n",
            "[\"c'\", 'est', 'cela', \"l'\", 'avenir', '.']\n",
            "[['voilà', 'est', \"l'\", 'avenir', '.', '<eos>']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# example_idx = 432\n",
        "# src = vars(valid_dt.examples[example_idx])['en']\n",
        "# trg = vars(valid_dt.examples[example_idx])['fr']\n",
        "# print(src)\n",
        "# print(trg)\n",
        "# pred = translate_sentence_pivot(src, EN_FIELD, FR_FIELD, model, device)"
      ],
      "metadata": {
        "id": "M3bCCUKEDeZN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1060907-5167-4c3e-faf6-d6c9c83ccc4c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "src = ['that', 'is', 'the', 'future', '.']\n",
            "trg = [\"c'\", 'est', 'cela', \"l'\", 'avenir', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_iterator_1 = BucketIterator(\n",
        "     test_dt,\n",
        "     batch_size = 3,\n",
        "     sort_within_batch = True,\n",
        "     sort_key = lambda x : len(x.fr),\n",
        "     device = device)\n",
        "\n",
        "for batch in test_iterator_1: break\n",
        "batch.fields"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwABYGQpZoFf",
        "outputId": "b1de9b6f-718a-4517-bf22-82ad57337903"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['en', 'fr'])"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_batch_seq2seq(model_infer, iterator, trg_field, device):\n",
        "  model_infer.eval()\n",
        "  with torch.no_grad():\n",
        "    gt_sents = []\n",
        "    pred_sents = []\n",
        "    for i, batch in tqdm(enumerate(iterator)):\n",
        "      data = [batch.en, batch.fr] # modify based on model\n",
        "      output = model_infer(data, criterion=None, teacher_forcing_ratio=0)\n",
        "      \n",
        "      pred = output.argmax(-1).detach().cpu().numpy() # [seq_len, N]\n",
        "      truth = batch.fr[0].detach().cpu().numpy()  # [seq_len, N]\n",
        "\n",
        "      gt_sents = gt_sents + idx2sent(trg_field, truth)\n",
        "      pred_sents = pred_sents + idx2sent(trg_field, pred)\n",
        "      \n",
        "    return gt_sents, pred_sents"
      ],
      "metadata": {
        "id": "oIRZYHOsi0LU"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gt_sents, pred_sents = translate_batch_seq2seq(model_infer, test_iterator_1, FR_FIELD, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fhhvOpTkQkD",
        "outputId": "ae4672c8-18c9-4b75-a632-4bb0d9bf3030"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2134it [00:36, 58.25it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (gt_sent, pred_sent) in enumerate(zip(gt_sents, pred_sents)):\n",
        "  print(gt_sent)\n",
        "  print(pred_sent)\n",
        "  print()\n",
        "  if i==5: break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKokrGMnlDp5",
        "outputId": "c60e43fe-e8ea-4c9e-a594-c9d8422fae2d"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['je', 'vous', 'remercie', ',', 'm.', 'le', 'président', '.', '<eos>']\n",
            "['merci', 'beaucoup', ',', 'monsieur', 'le', 'président', '.', '<eos>', '<eos>']\n",
            "\n",
            "['les', 'rapporteurs', 'ont', 'fait', 'du', 'bon', 'travail', '.', '<eos>']\n",
            "['les', 'rapporteurs', 'ont', 'très', 'bien', 'très', 'bien', '.', '<eos>']\n",
            "\n",
            "['nous', 'avons', 'dès', 'lors', 'voté', 'en', 'conséquence', '.', '<eos>']\n",
            "['nous', 'avons', 'donc', 'conséquent', 'voté', 'en', 'raison', '.', '<eos>']\n",
            "\n",
            "[\"c'\", 'est', 'pour', 'moi', 'un', 'des', 'aspects', 'fondamentaux', 'de', 'ce', 'débat', '.', '<eos>']\n",
            "['c’', 'est', 'pour', 'moi', ',', 'une', 'des', 'aspects', 'fondamentaux', 'de', 'ce', 'débat', '.']\n",
            "\n",
            "['si', 'vous', 'me', 'le', 'permettez', ',', \"j'\", 'aimerais', 'relever', 'un', 'détail', '.', '<eos>']\n",
            "['je', 'vais', ',', 'si', 'vous', ',', 'y', 'en', 'mentionner', '.', '<eos>', '.', '<eos>']\n",
            "\n",
            "['notre', 'objectif', 'est', \"d'\", 'accroître', 'la', 'production', 'et', \"l'\", 'activité', 'économique', '.', '<eos>']\n",
            "['notre', 'objectif', 'est', 'de', 'la', 'production', 'et', 'la', 'activité', 'économique', '.', '<eos>', '<eos>']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BLEU (not yet modified)"
      ],
      "metadata": {
        "id": "_gNHZNQk2e4M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main"
      ],
      "metadata": {
        "id": "celagCJTtFd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_bleu_old(data, src_field, trg_field, model, device, max_len = 50):\n",
        "  trgs = []\n",
        "  pred_trgs = []\n",
        "  for i, datum in tqdm(enumerate(data)):\n",
        "    src = vars(datum)['src']\n",
        "    trg = vars(datum)['trg']\n",
        "    pred_trg = translate_sentence_seq2seq(src, src_field, trg_field, model, device, max_len)\n",
        "    #cut off <eos> token\n",
        "    pred_trg = pred_trg[:-1]\n",
        "    pred_trgs.append(pred_trg)\n",
        "    trgs.append([trg])\n",
        "    if i==1500: break\n",
        "  return bleu_score(pred_trgs, trgs)\n",
        "\n",
        "def calculate_bleu(translator, data, src_field, trg_field, model, device, max_len = 50):\n",
        "  trgs = []\n",
        "  pred_trgs = []\n",
        "  for i, datum in tqdm(enumerate(data)):\n",
        "    src = vars(datum)['src']\n",
        "    trg = vars(datum)['trg']\n",
        "    pred = translator(src, src_field, trg_field, model, device, max_len)\n",
        "    #cut off <eos> token\n",
        "    pred_trgs.append(pred[:-1])\n",
        "    trgs.append([trg])\n",
        "    if i==2000: break\n",
        "  return bleu_score(pred_trgs, trgs)"
      ],
      "metadata": {
        "id": "gTwoCAVY2iib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_score = calculate_bleu(test_dt, EN_FIELD, FR_FIELD, model_infer, device)\n",
        "print(f'BLEU score = {bleu_score*100:.2f}')"
      ],
      "metadata": {
        "id": "UDzwuc3z2sDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_score = calculate_bleu_1piv(test_dt, EN_FIELD, DE_FIELD, FR_FIELD, model_infer, device, max_len = 50)\n",
        "print(f'BLEU score = {bleu_score*100:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMUJ2XL3ahPy",
        "outputId": "cbef3280-5e46-4c3a-d6db-0fff9d75d6b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2000it [00:54, 36.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU score = 26.33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IF error happens: index 4 is out of bounds for dimension 0 with size 4 ==> use this"
      ],
      "metadata": {
        "id": "yi7oQJcHs7KJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pred_trg_corpus(data, model_infer, device, SRC_FIELD, TRG_FIELD, PIV_FIELD=None, max_len=50):\n",
        "  candidate_corpus = []\n",
        "  references_corpus = []\n",
        "  for datum in tqdm(data):\n",
        "    src = vars(datum)['src']\n",
        "    trg = vars(datum)['trg']\n",
        "    if isinstance(model_infer, PivotSeq2Seq):\n",
        "      piv = vars(datum)['piv']\n",
        "      sent1, sent = translate_sentence_1piv(src, SRC_FIELD, PIV_FIELD, TRG_FIELD, model_infer, device, max_len=max_len)\n",
        "    else:\n",
        "      sent, attn = translate_sentence(src, SRC_FIELD, TRG_FIELD, model_infer, device, max_len=max_len)\n",
        "    candidate_corpus.append(sent[:-1])\n",
        "    references_corpus.append([trg])\n",
        "  return candidate_corpus, references_corpus"
      ],
      "metadata": {
        "id": "TTExRY2DnVC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "candidate_corpus, references_corpus = get_pred_trg_corpus(test_dt, model_infer, device, SRC_FIELD, TRG_FIELD, PIV_FIELD)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_a4WDhXZo3sJ",
        "outputId": "26aa6768-5387-4472-c33d-3c0a8865b55d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6400/6400 [02:43<00:00, 39.08it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "candidate_corpus[:1376], references_corpus[:1376]"
      ],
      "metadata": {
        "id": "CDkYu16Lq_tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 5000\n",
        "j = 100\n",
        "k = 30000\n",
        "bleu_score(candidate_corpus[: i]+candidate_corpus[i+j:k], references_corpus[: i]+references_corpus[i+j:k])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLDv3sEIo-Q9",
        "outputId": "7e0cdc70-b69a-4e5e-b7d5-0a2e88c90707"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.26010842469504236"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "candidate = ['hello', '']\n",
        "references = [['hello'], ['.']]\n",
        "bleu_score(candidate, references)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lU_iKKNCon4s",
        "outputId": "edfc8467-5ae7-48be-c8e9-3f58a6d433e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8187307530779819"
            ]
          },
          "metadata": {},
          "execution_count": 249
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Result"
      ],
      "metadata": {
        "id": "LcU3uVKbtI4X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* attn_en-fr_32k.pt: BLEU = 12.65\n",
        "* attn_enfr_160kset.pt: BLEU = 32.18\n",
        "* piv_endefr_74kset_2.pt: BLEU = 26.33\n"
      ],
      "metadata": {
        "id": "vIw-3jBB42zA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# End"
      ],
      "metadata": {
        "id": "_0vl6ryRfWl9"
      }
    }
  ]
}