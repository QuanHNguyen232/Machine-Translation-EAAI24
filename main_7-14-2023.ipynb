{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "DEdOoJ93e008",
        "f_Qg27JlTHS1",
        "jopehmrVTRSm",
        "limqRnayUnj5",
        "vJm8VpgbUsbK",
        "7M3M0cKNUsfb",
        "WwSO5W3KUsjX",
        "fdjE61pw7CBy",
        "KgwBXdjq3IkB",
        "H9Z6RxOnzLa9",
        "Jfky7439zNhN",
        "ARfI-GykzP9L",
        "VKGgTuL4tThr",
        "TmXsuwuctVw7",
        "ifgtDQwvtRcw",
        "ZhCTMvF2tbQs",
        "jRFofx7s1jJa",
        "_3qwJt8Q1qN1",
        "2_uEiN_p1ufE",
        "_gNHZNQk2e4M"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Start"
      ],
      "metadata": {
        "id": "nWTuUBqTv-Nv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NEW TASKS:\n",
        "* [X] Seq2Seq: sort by src_len and unsort output --> ensure output matches with trg\n",
        "* [X] Pivot model: ensure it works for $n$ seq2seq models\n",
        "* [ ] Triang model: ensure outputs from all submodels match"
      ],
      "metadata": {
        "id": "FCN31jhjnCzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# piv_endefr_74kset_2.pt using PivotModel in bentrevett/pytorch-seq2seq-OLD.ipynb"
      ],
      "metadata": {
        "id": "yTMYDLiGey7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/bentrevett/pytorch-seq2seq/blob/master/4%20-%20Packed%20Padded%20Sequences%2C%20Masking%2C%20Inference%20and%20BLEU.ipynb\n",
        "# based on https://gmihaila.github.io/tutorial_notebooks/pytorchtext_bucketiterator/#dataset-class"
      ],
      "metadata": {
        "id": "V6P0jGmKybf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "piv_langs = ['es', 'it', 'pt', 'ro']\n",
        "langs = ['en', 'fr'] + piv_langs\n",
        "DIR_PATH = '/content/gdrive/MyDrive/Colab Notebooks/eaai24'"
      ],
      "metadata": {
        "id": "bktHQ8XCnDIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "DEdOoJ93e008"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOnOF0x_PM0F",
        "outputId": "c9762888-d5d5-4e02-b1c5-89fa03fd9e66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install torch==1.8.2 torchvision==0.9.2 torchaudio==0.8.2 --extra-index-url https://download.pytorch.org/whl/lts/1.8/cu111 -q"
      ],
      "metadata": {
        "id": "SA0jFbKHbfHP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ba67d41-d226-4a50-f2e3-93194d4f3da8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 GB\u001b[0m \u001b[31m788.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.14.1 requires torch==1.13.1, but you have torch 1.8.2+cu111 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1754nP4obkXv",
        "outputId": "46ea9f8f-1a6e-40b9-b8e9-46fe3f8dfd32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.8.2+cu111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMlSEzldbP3q",
        "outputId": "ccc2467e-0efe-48cf-e420-2df790c48d8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m735.5/735.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.9.2+cu111 requires torch==1.8.2, but you have torch 1.8.0 which is incompatible.\n",
            "torchaudio 0.8.2 requires torch==1.8.2, but you have torch 1.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install torchtext==0.9 -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if 'en' in langs:\n",
        "  !python -m spacy download en_core_web_sm -q\n",
        "if 'de' in langs:\n",
        "  !python -m spacy download de_core_news_sm -q\n",
        "if 'fr' in langs:\n",
        "  !python -m spacy download fr_core_news_sm -q\n",
        "if 'it' in langs:\n",
        "  !python -m spacy download it_core_news_sm -q\n",
        "if 'es' in langs:\n",
        "  !python -m spacy download es_core_news_sm -q\n",
        "if 'pt' in langs:\n",
        "  !python -m spacy download pt_core_news_sm -q\n",
        "if 'ro' in langs:\n",
        "  !python -m spacy download ro_core_news_sm -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dw6ZjirTbTE8",
        "outputId": "2a6241d5-46ea-4092-be78-8675cc98c6c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-03-31 17:58:09.435812: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-31 17:58:14.207604: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-03-31 17:58:19.461278: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-03-31 17:58:19.466276: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-03-31 17:58:19.466560: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "2023-03-31 17:58:35.544328: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-31 17:58:36.537268: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-03-31 17:58:37.964611: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-03-31 17:58:37.965238: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-03-31 17:58:37.965469: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n",
            "2023-03-31 17:58:49.543930: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-31 17:58:50.572452: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-03-31 17:58:52.083326: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-03-31 17:58:52.084132: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-03-31 17:58:52.084347: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('it_core_news_sm')\n",
            "2023-03-31 17:59:03.826154: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-31 17:59:04.849404: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-03-31 17:59:06.342844: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-03-31 17:59:06.343525: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-03-31 17:59:06.343743: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "2023-03-31 17:59:18.091608: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-31 17:59:19.013529: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-03-31 17:59:20.538951: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-03-31 17:59:20.539737: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-03-31 17:59:20.539942: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pt_core_news_sm')\n",
            "2023-03-31 17:59:32.719915: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-31 17:59:33.724736: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-03-31 17:59:35.229675: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-03-31 17:59:35.230481: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-03-31 17:59:35.230708: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ro_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchtext.legacy.data import Field, BucketIterator\n",
        "from torchtext.legacy.data import Dataset, Example\n",
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import pickle\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "6N_lgaPWd4AX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# My Section"
      ],
      "metadata": {
        "id": "IZIH_L_HeyEJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "f_Qg27JlTHS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FIELD_DICT = {}\n",
        "if 'en' in langs:\n",
        "  spacy_en = spacy.load('en_core_web_sm')\n",
        "  def tokenize_en(text):\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "  EN_FIELD = Field(tokenize = tokenize_en, init_token = '<sos>', eos_token = '<eos>', lower = True, include_lengths = True)\n",
        "  FIELD_DICT['en'] = EN_FIELD\n",
        "if 'de' in langs:\n",
        "  spacy_de = spacy.load('de_core_news_sm')\n",
        "  def tokenize_de(text):\n",
        "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
        "  DE_FIELD = Field(tokenize = tokenize_de, init_token = '<sos>', eos_token = '<eos>', lower = True, include_lengths = True)\n",
        "  FIELD_DICT['de'] = DE_FIELD\n",
        "if 'fr' in langs:\n",
        "  spacy_fr = spacy.load('fr_core_news_sm')\n",
        "  def tokenize_fr(text):\n",
        "    return [tok.text for tok in spacy_fr.tokenizer(text)]\n",
        "  FR_FIELD = Field(tokenize = tokenize_fr, init_token = '<sos>', eos_token = '<eos>', lower = True, include_lengths = True)\n",
        "  FIELD_DICT['fr'] = FR_FIELD\n",
        "if 'it' in langs:\n",
        "  spacy_it = spacy.load('it_core_news_sm')\n",
        "  def tokenize_it(text):\n",
        "    return [tok.text for tok in spacy_it.tokenizer(text)]\n",
        "  IT_FIELD = Field(tokenize = tokenize_it, init_token = '<sos>', eos_token = '<eos>', lower = True, include_lengths = True)\n",
        "  FIELD_DICT['it'] = IT_FIELD\n",
        "if 'es' in langs:\n",
        "  spacy_es = spacy.load('es_core_news_sm')\n",
        "  def tokenize_es(text):\n",
        "    return [tok.text for tok in spacy_es.tokenizer(text)]\n",
        "  ES_FIELD = Field(tokenize = tokenize_es, init_token = '<sos>', eos_token = '<eos>', lower = True, include_lengths = True)\n",
        "  FIELD_DICT['es'] = ES_FIELD\n",
        "if 'pt' in langs:\n",
        "  spacy_pt = spacy.load('pt_core_news_sm')\n",
        "  def tokenize_pt(text):\n",
        "    return [tok.text for tok in spacy_pt.tokenizer(text)]\n",
        "  PT_FIELD = Field(tokenize = tokenize_pt, init_token = '<sos>', eos_token = '<eos>', lower = True, include_lengths = True)\n",
        "  FIELD_DICT['pt'] = PT_FIELD\n",
        "if 'ro' in langs:\n",
        "  spacy_ro = spacy.load('ro_core_news_sm')\n",
        "  def tokenize_ro(text):\n",
        "    return [tok.text for tok in spacy_ro.tokenizer(text)]\n",
        "  RO_FIELD = Field(tokenize = tokenize_ro, init_token = '<sos>', eos_token = '<eos>', lower = True, include_lengths = True)\n",
        "  FIELD_DICT['ro'] = RO_FIELD"
      ],
      "metadata": {
        "id": "aXSlRvZqPRqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "jopehmrVTRSm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 8\n",
        "\n",
        "train_len = 64000\n",
        "valid_len = 3200\n",
        "test_len = 6400\n",
        "\n",
        "train_pt = train_len\n",
        "valid_pt = train_pt + valid_len\n",
        "test_pt = valid_pt + test_len\n",
        "\n",
        "print(train_pt, valid_pt, test_pt)"
      ],
      "metadata": {
        "id": "T5DlQGmUoKoF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43d37df8-f0c2-45bc-b804-384329bce9f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64000 67200 73600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://discuss.pytorch.org/t/how-to-save-and-load-torchtext-data-field-build-vocab-result/50407/3\n",
        "def save_vocab(vocab, path):\n",
        "  with open(path, 'w+', encoding='utf-8') as f:\n",
        "    for token, index in vocab.stoi.items():\n",
        "      f.write(f'{index}\\t{token}\\n')\n",
        "def read_vocab(path):\n",
        "  vocab = dict()\n",
        "  with open(path, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "      index, token = line.split('\\t')\n",
        "      vocab[token] = int(index)\n",
        "  return vocab"
      ],
      "metadata": {
        "id": "lqFlyKPUq1Bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with open('/content/gdrive/MyDrive/Colab Notebooks/eaai24/Datasets/endefr_75kpairs_2k5-freq-words.pkl', 'rb') as f:\n",
        "# with open('/content/gdrive/MyDrive/Colab Notebooks/eaai24/Datasets/enfr_160kpairs_2k5-freq-words.pkl', 'rb') as f:\n",
        "#   data = pickle.load(f)\n",
        "# data[80], len(data)"
      ],
      "metadata": {
        "id": "pHT8AUMJe7PF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EnDeFrItEsPtRo-60k-most10k-1.pkl is most suitable. Each lang has ~5-7k words\n",
        "# EnDeFrItEsPtRo-76k-most5k.pkl: each lang has 6->12k words (too much. use this in the future)"
      ],
      "metadata": {
        "id": "0jNRBlIUzV_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataname = 'EnDeFrItEsPtRo-76k-most5k.pkl'\n",
        "with open(f'{DIR_PATH}/dataset/{dataname}', 'rb') as f:\n",
        "  data = pickle.load(f)\n",
        "data[8], len(data)"
      ],
      "metadata": {
        "id": "g8r6JHsEXCDI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91e3c469-1a4b-422d-d9b9-0af2e6b8146e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'en': 'What is the result?',\n",
              "  'de': 'Und wie sehen die Ergebnisse aus?',\n",
              "  'fr': 'Quelle en est la conséquence ?',\n",
              "  'it': 'Quali sono i risultati?',\n",
              "  'es': '¿Cuáles son los resultados?',\n",
              "  'pt': 'Quais são os resultados?',\n",
              "  'ro': 'Care este rezultatul?'},\n",
              " 76245)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data_set = [[pair['en'], pair['it'], pair['fr']] for pair in data]\n",
        "# FIELDS = [('en', EN_FIELD), ('it', IT_FIELD), ('fr', FR_FIELD)]\n",
        "data_set = [[pair[lang] for lang in langs] for pair in data]\n",
        "FIELDS = [(lang, FIELD_DICT[lang]) for lang in langs]\n",
        "train_examples = list(map(lambda x: Example.fromlist(list(x), fields=FIELDS), data_set[: train_pt]))\n",
        "valid_examples = list(map(lambda x: Example.fromlist(list(x), fields=FIELDS), data_set[train_pt : valid_pt]))\n",
        "test_examples = list(map(lambda x: Example.fromlist(list(x), fields=FIELDS), data_set[valid_pt : test_pt]))"
      ],
      "metadata": {
        "id": "cyFzD7FQfHBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dt = Dataset(train_examples, fields=FIELDS)\n",
        "valid_dt = Dataset(valid_examples, fields=FIELDS)\n",
        "test_dt = Dataset(test_examples, fields=FIELDS)"
      ],
      "metadata": {
        "id": "Mron6AMwfH6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EN_FIELD.build_vocab(train_dt, min_freq = 2)\n",
        "# IT_FIELD.build_vocab(train_dt, min_freq = 2)\n",
        "# FR_FIELD.build_vocab(train_dt, min_freq = 2)\n",
        "# len(EN_FIELD.vocab), len(IT_FIELD.vocab), len(FR_FIELD.vocab)\n",
        "for lang in langs:\n",
        "  FIELD_DICT[lang].build_vocab(train_dt, min_freq = 2)\n",
        "  print(f'{lang}: {len(FIELD_DICT[lang].vocab)}')"
      ],
      "metadata": {
        "id": "KYY067rbwZGN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d936cc8-d06d-4eb6-a4f9-afdc63430ad1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "en: 6964\n",
            "fr: 9703\n",
            "es: 10461\n",
            "it: 10712\n",
            "pt: 10721\n",
            "ro: 11989\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save_vocab(EN_FIELD.vocab, f'{DIR_PATH}/Datasets/EnDeFrItEsPtRo-76k-most5k-en_vocab.txt')\n",
        "# EN_FIELD.vocab = read_vocab(f'{DIR_PATH}/Datasets/EnDeFrItEsPtRo-76k-most5k-en_vocab.txt')\n",
        "# FR_FIELD.vocab = read_vocab(f'{DIR_PATH}/Datasets/EnDeFrItEsPtRo-76k-most5k-fr_vocab.txt')"
      ],
      "metadata": {
        "id": "_IpSjObrOs8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_dt, valid_dt, test_dt),\n",
        "     batch_size = BATCH_SIZE,\n",
        "     sort_within_batch = True,\n",
        "     sort_key = lambda x : len(x.en),\n",
        "     device = device)"
      ],
      "metadata": {
        "id": "9ZrebZgnkiuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i, batch in enumerate(train_iterator):\n",
        "#   break\n",
        "# print(batch.en[0].shape, batch.en[1])\n",
        "# print(batch.fr[0].shape, batch.fr[1])\n",
        "# print(batch.fields)"
      ],
      "metadata": {
        "id": "XxntiGWVlA4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(vars(batch)['en'][0].shape, vars(batch)['en'][1])\n",
        "# print(vars(batch)['fr'][0].shape, vars(batch)['fr'][1])"
      ],
      "metadata": {
        "id": "oVWvSXCMjojs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# src_sent, piv_sent, trg_sent = [], [], []\n",
        "# for i in batch.en[0][: , 0]:\n",
        "#   src_sent.append(EN_FIELD.vocab.itos[i])\n",
        "# for i in batch.fr[0][:, 0]:\n",
        "#   trg_sent.append(FR_FIELD.vocab.itos[i])\n",
        "# print(' '.join(src_sent))\n",
        "# print(' '.join(trg_sent))"
      ],
      "metadata": {
        "id": "m7IO5L0nvEeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in range(10):\n",
        "#   print(EN_FIELD.vocab.itos[i], FR_FIELD.vocab.itos[i])"
      ],
      "metadata": {
        "id": "-XROmZP0olgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "3Cp4hrsvTlPy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder"
      ],
      "metadata": {
        "id": "limqRnayUnj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "    self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
        "    self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, src, src_len):\n",
        "    #src = [src len, batch size]\n",
        "    #src_len = [batch size]\n",
        "    embedded = self.dropout(self.embedding(src))  #embedded = [src len, batch size, emb dim]\n",
        "\n",
        "    #need to explicitly put lengths on cpu!\n",
        "    packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len.to('cpu'))\n",
        "\n",
        "    #  when the input is a pad token are all zeros\n",
        "    packed_outputs, hidden = self.rnn(packed_embedded)\n",
        "    #packed_outputs is a packed sequence containing all hidden states\n",
        "    #hidden is now from the final non-padded element in the batch\n",
        "\n",
        "    outputs, len_list = nn.utils.rnn.pad_packed_sequence(packed_outputs) #outputs is now a non-packed sequence, all hidden states obtained\n",
        "    #  when the input is a pad token are all zeros\n",
        "\n",
        "    #outputs = [src len, batch size, hid dim * num directions]\n",
        "    #hidden = [n layers * num directions, batch size, hid dim]\n",
        "\n",
        "    #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
        "    #outputs are always from the last layer\n",
        "\n",
        "    #hidden [-2, :, : ] is the last of the forwards RNN\n",
        "    #hidden [-1, :, : ] is the last of the backwards RNN\n",
        "\n",
        "    #initial decoder hidden is final hidden state of the forwards and backwards\n",
        "    #  encoder RNNs fed through a linear layer\n",
        "    hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
        "\n",
        "    #outputs = [src len, batch size, enc hid dim * 2]\n",
        "    #hidden = [batch size, dec hid dim]\n",
        "    return outputs, hidden"
      ],
      "metadata": {
        "id": "kYO9-yg4UpEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attn"
      ],
      "metadata": {
        "id": "vJm8VpgbUsbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "  def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "    super().__init__()\n",
        "    self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
        "    self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
        "\n",
        "  def forward(self, hidden, encoder_outputs, mask):\n",
        "    #hidden = [batch size, dec hid dim]\n",
        "    #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
        "    batch_size = encoder_outputs.shape[1]\n",
        "    src_len = encoder_outputs.shape[0]\n",
        "\n",
        "    #repeat decoder hidden state src_len times\n",
        "    hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)  #hidden = [batch size, src len, dec hid dim]\n",
        "    encoder_outputs = encoder_outputs.permute(1, 0, 2)  #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
        "    energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) #energy = [batch size, src len, dec hid dim]\n",
        "\n",
        "    attention = self.v(energy).squeeze(2) #attention = [batch size, src len]\n",
        "    attention = attention.masked_fill(mask == 0, -1e10)\n",
        "    return F.softmax(attention, dim = 1)"
      ],
      "metadata": {
        "id": "hthEkeIXU1lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder"
      ],
      "metadata": {
        "id": "7M3M0cKNUsfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
        "    super().__init__()\n",
        "    self.output_dim = output_dim\n",
        "    self.attention = attention\n",
        "    self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "    self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
        "    self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, input, hidden, encoder_outputs, mask):\n",
        "    #input = [batch size]\n",
        "    #hidden = [batch size, dec hid dim]\n",
        "    #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
        "    #mask = [batch size, src len]\n",
        "    input = input.unsqueeze(0)  #input = [1, batch size]\n",
        "    embedded = self.dropout(self.embedding(input))  #embedded = [1, batch size, emb dim]\n",
        "\n",
        "    a = self.attention(hidden, encoder_outputs, mask) #a = [batch size, src len]\n",
        "    a = a.unsqueeze(1)  #a = [batch size, 1, src len]\n",
        "\n",
        "    encoder_outputs = encoder_outputs.permute(1, 0, 2)  #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
        "\n",
        "    weighted = torch.bmm(a, encoder_outputs)  #weighted = [batch size, 1, enc hid dim * 2]\n",
        "    weighted = weighted.permute(1, 0, 2)  #weighted = [1, batch size, enc hid dim * 2]\n",
        "\n",
        "    rnn_input = torch.cat((embedded, weighted), dim = 2)  #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n",
        "\n",
        "    output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
        "    #output = [seq len, batch size, dec hid dim * n directions]\n",
        "    #hidden = [n layers * n directions, batch size, dec hid dim]\n",
        "\n",
        "    #seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
        "    #output = [1, batch size, dec hid dim]\n",
        "    #hidden = [1, batch size, dec hid dim]\n",
        "    #this also means that output == hidden\n",
        "    assert (output == hidden).all()\n",
        "\n",
        "    embedded = embedded.squeeze(0)\n",
        "    output = output.squeeze(0)\n",
        "    weighted = weighted.squeeze(0)\n",
        "\n",
        "    prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))  #prediction = [batch size, output dim]\n",
        "    return prediction, hidden.squeeze(0), a.squeeze(1)"
      ],
      "metadata": {
        "id": "JVIADpjwU48J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Seq2Seq"
      ],
      "metadata": {
        "id": "WwSO5W3KUsjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, encoder, decoder, src_pad_idx, device):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.src_pad_idx = src_pad_idx\n",
        "    self.device = device\n",
        "\n",
        "  def create_mask(self, src):\n",
        "    mask = (src != self.src_pad_idx).permute(1, 0)\n",
        "    return mask\n",
        "\n",
        "  def forward(self, datas, criterion=None, teacher_forcing_ratio = 0.5):\n",
        "    #src = [src len, batch size]\n",
        "    #src_len = [batch size]\n",
        "    #trg = [trg len, batch size]\n",
        "    #trg_len = [batch size]\n",
        "    #teacher_forcing_ratio is probability of using trg to be input else prev output to be input for next prediction.\n",
        "    (src, src_len), (trg, _) = datas\n",
        "    batch_size = src.shape[1]\n",
        "    trg_len = trg.shape[0]\n",
        "    trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "    # SORT\n",
        "    sort_ids, unsort_ids = self.sort_by_sent_len(src_len)\n",
        "    src, src_len, trg = src[:, sort_ids], src_len[sort_ids], trg[:, sort_ids]\n",
        "\n",
        "    #tensor to store decoder outputs\n",
        "    outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "\n",
        "    #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
        "    #hidden is the final forward and backward hidden states, passed through a linear layer\n",
        "    encoder_outputs, hidden = self.encoder(src, src_len)\n",
        "\n",
        "    #first input to the decoder is the <sos> tokens\n",
        "    input = trg[0,:]\n",
        "\n",
        "    mask = self.create_mask(src)  #mask = [batch size, src len]\n",
        "\n",
        "    for t in range(1, trg_len):\n",
        "      #insert input token embedding, previous hidden state, all encoder hidden states and mask\n",
        "      #receive output tensor (predictions) and new hidden state\n",
        "      output, hidden, _ = self.decoder(input, hidden, encoder_outputs, mask)\n",
        "\n",
        "      #place predictions in a tensor holding predictions for each token\n",
        "      outputs[t] = output\n",
        "\n",
        "      #if teacher forcing, use actual next token as next input. Else, use predicted token\n",
        "      input = trg[t] if random.random() < teacher_forcing_ratio else output.argmax(1)\n",
        "\n",
        "    if criterion != None:\n",
        "      loss = self.compute_loss(outputs, trg, criterion)\n",
        "      return loss, outputs[:, unsort_ids, :]\n",
        "    return outputs[:, unsort_ids, :]\n",
        "\n",
        "  def compute_loss(self, output, trg, criterion):\n",
        "    #output = (trg_len, batch_size, trg_vocab_size)\n",
        "    #trg = [trg len, batch size]\n",
        "    output = output[1:].view(-1, output.shape[-1])  #output = [(trg len - 1) * batch size, output dim]\n",
        "    trg = trg[1:].view(-1)  #trg = [(trg len - 1) * batch size]\n",
        "    loss = criterion(output, trg)\n",
        "    return loss\n",
        "\n",
        "  # NEWLY ADDED ##########################\n",
        "  def sort_by_sent_len(self, sent_len):\n",
        "    _, sort_ids = sent_len.sort(descending=True)\n",
        "    unsort_ids = sort_ids.argsort()\n",
        "    return sort_ids, unsort_ids\n",
        "  # END ADDED ############################"
      ],
      "metadata": {
        "id": "Pyz0ZITCU9r5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pivot model (update)"
      ],
      "metadata": {
        "id": "fdjE61pw7CBy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**still need to reorganize code to use for infer (no criterions, only 1 data: src, src_len)**"
      ],
      "metadata": {
        "id": "qt2bmz1GTTnj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PivotSeq2Seq(nn.Module):\n",
        "  def __init__(self, models: list, fields: list, device, alpha=1.1, lamda=0.75):\n",
        "    super().__init__()\n",
        "    self.num_model = len(models)\n",
        "    self.fields = fields\n",
        "    self.num_field = len(fields)\n",
        "    self.device = device\n",
        "    self.alpha = alpha\n",
        "    self.lamda = lamda\n",
        "\n",
        "    for i in range(self.num_model):\n",
        "      self.add_module(f'model_{i}', models[i])\n",
        "\n",
        "    assert len(models)+1 == len(fields), f\"Not enough Fields for models: num_field={len(fields)} != {len(models)+1}\"\n",
        "\n",
        "  def forward(self, datas: list, criterions=None, teacher_forcing_ratio=0.5):\n",
        "    '''\n",
        "    datas: list of data: [(src, src_len), (piv1, piv_len1), ... , (pivM, piv_lenM), (trg, trg_len)] given M models\n",
        "      src = [src len, batch_size]\n",
        "      src_len = [batch_size]\n",
        "      ...\n",
        "      trg = [trg len, batch_size]\n",
        "      trg_len = [batch_size]\n",
        "    criterions: list of criterion for each model\n",
        "    '''\n",
        "    if criterions != None:\n",
        "      loss_list, output_list = self.run(datas, criterions, teacher_forcing_ratio)\n",
        "      total_loss = self.compute_loss(loss_list)\n",
        "      return total_loss, output_list[-1]\n",
        "    else:\n",
        "      criterions = [None for _ in range(self.num_model)]\n",
        "      _, output_list = self.run(datas, criterions, teacher_forcing_ratio)\n",
        "      return output_list[-1]\n",
        "\n",
        "  def run(self, datas, criterions, teacher_forcing_ratio):\n",
        "    assert self.num_model+1 == len(datas), f\"Not enough datas for models: data_len={len(datas)} != {self.num_model+1}\"\n",
        "    assert self.num_model == len(criterions), f'Criterions must have for each model: num_criterion={len(criterions)} != {self.num_model}'\n",
        "\n",
        "    output_list, loss_list = [], []\n",
        "    for i in range(self.num_model):\n",
        "      isForceOn = True if i==0 else random.random() < teacher_forcing_ratio # 1st model must always use src\n",
        "\n",
        "      # GET NEW INPUT\n",
        "      src, src_len = datas[i] if isForceOn else self.process_output(output_list[-1], self.fields[i+1])\n",
        "      trg, trg_len = datas[i+1]\n",
        "\n",
        "      # FORWARD MODEL\n",
        "      model = getattr(self, f'model_{i}') # Seq2Seq model already sort src by src_len in forward\n",
        "      data = [(src, src_len), (trg, trg_len)]\n",
        "      criterion = criterions[i]\n",
        "      output = model(data, criterion, 0 if criterion==None else teacher_forcing_ratio)\n",
        "\n",
        "      if criterion == None:\n",
        "        output_list.append(output)\n",
        "      else:\n",
        "        assert len(output) == 2, 'With criterion, model should return loss & prediction'\n",
        "        loss, out = output\n",
        "        loss_list.append(loss)\n",
        "        output_list.append(out)\n",
        "\n",
        "    return loss_list, output_list\n",
        "\n",
        "  def compute_loss(self, loss_list):\n",
        "    total_loss = 0.0\n",
        "    for i in range(len(loss_list) - 1): # except final output\n",
        "      total_loss += loss_list[i]\n",
        "    total_loss += self.alpha*loss_list[-1]\n",
        "    return total_loss + self.lamda*self.compute_embed_loss()\n",
        "\n",
        "  def compute_embed_loss(self):\n",
        "    embed_loss = 0.0\n",
        "    for i in range(1, self.num_model):\n",
        "      model1 = getattr(self, f'model_{i-1}')\n",
        "      model2 = getattr(self, f'model_{i}')\n",
        "      embed_loss += torch.sum(F.pairwise_distance(model1.decoder.embedding.weight, model2.encoder.embedding.weight, p=2))\n",
        "    return embed_loss\n",
        "\n",
        "  def sort_by_src_len(self, piv, piv_len, datas): # piv = [piv_len, batch_size]\n",
        "    piv_len, sorted_ids = piv_len.sort(descending=True)\n",
        "    sorted_datas = [(sent[:, sorted_ids], sent_len[sorted_ids]) for (sent, sent_len) in datas]\n",
        "    return piv[:, sorted_ids], piv_len, sorted_datas  # piv sorted along batch_size\n",
        "\n",
        "  def process_output(self, output, piv_field):\n",
        "    # output = [trg len, batch size, output dim]\n",
        "    # trg = [trg len, batch size]\n",
        "    # Process output1 to be input for model2\n",
        "    seq_len, N, _ = output.shape\n",
        "    tmp_out = output.argmax(2)  # tmp_out = [seq_len, batch_size]\n",
        "    # re-create pivot as src for model2\n",
        "    piv = torch.zeros_like(tmp_out).type(torch.long).to(output.device)\n",
        "    piv[0, :] = torch.full_like(piv[0, :], piv_field.vocab.stoi[piv_field.init_token])  # fill all first idx with sos_token\n",
        "\n",
        "    for i in range(1, seq_len):  # for each i in seq_len\n",
        "      # if tmp_out's prev is eos_token, replace w/ pad_token, else current value\n",
        "      eos_mask = (tmp_out[i-1, :] == piv_field.vocab.stoi[piv_field.eos_token])\n",
        "      piv[i, :] = torch.where(eos_mask, piv_field.vocab.stoi[piv_field.pad_token], tmp_out[i, :])\n",
        "      # if piv's prev is pad_token, replace w/ pad_token, else current value\n",
        "      pad_mask = (piv[i-1, :] == piv_field.vocab.stoi[piv_field.pad_token])\n",
        "      piv[i, :] = torch.where(pad_mask, piv_field.vocab.stoi[piv_field.pad_token], piv[i, :])\n",
        "\n",
        "    # Trim down extra pad tokens\n",
        "    tensor_list = [piv[i] for i in range(seq_len) if not all(piv[i] == piv_field.vocab.stoi[piv_field.pad_token])]  # tensor_list = [new_seq_len, batch_size]\n",
        "    piv = torch.stack([x for x in tensor_list], dim=0).type(torch.long).to(output.device)\n",
        "    assert not all(piv[-1] == piv_field.vocab.stoi[piv_field.pad_token]), 'Not completely trim down tensor'\n",
        "\n",
        "    # get seq_id + eos_tok id of each sequence\n",
        "    piv_ids, eos_ids = (piv.permute(1, 0) == piv_field.vocab.stoi[piv_field.eos_token]).nonzero(as_tuple=True)  # piv_len = [N]\n",
        "    piv_len = torch.full_like(piv[0], seq_len).type(torch.long)  # init w/ longest seq\n",
        "    piv_len[piv_ids] = eos_ids + 1 # seq_len = eos_tok + 1\n",
        "\n",
        "    return piv, piv_len"
      ],
      "metadata": {
        "id": "kdQAxGHj7E2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Triangulate model"
      ],
      "metadata": {
        "id": "KgwBXdjq3IkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TriangSeq2Seq(nn.Module):\n",
        "  def __init__(self, models: list, output_dim, device, alpha=1.1, method='max', train_backbone=True):\n",
        "    # output_dim = trg vocab size\n",
        "    super(TriangSeq2Seq, self).__init__()\n",
        "    self.num_model = len(models)\n",
        "    self.output_dim = output_dim\n",
        "    self.device = device\n",
        "    self.alpha = alpha\n",
        "    self.method = method\n",
        "    self.train_backbone = train_backbone\n",
        "    if method=='weighted':\n",
        "      self.head = nn.Sequential(\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(output_dim*self.num_model, output_dim)\n",
        "      )\n",
        "    elif method=='weighted_1':\n",
        "      # self.head = nn.Sequential(  # traing-EnFr-EnEsFr-dropout-1\n",
        "      #     nn.Linear(self.num_model, self.num_model),\n",
        "      #     nn.ReLU(),\n",
        "      #     nn.Dropout(p=0.5),\n",
        "      #     nn.Linear(self.num_model, 1),\n",
        "      # )\n",
        "      # self.head = nn.Sequential(  # traing-EnFr-EnEsFr-1\n",
        "      #     nn.Linear(self.num_model, self.num_model*2),\n",
        "      #     nn.ReLU(),\n",
        "      #     nn.Linear(self.num_model*2, self.num_model*2),\n",
        "      #     nn.ReLU(),\n",
        "      #     nn.Linear(self.num_model*2, 1),\n",
        "      # )\n",
        "      # self.head = nn.Sequential(  # traing-EnFr-EnEsFr-1-1\n",
        "      #     nn.Linear(self.num_model, self.num_model*2),\n",
        "      #     nn.ReLU(),\n",
        "      #     nn.Dropout(p=0.2),\n",
        "      #     nn.Linear(self.num_model*2, self.num_model*2),\n",
        "      #     nn.ReLU(),\n",
        "      #     nn.Dropout(p=0.2),\n",
        "      #     nn.Linear(self.num_model*2, 1),\n",
        "      # )\n",
        "      self.head = nn.Sequential(  # traing-EnFr-EnEsFr-dense5\n",
        "            nn.Linear(self.num_model, 5),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.Linear(5, 5),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.Linear(5, 1),\n",
        "        )\n",
        "    for i in range(self.num_model):\n",
        "      for param in models[i].parameters():\n",
        "        param.requires_grad = train_backbone\n",
        "      self.add_module(f'model_{i}', models[i])\n",
        "\n",
        "  def forward(self, datas: dict, criterions=None, teacher_forcing_ratio=0.5):\n",
        "    '''\n",
        "    datas: dict of data:\n",
        "      {\"model_0\": (src, src_len, trg, trg_len), \"model_1\": [(src, src_len), (piv, piv_len), (trg, trg_len)], ..., \"TRG\": (trg, trg_len)}\n",
        "      src = [src len, batch size]\n",
        "      src_len = [batch size]\n",
        "    criterions: dict of criterions\n",
        "      {\"model_0\": criterion_0, \"model_1\": criterion_1, ..., \"TRG\": criterion_M}\n",
        "    '''\n",
        "    if criterions != None:\n",
        "      loss_list, output_list = self.run(datas, criterions, teacher_forcing_ratio)\n",
        "      final_out = self.get_final_pred(output_list)\n",
        "      if self.method!='max':total_loss = self.alpha*self.compute_final_pred_loss(final_out, datas[\"TRG\"], criterions[\"TRG\"]) + self.compute_submodels_loss(loss_list)\n",
        "      else: total_loss = self.compute_submodels_loss(loss_list)\n",
        "      return total_loss, final_out\n",
        "    else:\n",
        "      criterions = {f'model_{i}':None for i in range(self.num_model)}\n",
        "      criterions['TRG'] = None\n",
        "      loss_list, output_list = self.run(datas, criterions, teacher_forcing_ratio)\n",
        "      final_out = self.get_final_pred(output_list)\n",
        "      return final_out\n",
        "\n",
        "  def run(self, datas, criterions, teacher_forcing_ratio):\n",
        "    assert self.num_model+1 == len(datas), f\"Not enough datas for models: data_len={len(datas)} != {self.num_model+1}\"  # include 'TRG'\n",
        "    assert self.num_model+1 == len(criterions), f'Criterions must have for each model: num_criterion={len(criterions)} != {self.num_model+1}' # include 'TRG'\n",
        "\n",
        "    output_list = []\n",
        "    loss_list = []\n",
        "    for i in range(self.num_model):\n",
        "      data = datas[f'model_{i}']\n",
        "      model = getattr(self, f'model_{i}')\n",
        "      criterion = criterions[f'model_{i}']\n",
        "      output = model(data, criterion, 0 if criterion==None else teacher_forcing_ratio)\n",
        "\n",
        "      if criterion == None:\n",
        "        output_list.append(output)\n",
        "      else:\n",
        "        assert len(output) == 2, 'With criterion, model should return loss & prediction'\n",
        "        loss_list.append(output[0])\n",
        "        output_list.append(output[1])\n",
        "\n",
        "    return loss_list, output_list\n",
        "\n",
        "  def compute_submodels_loss(self, loss_list):\n",
        "    total_loss = 0.0\n",
        "    for loss in loss_list:\n",
        "      total_loss += loss\n",
        "    return total_loss\n",
        "\n",
        "  def compute_final_pred_loss(self, output, data, criterion):\n",
        "    #output = (trg_len, batch_size, trg_vocab_size)\n",
        "    #data = [trg, trg_len]  # trg.shape = [seq_len, batch_size]\n",
        "    trg, _ = data\n",
        "    output = output[1:].reshape(-1, output.shape[-1])  #output = [(trg len - 1) * batch size, output dim]\n",
        "    trg = trg[1:].reshape(-1)  #trg = [(trg len - 1) * batch size]\n",
        "    loss = criterion(output, trg)\n",
        "    return loss\n",
        "\n",
        "  def get_final_pred(self, output_list):  # output_list[0] shape = [seq_len, N, out_dim]\n",
        "    # assert all([output_list[i].shape == output_list[i-1].shape for i in range(1, len(output_list))]), 'all outputs must match shape [seq_len, N, out_dim]'\n",
        "    seq_len, N, out_dim = output_list[0].shape\n",
        "    if self.method=='weighted':\n",
        "      linear_in = torch.cat([out for out in output_list], dim=-1) # linear_in = [seq_len, N, out_dim * num_model]. Note that num_model = len(output_list)\n",
        "      final_out = self.head(linear_in)  # final_out = [seq_len, N, out_dim]\n",
        "      return final_out\n",
        "    elif self.method=='weighted_1':\n",
        "      output_list = [out.permute(1, 0, 2).reshape(N, -1) for out in output_list]  # [N, seq_len, out_dim] --> [N, seq_len*out_dim]\n",
        "      final_out = self.head(torch.stack(output_list, dim=-1)).squeeze(-1)  # [N, seq_len*out_dim, num_model] --> [N, seq_len*out_dim, 1] --> [N, seq_len*out_dim]\n",
        "      return final_out.reshape(N, seq_len, out_dim).permute(1, 0, 2) # [N, seq_len, out_dim] --> [seq_len, N, out_dim]\n",
        "    elif self.method=='average':\n",
        "      outputs = torch.mean(torch.stack(output_list, dim=0), dim=0)\n",
        "      return outputs\n",
        "    elif self.method=='max':\n",
        "      all_t = torch.stack(output_list, dim=-1)\n",
        "      prob_ts = torch.stack([F.softmax(d, -1) for d in output_list], dim=-1)\n",
        "\n",
        "      final_selected_ts = []\n",
        "      for sent_id in range(N):\n",
        "        # get ids of each model (get selected words)\n",
        "        ids_list = []\n",
        "        for m in range(self.num_model):\n",
        "          m_t = prob_ts[..., m] # [seq_len, N, out_dim]\n",
        "          ids_list.append(torch.argmax(m_t[:, sent_id, :], dim=-1, keepdim=True))\n",
        "        # get the confusion matrix\n",
        "        all_pairs = []\n",
        "        for t in range(self.num_model):\n",
        "          t_eachM = []\n",
        "          for m in range(self.num_model):\n",
        "            m_t = prob_ts[..., m]\n",
        "            t_eachM.append(torch.gather(m_t[:, sent_id, :], -1, ids_list[t]))\n",
        "          all_pairs.append(t_eachM)\n",
        "        # calculate the prob of each sent\n",
        "        t_m_prod = []\n",
        "        for t in range(self.num_model):\n",
        "          t_eachM = []\n",
        "          for m in range(self.num_model):\n",
        "            t1_m1 = all_pairs[t][m]\n",
        "            t_eachM.append(torch.mean(t1_m1, dim=0))  # original: prod\n",
        "          t_m_prod.append(t_eachM)\n",
        "        t_totals = [torch.stack(t_m_prod[t], dim=-1)for t in range(self.num_model)]\n",
        "        t_means = [torch.mean(t_totals[t], dim=-1) for t in range(self.num_model)]\n",
        "        t_cats = torch.cat(t_means, dim=-1)\n",
        "        t_selects = torch.argmax(t_cats)\n",
        "        selected_t = torch.select(all_t[:, sent_id, ...], -1, t_selects)\n",
        "        final_selected_ts.append(selected_t)\n",
        "      output = torch.stack(final_selected_ts, dim=1)\n",
        "      return output\n",
        "    else:\n",
        "      return output_list[0]"
      ],
      "metadata": {
        "id": "FKCumOEK3T6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train func"
      ],
      "metadata": {
        "id": "2cDqYW-LTnGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_trainlog(data: list, filename: str=f'{DIR_PATH}/training_log.txt'):\n",
        "  ''' Update training log w/ new losses\n",
        "  Args:\n",
        "      data (List): a list of infor for many epochs as tuple, each tuple has model_name, loss, etc.\n",
        "      filename (String): path + file_name\n",
        "  Return:\n",
        "      None: new data is appended into train-log\n",
        "  '''\n",
        "  with open(filename, 'a') as f: # save\n",
        "    for epoch in data:\n",
        "      f.write(','.join(epoch))\n",
        "      f.write(\"\\n\")\n",
        "  print('update_trainlog SUCCESS')\n",
        "  return []\n",
        "def init_weights(m):\n",
        "  for name, param in m.named_parameters():\n",
        "    if 'weight' in name:\n",
        "      nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "    else:\n",
        "      nn.init.constant_(param.data, 0)\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "metadata": {
        "id": "iaocGp1dS3RJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Seq2Seq"
      ],
      "metadata": {
        "id": "H9Z6RxOnzLa9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trainSeq2Seq(model, iterator, optimizer, criterion, clip):\n",
        "  model.train()\n",
        "  epoch_loss = 0.0\n",
        "  for batch in tqdm(iterator):\n",
        "    optimizer.zero_grad()\n",
        "    datas = [batch.en, batch.fr]\n",
        "    loss, _ = model(datas, criterion, 0.5)\n",
        "\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "    optimizer.step()\n",
        "    epoch_loss += loss.item()\n",
        "  return epoch_loss / len(iterator)\n",
        "\n",
        "def evaluateSeq2Seq(model, iterator, criterion):\n",
        "  model.eval()\n",
        "  epoch_loss = 0.0\n",
        "  with torch.no_grad():\n",
        "    for batch in tqdm(iterator):\n",
        "      datas = [batch.en, batch.fr]\n",
        "      loss, _ = model(datas, criterion, 0) # turn off teacher forcing\n",
        "      epoch_loss += loss.item()\n",
        "    return epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "PyiHo_rj7b3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pivot"
      ],
      "metadata": {
        "id": "Jfky7439zNhN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trainPivot(model, iterator, optimizer, criterions, clip):\n",
        "  model.train()\n",
        "  epoch_loss = 0.0\n",
        "  for batch in tqdm(iterator):\n",
        "    optimizer.zero_grad()\n",
        "    model_inputs = [batch.en, batch.es, batch.fr]\n",
        "    loss, _ = model(model_inputs, criterions, 0.5)\n",
        "\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "    optimizer.step()\n",
        "    epoch_loss += loss.item()\n",
        "  return epoch_loss / len(iterator)\n",
        "\n",
        "def evaluatePivot(model, iterator, criterions):\n",
        "  model.eval()\n",
        "  epoch_loss = 0.0\n",
        "  with torch.no_grad():\n",
        "    for batch in tqdm(iterator):\n",
        "      model_inputs = [batch.en, batch.es, batch.fr]\n",
        "      loss, _ = model(model_inputs, criterions, 0)\n",
        "      epoch_loss += loss.item()\n",
        "    return epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "Au60zQ85n8No"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Triangulate"
      ],
      "metadata": {
        "id": "ARfI-GykzP9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for triangulate model\n",
        "def trainTriang(model, iterator, optimizer, criterions, clip):\n",
        "  model.train()\n",
        "  epoch_loss = 0.0\n",
        "  for batch in tqdm(iterator):\n",
        "    optimizer.zero_grad()\n",
        "    model_inputs = {\n",
        "        'model_0': [batch.en, batch.fr],\n",
        "        'model_1': [batch.en, vars(batch)['es'], batch.fr],\n",
        "        'TRG': batch.fr\n",
        "    }\n",
        "    loss, _ = model(model_inputs, criterions, 0.5)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "    optimizer.step()\n",
        "    epoch_loss += loss.item()\n",
        "  return epoch_loss / len(iterator)\n",
        "\n",
        "def evaluateTriang(model, iterator, criterions):\n",
        "  model.eval()\n",
        "  epoch_loss = 0.0\n",
        "  with torch.no_grad():\n",
        "    for batch in tqdm(iterator):\n",
        "      model_inputs = {\n",
        "        'model_0': [batch.en, batch.fr],\n",
        "        'model_1': [batch.en, vars(batch)['es'], batch.fr],\n",
        "        'TRG': batch.fr\n",
        "      }\n",
        "      loss, _ = model(model_inputs, criterions, 0)\n",
        "      epoch_loss += loss.item()\n",
        "    return epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "TsX8opIkn7y7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "MYFp2rpQUEbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = {\n",
        "    'EMB_DIM': 256,\n",
        "    'HID_DIM': 512,\n",
        "    'DROPOUT': 0.5,\n",
        "    'en_DIM': 6964,\n",
        "    'fr_DIM': 9703,\n",
        "    'es_DIM': 10461,\n",
        "    'it_DIM': 10712,\n",
        "    'pt_DIM': 10721,\n",
        "    'ro_DIM': 11989\n",
        "}"
      ],
      "metadata": {
        "id": "f3fRYyFRUYjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Seq2Seq"
      ],
      "metadata": {
        "id": "VKGgTuL4tThr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For 2 langs\n",
        "EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "DROPOUT = 0.5\n",
        "\n",
        "INPUT_DIM = len(EN_FIELD.vocab)\n",
        "OUTPUT_DIM = len(FR_FIELD.vocab)\n",
        "SRC_PAD_IDX = EN_FIELD.vocab.stoi[EN_FIELD.pad_token]\n",
        "\n",
        "attn = Attention(HID_DIM, HID_DIM)\n",
        "enc = Encoder(INPUT_DIM, EMB_DIM, HID_DIM, HID_DIM, DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM, HID_DIM, DROPOUT, attn)\n",
        "\n",
        "model = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)"
      ],
      "metadata": {
        "id": "9b_1FE6ZlpEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pivot"
      ],
      "metadata": {
        "id": "TmXsuwuctVw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "DROPOUT = 0.5\n",
        "\n",
        "INPUT_DIM = len(EN_FIELD.vocab)\n",
        "PIV_DIM = len(ES_FIELD.vocab)\n",
        "OUTPUT_DIM = len(FR_FIELD.vocab)\n",
        "\n",
        "SRC_PAD_IDX = EN_FIELD.vocab.stoi[EN_FIELD.pad_token]\n",
        "attn1 = Attention(HID_DIM, HID_DIM)\n",
        "enc1 = Encoder(INPUT_DIM, EMB_DIM, HID_DIM, HID_DIM, DROPOUT)\n",
        "dec1 = Decoder(PIV_DIM, EMB_DIM, HID_DIM, HID_DIM, DROPOUT, attn1)\n",
        "model1 = Seq2Seq(enc1, dec1, SRC_PAD_IDX, device).to(device)\n",
        "\n",
        "PIV_PAD_IDX = ES_FIELD.vocab.stoi[ES_FIELD.pad_token]\n",
        "attn2 = Attention(HID_DIM, HID_DIM)\n",
        "enc2 = Encoder(PIV_DIM, EMB_DIM, HID_DIM, HID_DIM, DROPOUT)\n",
        "dec2 = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM, HID_DIM, DROPOUT, attn2)\n",
        "model2 = Seq2Seq(enc2, dec2, PIV_PAD_IDX, device).to(device)\n",
        "\n",
        "models = [model1, model2]\n",
        "fields = [EN_FIELD, ES_FIELD, FR_FIELD]\n",
        "model = PivotSeq2Seq(models, fields, device).to(device)"
      ],
      "metadata": {
        "id": "zhpJPc2DaIHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src, trg = (1, 2), (3, 4)\n",
        "data = {'TRG': trg}\n",
        "for i in range(model_1.num_model):\n",
        "  submodel = model_1.get_submodule(f'model_{i}')\n",
        "  if isinstance(submodel, Seq2Seq):\n",
        "    data[f'model_{i}'] = [src, trg]\n",
        "  elif isinstance(submodel, PivotSeq2Seq):\n",
        "    data[f'model_{i}'] = [src] + [trg for _ in range(submodel.num_model)]\n",
        "data"
      ],
      "metadata": {
        "id": "w17mudUK93th"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = f'{DIR_PATH}/piv-EnEsFr.pt'\n",
        "ckpt = torch.load(model_path)\n",
        "# optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
        "# scheduler.load_state_dict(ckpt['scheduler_state_dict'])\n",
        "model.load_state_dict(ckpt['model_state_dict']) # strict=False if some dimensions are different"
      ],
      "metadata": {
        "id": "P4-h-OaXw-5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Triangulate"
      ],
      "metadata": {
        "id": "ifgtDQwvtRcw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DIRECT\n",
        "SRC_PAD_IDX = EN_FIELD.vocab.stoi[EN_FIELD.pad_token]\n",
        "attn = Attention(cfg['HID_DIM'], cfg['HID_DIM'])\n",
        "enc = Encoder(cfg['en_DIM'], cfg['EMB_DIM'], cfg['HID_DIM'], cfg['HID_DIM'], cfg['DROPOUT'])\n",
        "dec = Decoder(cfg['fr_DIM'], cfg['EMB_DIM'], cfg['HID_DIM'], cfg['HID_DIM'], cfg['DROPOUT'], attn)\n",
        "\n",
        "model_direct = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)\n",
        "modelname = f'seq2seq-EnFr-1.pt'\n",
        "model_direct.load_state_dict(torch.load(f'{DIR_PATH}/{modelname}')['model_state_dict'])\n",
        "print(f'loaded model {modelname}')\n",
        "\n",
        "# PIVOT\n",
        "lang = 'es'\n",
        "TMP_FIELD = FIELD_DICT[lang]\n",
        "\n",
        "SRC_PAD_IDX = EN_FIELD.vocab.stoi[EN_FIELD.pad_token]\n",
        "attn1 = Attention(cfg['HID_DIM'], cfg['HID_DIM'])\n",
        "enc1 = Encoder(cfg['en_DIM'], cfg['EMB_DIM'], cfg['HID_DIM'], cfg['HID_DIM'], cfg['DROPOUT'])\n",
        "dec1 = Decoder(cfg[f'{lang}_DIM'], cfg['EMB_DIM'], cfg['HID_DIM'], cfg['HID_DIM'], cfg['DROPOUT'], attn1)\n",
        "model1 = Seq2Seq(enc1, dec1, SRC_PAD_IDX, device).to(device)\n",
        "\n",
        "PIV_PAD_IDX = TMP_FIELD.vocab.stoi[TMP_FIELD.pad_token]\n",
        "attn2 = Attention(cfg['HID_DIM'], cfg['HID_DIM'])\n",
        "enc2 = Encoder(cfg[f'{lang}_DIM'], cfg['EMB_DIM'], cfg['HID_DIM'], cfg['HID_DIM'], cfg['DROPOUT'])\n",
        "dec2 = Decoder(cfg['fr_DIM'], cfg['EMB_DIM'], cfg['HID_DIM'], cfg['HID_DIM'], cfg['DROPOUT'], attn2)\n",
        "model2 = Seq2Seq(enc2, dec2, PIV_PAD_IDX, device).to(device)\n",
        "\n",
        "models = [model1, model2]\n",
        "fields = [EN_FIELD, TMP_FIELD, FR_FIELD]\n",
        "model_piv = PivotSeq2Seq(models, fields, device).to(device)\n",
        "modelname = f'piv-En{lang[0].upper()}{lang[1]}Fr.pt'\n",
        "model_piv.load_state_dict(torch.load(f'{DIR_PATH}/{modelname}')['model_state_dict'])\n",
        "print(f'loaded model {modelname}')\n",
        "\n",
        "# TRIANGULATE\n",
        "models = [model_direct, model_piv]\n",
        "model = TriangSeq2Seq(models, cfg['fr_DIM'], device, alpha=1.1, method='max', train_backbone=False).to(device)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "metadata": {
        "id": "6bCDFxzKtZPa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8f42780-e9ce-41b8-f0ab-cb8ddd2fe8d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded model seq2seq-EnFr-1.pt\n",
            "loaded model piv-EnEsFr.pt\n",
            "The model has 0 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train loop"
      ],
      "metadata": {
        "id": "ZhCTMvF2tbQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if isinstance(model, Seq2Seq) or isinstance(model, PivotSeq2Seq) or (isinstance(model, TriangSeq2Seq) and model.train_backbone):\n",
        "  # model.apply(init_weights) # remove if backbone models are freezed\n",
        "  print('init_weights success')\n",
        "elif isinstance(model, TriangSeq2Seq) and not model.train_backbone:\n",
        "  # model.head.apply(init_weights)\n",
        "  print('init_weights head ONLY success')\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "metadata": {
        "id": "M7CeajzjlzWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion2 = nn.CrossEntropyLoss(ignore_index = FR_FIELD.vocab.stoi[FR_FIELD.pad_token])\n",
        "\n",
        "if isinstance(model, PivotSeq2Seq) or isinstance(model, TriangSeq2Seq):\n",
        "  criterion1 = nn.CrossEntropyLoss(ignore_index = FIELD_DICT['es'].vocab.stoi[FIELD_DICT['es'].pad_token])\n",
        "  criterions3 = [criterion1, criterion2]\n",
        "  print('Created criterion for piv')\n",
        "\n",
        "if isinstance(model, TriangSeq2Seq):\n",
        "  criterions = {\n",
        "      'model_0': criterion2,\n",
        "      'model_1': criterions3,\n",
        "      'TRG': nn.CrossEntropyLoss(ignore_index = FR_FIELD.vocab.stoi[FR_FIELD.pad_token])\n",
        "  }\n",
        "  print('Created criterion for triang')"
      ],
      "metadata": {
        "id": "V_5TFOZsmPsH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b154297-61e1-4d73-c0dd-6a3138d83b72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created criterion for piv\n",
            "Created criterion for triang\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "N_EPOCHS = 1\n",
        "CLIP = 1\n",
        "best_valid_loss = float('inf')\n",
        "best_train_loss = float('inf')\n",
        "model_name = 'traing-EnFr-EnEsFr-dense5.pt'\n",
        "train_log = []"
      ],
      "metadata": {
        "id": "FIYc-s0FTPRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LR = 1e-4\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[1], gamma=1.0/3.0)\n",
        "scheduler.get_last_lr()"
      ],
      "metadata": {
        "id": "edsBDaCEmQD_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "846b5a57-45a6-43b0-e7da-952251ca146c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0001]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(N_EPOCHS):\n",
        "  if isinstance(model, Seq2Seq):\n",
        "    train_loss = trainSeq2Seq(model, train_iterator, optimizer, criterion2, CLIP)\n",
        "    valid_loss = evaluateSeq2Seq(model, valid_iterator, criterion2)\n",
        "  elif isinstance(model, PivotSeq2Seq):\n",
        "    train_loss = trainPivot(model, train_iterator, optimizer, criterions3, CLIP)\n",
        "    valid_loss = evaluatePivot(model, valid_iterator, criterions)\n",
        "  elif isinstance(model, TriangSeq2Seq):\n",
        "    train_loss = trainTriang(model, train_iterator, optimizer, criterions, CLIP)\n",
        "    valid_loss = evaluateTriang(model, valid_iterator, criterions)\n",
        "  else: raise Exception('Model type is unknown')\n",
        "\n",
        "  print('scheduler.get_last_lr()', scheduler.get_last_lr())\n",
        "  epoch_info = [model_name, dataname, scheduler.get_last_lr()[0], BATCH_SIZE, HID_DIM, DROPOUT, epoch, N_EPOCHS, train_loss, valid_loss]\n",
        "  train_log.append([str(info) for info in epoch_info])\n",
        "\n",
        "  scheduler.step()\n",
        "\n",
        "  if train_loss < best_train_loss or valid_loss < best_valid_loss:\n",
        "  # if valid_loss < best_valid_loss:\n",
        "    best_train_loss = train_loss\n",
        "    best_valid_loss = valid_loss\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict()\n",
        "    }, f'{DIR_PATH}/{model_name}')\n",
        "    print('SAVED MODEL')\n",
        "    train_log = update_trainlog(train_log, f'{DIR_PATH}/training_log-OLD.txt')\n",
        "\n",
        "  print(f'Epoch: {epoch:02} \\t Train Loss: {train_loss:.3f} \\t Val. Loss: {valid_loss:.3f}')"
      ],
      "metadata": {
        "id": "3ngIAH5cldRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eval"
      ],
      "metadata": {
        "id": "jRFofx7s1jJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in test_iterator:\n",
        "  break\n",
        "batch.fields"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qS7TxmFB40s9",
        "outputId": "86ef0fdc-aae2-4de1-ad17-ecd3a0df0dc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['en', 'fr', 'it'])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = f'{DIR_PATH}/traing-EnFr-EnEsFr-1.pt'\n",
        "ckpt = torch.load(model_path)"
      ],
      "metadata": {
        "id": "g-RDSB1LkKys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
        "# scheduler.load_state_dict(ckpt['scheduler_state_dict'])\n",
        "model.load_state_dict(ckpt['model_state_dict']) # strict=False if some dimensions are different"
      ],
      "metadata": {
        "id": "PwZxT4MZ8tV3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cd8843d-8e3d-4060-8350-922cef5664bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test_loss = evaluate(model_infer, test_iterator, criterion2, isPivot=False, force=0)\n",
        "# print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "metadata": {
        "id": "blKOkBAe1m5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "cbS1GfWq2DmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sent2tensor(src_field, trg_field, device, max_len, sentence=None):\n",
        "  if sentence != None:\n",
        "    if isinstance(sentence, str):\n",
        "      tokens = tokenize_en(sentence)\n",
        "    else:\n",
        "      tokens = [token.lower() for token in sentence]\n",
        "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
        "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)  # [seq_len, N] w/ N=1 for batch\n",
        "    src_len_tensor = torch.LongTensor([len(src_indexes)]).to(device)\n",
        "    return src_tensor, src_len_tensor\n",
        "\n",
        "  trg_tensor = torch.LongTensor([trg_field.vocab.stoi[trg_field.init_token]] + [0 for i in range(1, max_len)]).view(-1, 1).to(device) # [seq_len, 1]\n",
        "  trg_len_tensor = torch.LongTensor([max_len]).to(device)\n",
        "  return trg_tensor, trg_len_tensor\n",
        "\n",
        "def idx2sent(trg_field, arr):\n",
        "  n_sents = arr.shape[1]  # arr = [seq_len, N]\n",
        "  results = []\n",
        "  for i in range(n_sents):  # for each sent\n",
        "    pred_sent = []\n",
        "    pred = arr[:, i]\n",
        "    for i in pred[1:]:  # for each word\n",
        "      pred_sent.append(trg_field.vocab.itos[i])\n",
        "      if i == trg_field.vocab.stoi[trg_field.eos_token]: break\n",
        "    results.append(pred_sent)\n",
        "  return results"
      ],
      "metadata": {
        "id": "xUMMEYoLaDr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Translate by sent"
      ],
      "metadata": {
        "id": "_3qwJt8Q1qN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sentence(sentence, src_field, trg_field, model, device, max_len=64):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    # create data\n",
        "    src = sent2tensor(src_field, trg_field, device, max_len, sentence)\n",
        "    trg = sent2tensor(src_field, trg_field, device, max_len)\n",
        "    # get data\n",
        "    if isinstance(model, Seq2Seq):\n",
        "      data = [src, trg]\n",
        "    elif isinstance(model, PivotSeq2Seq):\n",
        "      data = [src] + [trg for _ in range(model.num_model)]\n",
        "    elif isinstance(model, TriangSeq2Seq):\n",
        "      data = {'TRG': trg}\n",
        "      for i in range(model.num_model):\n",
        "        submodel = getattr(model, f'model_{i}')\n",
        "        if isinstance(submodel, Seq2Seq):\n",
        "          data[f'model_{i}'] = [src, trg]\n",
        "        elif isinstance(submodel, PivotSeq2Seq):\n",
        "          data[f'model_{i}'] = [src] + [trg for _ in range(submodel.num_model)]\n",
        "        else: raise Exception('Only support Seq2Seq & PivotSeq2Seq nested inside TriangSeq2Seq')\n",
        "    else: raise Exception('Model type is unknown')\n",
        "    # feed model\n",
        "    output = model(data, None, 0) # output = [trg_len, N, dec_emb_dim] w/ N=1\n",
        "    output = output.argmax(-1).detach().cpu().numpy() # output = [seq_len, N]\n",
        "    results = idx2sent(trg_field, output)\n",
        "    return results"
      ],
      "metadata": {
        "id": "gS4-Qw0OYCug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sentence_seq2seq(sentence, src_field, trg_field, model: Seq2Seq, device, max_len=64):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    # get data\n",
        "    src_tensor, src_len_tensor = sent2tensor(src_field, trg_field, device, max_len, sentence)\n",
        "    trg_tensor, trg_len_tensor = sent2tensor(src_field, trg_field, device, max_len)\n",
        "    data = [(src_tensor, src_len_tensor), (trg_tensor, trg_len_tensor)]\n",
        "    # feed model\n",
        "    output = model(data, None, 0) # output = [trg_len, N, dec_emb_dim] w/ N=1\n",
        "    output = output.argmax(-1).detach().cpu().numpy() # output = [seq_len, N]\n",
        "    results = idx2sent(trg_field, output)\n",
        "    return results\n",
        "\n",
        "def translate_sentence_pivot(sentence, src_field, trg_field, model, device, max_len=64):  # not yet modified\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    # get data\n",
        "    src_tensor, src_len_tensor = sent2tensor(src_field, trg_field, device, max_len, sentence)\n",
        "    trg_tensor, trg_len_tensor = sent2tensor(src_field, trg_field, device, max_len)\n",
        "    data = [(src_tensor, src_len_tensor)] + [(trg_tensor.clone().detach().to(device), trg_len_tensor.clone().detach().to(device)) for _ in range(model.num_model)]\n",
        "    # feed model\n",
        "    output = model(data, None, 0) # output = [trg_len, N, dec_emb_dim]\n",
        "    output = output.argmax(-1).detach().cpu().numpy()\n",
        "    results = idx2sent(trg_field, output)\n",
        "    return results\n",
        "\n",
        "def translate_batch_triang(sentence, src_field, trg_field, model, device, max_len=64):  # not yet complete. modify sentence/sentences, fields base on type of models\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    # get data\n",
        "    src_tensor, src_len_tensor = sent2tensor(src_field, trg_field, device, max_len, sentence)\n",
        "    trg_tensor, trg_len_tensor = sent2tensor(src_field, trg_field, device, max_len)\n",
        "    data = {\n",
        "      'model_0': [(src_tensor, src_len_tensor), (trg_tensor, trg_len_tensor)],\n",
        "      'model_1': [(src_tensor, src_len_tensor), (trg_tensor, trg_len_tensor), (trg_tensor, trg_len_tensor)],\n",
        "      'model_2': [(src_tensor, src_len_tensor), (trg_tensor, trg_len_tensor), (trg_tensor, trg_len_tensor)],\n",
        "      'TRG': (trg_tensor, trg_len_tensor)}\n",
        "    # feed model\n",
        "    output = model(data, None, 0)\n",
        "    output = output.argmax(-1).detach().cpu().numpy()\n",
        "    results = idx2sent(trg_field, output)\n",
        "    return results"
      ],
      "metadata": {
        "id": "c6d_Ggwz2EcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if isinstance(model, Seq2Seq):\n",
        "  translator = translate_sentence_seq2seq\n",
        "  print('Created translator for seq2seq')\n",
        "elif isinstance(model, PivotSeq2Seq):\n",
        "  translator = translate_sentence_pivot\n",
        "  print('Created translator for piv')\n",
        "elif isinstance(model, TriangSeq2Seq):\n",
        "  translator = translate_batch_triang\n",
        "  print('Created translator for triang')\n",
        "else:\n",
        "  try:\n",
        "    translator = translate_sentence\n",
        "  except:\n",
        "    raise Exception('Model type is unknown')"
      ],
      "metadata": {
        "id": "nCEv3JC-i8_2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f93432b1-46ff-4ce3-d1b3-fb61503eab97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created translator for triang\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_idx = 2132\n",
        "src = vars(valid_dt.examples[example_idx])['en']\n",
        "trg = vars(valid_dt.examples[example_idx])['fr']\n",
        "pred = translator(src, EN_FIELD, FR_FIELD, model, device)\n",
        "print(src)\n",
        "print(trg)\n",
        "print(pred)"
      ],
      "metadata": {
        "id": "sK5gR8S7Ecle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Translate by batch"
      ],
      "metadata": {
        "id": "2_uEiN_p1ufE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in test_iterator: break\n",
        "\n",
        "print(batch.fields)\n",
        "print(batch.en[0].shape, batch.en[1].shape)\n",
        "print(batch.en[1])\n",
        "print(batch.fr[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwABYGQpZoFf",
        "outputId": "82baf8b2-b248-4b7d-cfbc-3f92831f30f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['en', 'fr', 'es', 'it', 'pt', 'ro'])\n",
            "tensor([7, 5, 9, 3], device='cuda:0')\n",
            "tensor([5, 5, 5, 5], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_batch(model, iterator, trg_field, device, max_len):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    gt_sents = []\n",
        "    pred_sents = []\n",
        "    for idx, batch in enumerate(tqdm(iterator)):\n",
        "      # get data\n",
        "      seq_len, N = batch.en[0].shape  # batch.en = (data, len)\n",
        "      src = batch.en\n",
        "      (trg_data, trg_len) = sent2tensor(EN_FIELD, FR_FIELD, device, max_len)\n",
        "      trg_datas = torch.cat([trg_data for _ in range(N)], dim=1)\n",
        "      trg_lens = torch.cat([trg_len for _ in range(N)], dim=0)\n",
        "      trg = (trg_datas, trg_lens)\n",
        "      if isinstance(model, Seq2Seq):\n",
        "        data = [src, trg]\n",
        "      elif isinstance(model, PivotSeq2Seq):\n",
        "        data = [src] + [trg for _ in range(model.num_model)]\n",
        "      elif isinstance(model, TriangSeq2Seq):\n",
        "        data = {'TRG': trg}\n",
        "        for i in range(model.num_model):\n",
        "          submodel = getattr(model, f'model_{i}')\n",
        "          if isinstance(submodel, Seq2Seq):\n",
        "            data[f'model_{i}'] = [src, trg]\n",
        "          elif isinstance(submodel, PivotSeq2Seq):\n",
        "            data[f'model_{i}'] = [src] + [trg for _ in range(submodel.num_model)]\n",
        "          else: raise Exception('Only support Seq2Seq & PivotSeq2Seq nested inside TriangSeq2Seq')\n",
        "      else: raise Exception('Model type is unknown')\n",
        "      # feed model\n",
        "      output = model(data, None, 0)\n",
        "      pred = output.argmax(-1).detach().cpu().numpy() # [seq_len, N]\n",
        "      truth = batch.fr[0].detach().cpu().numpy()  # [seq_len, N]\n",
        "\n",
        "      gt_sents = gt_sents + idx2sent(trg_field, truth)\n",
        "      pred_sents = pred_sents + idx2sent(trg_field, pred)\n",
        "    return gt_sents, pred_sents"
      ],
      "metadata": {
        "id": "oIRZYHOsi0LU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gt_sents, pred_sents = translate_batch(model, test_iterator, FR_FIELD, device, 64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fhhvOpTkQkD",
        "outputId": "fb11c4ac-69a5-44ea-e037-639ab9d46a95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 10/1600 [00:04<13:09,  2.01it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (gt_sent, pred_sent) in enumerate(zip(gt_sents, pred_sents)):\n",
        "  print(gt_sent[:-1])\n",
        "  print(pred_sent[:-1])\n",
        "  print()\n",
        "  if i==3: break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKokrGMnlDp5",
        "outputId": "ac9950b3-37bd-4005-803f-429c2cafbacd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['il', 'faut', 'à', 'présent', 'de', 'bonnes', 'politiques', ',', 'par', 'un', 'effort', '<unk>', 'de', 'la', 'communauté', 'internationale', ',', 'afin', 'de', 'chercher', 'à', 'obtenir', 'libération', 'et', 'justice', 'pour', 'tous', 'ceux', 'qui', 'ont', 'été', 'si', '<unk>', 'touchés', '.']\n",
            "['les', 'politiques', 'devraient', 'être', 'développées', ',', 'un', 'effort', 'effort', 'pour', 'la', 'communauté', 'internationale', ',', 'pour', 'la', 'justice', 'et', 'tous', 'les', 'personnes', 'qui', 'ont', 'été', 'touchés', '.']\n",
            "\n",
            "[\"c'\", 'est', 'pourquoi', 'nous', 'devons', 'nous', 'battre', 'pour', 'défendre', 'cette', 'politique', ',', 'pour', 'défendre', 'son', 'budget', ',', 'pour', 'convaincre', 'les', 'états', 'que', 'la', 'politique', 'régionale', \"n'\", 'est', 'pas', 'un', '<unk>', 'mais', 'une', 'nécessité', '.']\n",
            "[\"c'\", 'est', 'pourquoi', 'nous', 'devons', 'lutter', 'défendre', 'cette', 'politique', ',', 'défendre', 'sa', 'budget', ',', 'pour', 'les', 'états', 'membres', 'que', 'la', 'politique', 'régionale', \"n'\", 'est', 'pas', 'nécessaire', 'nécessité', \"d'\", 'une', 'nécessité', 'nécessité', \"d'\", 'une', 'nécessité', \"d'\", 'une', 'nécessité', \"d'\", 'une', 'nécessité', \"d'\", 'une', 'nécessité', '.']\n",
            "\n",
            "['ceux', '-', 'ci', 'sont', 'également', 'compétents', 'et', 'responsables', 'de', 'la', 'sélection', 'et', 'de', 'la', 'mise', 'en', 'œuvre', 'des', 'projets', 'et', ',', 'enfin', ',', 'de', 'la', 'surveillance', ',', 'au', 'moins', 'dans', 'un', 'premier', 'temps', '.']\n",
            "['elles', 'sont', 'également', 'responsables', 'et', 'la', 'la', 'de', 'la', 'mise', 'en', 'œuvre', 'des', 'projets', 'et', ',', 'en', 'en', 'fin', 'de', 'compte', ',', 'au', 'au', 'moins', 'au', 'moins', 'au', 'moins', 'au', 'niveau', 'au', 'niveau', '.']\n",
            "\n",
            "['ce', 'qui', 'se', 'passe', 'en', 'grèce', 'pourrait', \"s'\", 'appliquer', 'un', 'jour', 'à', \"l'\", 'irlande', 'ou', 'au', 'portugal', ',', 'à', \"l'\", 'espagne', 'ou', 'à', 'la', 'belgique', ',', 'à', \"l'\", 'italie', 'ou', 'à', 'la', 'france', '.']\n",
            "['ce', 'qui', 'se', 'passe', 'actuellement', 'la', 'grèce', ',', \"c'\", 'est', 'une', 'ou', 'pour', 'ou', 'ou', 'ou', 'ou', 'ou', 'ou', 'france', 'ou', 'de', 'france', '.']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BLEU"
      ],
      "metadata": {
        "id": "_gNHZNQk2e4M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main"
      ],
      "metadata": {
        "id": "celagCJTtFd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_bleu_batch(translator, iterator, trg_field, model, device, max_len=64):\n",
        "  gt_sents, pred_sents = translator(model, iterator, trg_field, device, max_len)\n",
        "  pred_trgs = [pred_sent[:-1] for pred_sent in pred_sents]\n",
        "  trgs = [[gt_sent[:-1]] for gt_sent in gt_sents]\n",
        "  return pred_trgs, trgs\n",
        "\n",
        "def calculate_bleu_sent(translator, data, src_field, trg_field, model, device, max_len = 50):\n",
        "  trgs = []\n",
        "  pred_trgs = []\n",
        "  for i, datum in enumerate(tqdm(data)):\n",
        "    src = vars(datum)['en']\n",
        "    trg = vars(datum)['fr']\n",
        "    pred = translator(src, src_field, trg_field, model, device, max_len)\n",
        "    pred_trgs.append(pred[0][:-1])  #cut off <eos> token\n",
        "    trgs.append([trg])\n",
        "  return pred_trgs, trgs"
      ],
      "metadata": {
        "id": "bBSv6ECcbIpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pred_trgs, trgs = calculate_bleu_sent(translator, test_dt, EN_FIELD, FR_FIELD, model, device)\n",
        "# score = bleu_score(pred_trgs, trgs)\n",
        "# print(f'BLEU score = {score*100:.2f}')"
      ],
      "metadata": {
        "id": "XxTAqzqz1LH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.method = 'max'\n",
        "pred_trgs, trgs = calculate_bleu_sent(translate_sentence, test_dt, EN_FIELD, FR_FIELD, model.model_0, device)\n",
        "score = bleu_score(pred_trgs, trgs)\n",
        "print('en-fr')\n",
        "print(f'BLEU score = {score*100:.3f}')"
      ],
      "metadata": {
        "id": "UDzwuc3z2sDq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "549db9ae-b5e8-4646-cc49-c843af20911b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6400/6400 [06:53<00:00, 15.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "en-fr\n",
            "BLEU score = 38.738\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.method = 'max'\n",
        "pred_trgs, trgs = calculate_bleu_sent(translate_sentence, test_dt, EN_FIELD, FR_FIELD, model.model_1, device)\n",
        "score = bleu_score(pred_trgs, trgs)\n",
        "print('en-es-fr')\n",
        "print(f'BLEU score = {score*100:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1G66Kc90FlS",
        "outputId": "3f4a0027-9fd9-4fb3-e9be-6523da33bbd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6400/6400 [14:02<00:00,  7.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "en-es-fr\n",
            "BLEU score = 32.273\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lang = 'it'\n",
        "TMP_FIELD = FIELD_DICT[lang]\n",
        "\n",
        "SRC_PAD_IDX = EN_FIELD.vocab.stoi[EN_FIELD.pad_token]\n",
        "attn1 = Attention(cfg['HID_DIM'], cfg['HID_DIM'])\n",
        "enc1 = Encoder(cfg['en_DIM'], cfg['EMB_DIM'], cfg['HID_DIM'], cfg['HID_DIM'], cfg['DROPOUT'])\n",
        "dec1 = Decoder(cfg[f'{lang}_DIM'], cfg['EMB_DIM'], cfg['HID_DIM'], cfg['HID_DIM'], cfg['DROPOUT'], attn1)\n",
        "model1 = Seq2Seq(enc1, dec1, SRC_PAD_IDX, device).to(device)\n",
        "\n",
        "PIV_PAD_IDX = TMP_FIELD.vocab.stoi[TMP_FIELD.pad_token]\n",
        "attn2 = Attention(cfg['HID_DIM'], cfg['HID_DIM'])\n",
        "enc2 = Encoder(cfg[f'{lang}_DIM'], cfg['EMB_DIM'], cfg['HID_DIM'], cfg['HID_DIM'], cfg['DROPOUT'])\n",
        "dec2 = Decoder(cfg['fr_DIM'], cfg['EMB_DIM'], cfg['HID_DIM'], cfg['HID_DIM'], cfg['DROPOUT'], attn2)\n",
        "model2 = Seq2Seq(enc2, dec2, PIV_PAD_IDX, device).to(device)\n",
        "\n",
        "models = [model1, model2]\n",
        "fields = [EN_FIELD, TMP_FIELD, FR_FIELD]\n",
        "model_piv2 = PivotSeq2Seq(models, fields, device).to(device)\n",
        "modelname = f'piv-En{lang[0].upper()}{lang[1]}Fr.pt'\n",
        "model_piv2.load_state_dict(torch.load(f'{DIR_PATH}/{modelname}')['model_state_dict'])\n",
        "print(f'loaded model {modelname}')\n",
        "model.method = 'max'\n",
        "\n",
        "pred_trgs, trgs = calculate_bleu_sent(translate_sentence, test_dt, EN_FIELD, FR_FIELD, model_piv2, device)\n",
        "score = bleu_score(pred_trgs, trgs)\n",
        "print(f'en-{lang}-fr')\n",
        "print(f'BLEU score = {score*100:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAS42sVr1Uxx",
        "outputId": "423f8c93-fdc3-4321-fead-862b6e71d48a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded model piv-EnItFr.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6400/6400 [14:00<00:00,  7.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "en-it-fr\n",
            "BLEU score = 31.369\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model.method = 'max'\n",
        "# pred_trgs, trgs = calculate_bleu_batch(translate_batch, test_iterator, FR_FIELD, model, device)\n",
        "# score = bleu_score(pred_trgs, trgs)\n",
        "# print(f'BLEU score = {score*100:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlrzHG46fHjG",
        "outputId": "9d74a481-cdfa-4724-83ae-f602942eee9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [04:42<00:00,  2.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU score = 33.698\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model.method = 'average'\n",
        "# pred_trgs, trgs = calculate_bleu_batch(translate_batch, test_iterator, FR_FIELD, model, device)\n",
        "# score = bleu_score(pred_trgs, trgs)\n",
        "# print(f'BLEU score = {score*100:.3f}')"
      ],
      "metadata": {
        "id": "qaNNsBEvdPix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Result"
      ],
      "metadata": {
        "id": "LcU3uVKbtI4X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. seq2seq-EnFr-1.pt: BLEU = 38.74\n",
        "1. piv_EnItFr.pt: BLEU = 31.37\n",
        "1. triang-EnFr-EnItFr.pt: BLEU = 31.95\n",
        "  * dir-EnFr: BLEU = 38.03\n",
        "  * piv-EnItFr: BLEU = 24.62\n",
        "1. triang-EnFr-EnEsFr-1.pt: BLEU = 30.57 (weight_1)\n",
        "1. Prof Neller's suggestion (max selection):\n",
        "  * triang(EnFr + EnEsFr) BLEU = 37.80\n",
        "  * triang(EnFr + EnEsFr) BLEU = 38.81 (2nd run)\n",
        "  * triang(EnEsFr + EnItFr) BLEU = 33.698 (2nd run, 1st run got 36.25)\n",
        "1. Paper averaging output (\"Late Averaging\"):\n",
        "  * triang(EnFr + EnEsFr) BLEU = 28.25\n",
        "  * triang(EnEsFr + EnItFr) BLEU = 25.749"
      ],
      "metadata": {
        "id": "cui8pV4FBbx_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* attn_en-fr_32k.pt: BLEU = 12.65\n",
        "* attn_enfr_160kset.pt: BLEU = 32.18\n",
        "* piv_endefr_74kset_2.pt: BLEU = 26.33\n"
      ],
      "metadata": {
        "id": "vIw-3jBB42zA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# End"
      ],
      "metadata": {
        "id": "_0vl6ryRfWl9"
      }
    }
  ]
}